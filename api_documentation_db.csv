tf_api,tf_doc,pytorch_api,pytorch_doc,similarity
tensorflow.python.summary.writer.event_file_writer_v2.flush,Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.,torch.utils.tensorboard.writer.flush,Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.,1.0000004
tensorflow.python.data.ops.dataset_ops.is_subtype_of,See base class.,torch.distributed.elastic.rendezvous.dynamic_rendezvous.state,See base class.,1.0000001
tensorflow.python.distribute.tpu_strategy.run,See base class.,torch.distributed.elastic.rendezvous.dynamic_rendezvous.shutdown,See base class.,1.0000001
tensorflow.python.data.ops.dataset_ops.most_specific_common_supertype,See base class.,torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.name,See base class.,1.0000001
tensorflow.python.training.monitored_session.run,See base class.,torch.distributed.elastic.rendezvous.dynamic_rendezvous.is_closed,See base class.,1.0000001
tensorflow.python.summary.writer.writer.close,Flushes the event file to disk and close the file. Call this method when you do not need the summary writer anymore.,torch.utils.tensorboard.writer.close,Flushes the event file to disk and close the file. Call this method when you do not need the summary writer anymore.,1.0000001
tensorflow.python.summary.writer.writer.reopen,Reopens the EventFileWriter. Can be called after `close()` to add more events in the same directory. The events will go into a new events file. Does nothing if the EventFileWriter was not closed.,torch.utils.tensorboard.writer.reopen,Reopens the EventFileWriter. Can be called after `close()` to add more events in the same directory. The events will go into a new events file. Does nothing if the EventFileWriter was not closed.,0.9999998
tensorflow.python.lib.io.file_io.name,Returns the file name.,torch.profiler.profiler.get_output_file_path,Returns the output file name.,0.98644507
tensorflow.python.distribute.test_util.get_running_threads,Returns a set of all running thread names.,torch.onnx._internal.registration.all_functions,Returns the set of all registered function names.,0.9771573
tensorflow.python.summary.writer.event_file_writer.get_logdir,Returns the directory where event file will be written.,torch.utils.tensorboard.writer.get_logdir,Return the directory where event file will be written.,0.97695184
tensorflow.python.framework.function.grad_func_name,Returns the name of the gradient function.,torch.onnx._internal.registration.get_function_group,Returns the function group for the given name.,0.9743528
tensorflow.python.ops.logging_ops.merge_summary,"Merges summaries. This op is deprecated. Please switch to tf.compat.v1.summary.merge, which has identical behavior. This op creates a [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto) protocol buffer that contains the union of all the values in the input summaries. When the Op is run, it reports an `InvalidArgument` error if multiple values in the summaries to merge use the same tag. Args: inputs: A list of `string` `Tensor` objects containing serialized `Summary` protocol buffers. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. name: A name for the operation (optional). Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer resulting from the merging.",torch.utils.tensorboard.summary.histogram,Output a `Summary` protocol buffer with a histogram. The generated [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto) has one summary value containing a histogram for `values`. This op reports an `InvalidArgument` error if any value is not finite. Args: name: A name for the generated node. Will also serve as a series name in TensorBoard. values: A real numeric `Tensor`. Any shape. Values to use to build the histogram. Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer.,0.9732198
tensorflow.python.keras.engine.base_layer_utils.make_variable,"Temporary util to create a variable (relies on `variable_scope.variable`). Some reuse-related technicalities prevent us from using `variable_scope.get_variable()` directly, so we use a subcomponent that has fewer constraints (`variable_scope.variable()`). In the longer term, it seems like a similar ""default variable creator"" method should exist in `Trackable` instead. When this happens, we can get rid of this temporary solution. TODO(fchollet): remove this method when no longer needed. Args: name: Variable name. shape: Variable shape. dtype: The type of the variable. Defaults to `self.dtype` or `float32`. initializer: Initializer instance (callable). trainable: Whether the variable should be part of the layer's ""trainable_variables"" (e.g. variables, biases) or ""non_trainable_variables"" (e.g. BatchNorm mean, stddev). Note, if the current variable scope is marked as non-trainable then this parameter is ignored and any added variables are also marked as non-trainable. `trainable` defaults to `True` unless `synchronization` is set to `ON_READ`. caching_device: Passed to `tf.Variable`. validate_shape: Passed to `tf.Variable`. constraint: Constraint instance (callable). use_resource: Whether to use a `ResourceVariable`. collections: List of graph collections keys. The new variable is added to these collections. Defaults to `[GraphKeys.GLOBAL_VARIABLES]`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. partitioner: Not handled at this time. Returns: Variable instance.",torch.distributed.checkpoint.state_dict_loader.load,"Load a distributed ``state_dict`` in SPMD style. Each rank will try to read the least amount of data necessary to fullfill the requested `state_dict`. When loading :class:`ShardedTensor` or :class:`DTensor` instances, each rank only reads data for their local shards. For each ``Stateful`` object (having both a ``state_dict`` and a ``load_state_dict``), load will first call ``state_dict`` before attempting deserialization, followed by ``load_state_dict`` once the deserialization is complete. .. warning:: All tensors in ``state_dict`` must be allocated on their destination device *prior to* calling this function. All non-tensor data is loaded using `torch.load()` and modified in place on state_dict. .. warning:: Users must call `load_state_dict` on the root module to ensure load pos-processing and non-tensor data properly propagates. .. note: If no process group is initialized, this function will assume the intent is to load a checkpoint into the local process. This can be useful in the case of local inference, and when using regular Tensors (as opposed to DTensor or ShardedTensor) .. note: Rank 0 is assumed to be the coordinator rank. Args: state_dict (Dict[str, Any]): The state_dict to save. checkpoint_id (Union[str, os.PathLike, None]): The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: ``None``) storage_reader (Optional[StorageReader]): Instance of StorageWriter used to perform reads. If this is not specified, DCP will automatically infer the reader based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: ``None``) planner (Optional[LoadPlanner]): Instance of LoadPlanner. If this is not specificed, the default planner will be used. (Default: ``None``) process_group (Optional[ProcessGroup]): ProcessGroup to be used for cross-rank synchronization. (Default: ``None``) Returns:",0.97279835
tensorflow.python.ops.math_ops.reduce_std,"Computes the standard deviation of elements across dimensions of a tensor. Reduces `input_tensor` along the dimensions given in `axis`. Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each of the entries in `axis`, which must be unique. If `keepdims` is true, the reduced dimensions are retained with length 1. If `axis` is None, all dimensions are reduced, and a tensor with a single element is returned. For example: >>> x = tf.constant([[1., 2.], [3., 4.]]) >>> tf.math.reduce_std(x) <tf.Tensor: shape=(), dtype=float32, numpy=1.118034> >>> tf.math.reduce_std(x, 0) <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)> >>> tf.math.reduce_std(x, 1) <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)> Args: input_tensor: The tensor to reduce. Should have real or complex type. axis: The dimensions to reduce. If `None` (the default), reduces all dimensions. Must be in the range `[-rank(input_tensor), rank(input_tensor))`. keepdims: If true, retains reduced dimensions with length 1. name: A name scope for the associated operations (optional). Returns: The reduced tensor, of the same dtype as the input_tensor. Note, for `complex64` or `complex128` input, the returned `Tensor` will be of type `float32` or `float64`, respectively. @compatibility(numpy) Equivalent to np.std Please note `np.std` has a `dtype` parameter that could be used to specify the output type. By default this is `dtype=float64`. On the other hand, `tf.math.reduce_std` has aggressive type inference from `input_tensor`. @end_compatibility",torch.autograd.functional.hvp,"Compute the dot product between the scalar function's Hessian and a vector ``v`` at a specified point. Args: func (function): a Python function that takes Tensor inputs and returns a Tensor with a single element. inputs (tuple of Tensors or Tensor): inputs to the function ``func``. v (tuple of Tensors or Tensor): The vector for which the Hessian vector product is computed. Must be the same size as the input of ``func``. This argument is optional when ``func``'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single ``1``. create_graph (bool, optional): If ``True``, both the output and result will be computed in a differentiable way. Note that when ``strict`` is ``False``, the result can not require gradients or be disconnected from the inputs. Defaults to ``False``. strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to ``False``. Returns: output (tuple): tuple with: func_output (tuple of Tensors or Tensor): output of ``func(inputs)`` hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD) >>> def pow_reducer(x): ... return x.pow(3).sum() >>> inputs = torch.rand(2, 2) >>> v = torch.ones(2, 2) >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> hvp(pow_reducer, inputs, v) (tensor(0.1448), tensor([[2.0239, 1.6456], [2.4988, 1.4310]])) >>> hvp(pow_reducer, inputs, v, create_graph=True) (tensor(0.1448, grad_fn=<SumBackward0>), tensor([[2.0239, 1.6456], [2.4988, 1.4310]], grad_fn=<MulBackward0>)) >>> def pow_adder_reducer(x, y): ... return (2 * x.pow(2) + 3 * y.pow(2)).sum() >>> inputs = (torch.rand(2), torch.rand(2)) >>> v = (torch.zeros(2), torch.ones(2)) >>> hvp(pow_adder_reducer, inputs, v) (tensor(",0.9723697
tensorflow.python.framework.experimental.context_stack.get_default,Returns the default execution context.,torch.onnx._internal.diagnostics.infra.utils.python_call_stack,Returns the current Python call stack.,0.97088796
tensorflow.python.ops.template.non_trainable_variables,Returns the list of non-trainable variables created by the Template.,torch._higher_order_ops.auto_functionalize.get_mutable_arg_names,Returns the list of argument names that get mutated according to the schema.,0.97078955
tensorflow.python.summary.writer.writer.add_session_log,Adds a `SessionLog` protocol buffer to the event file. This method wraps the provided session in an `Event` protocol buffer and adds it to the event file. Args: session_log: A `SessionLog` protocol buffer. global_step: Number. Optional global step value to record with the summary.,torch.utils.tensorboard.writer.add_summary,Add a `Summary` protocol buffer to the event file. This method wraps the provided summary in an `Event` protocol buffer and adds it to the event file. Args: summary: A `Summary` protocol buffer. global_step: Number. Optional global step value for training process to record with the summary. walltime: float. Optional walltime to override the default (current) walltime (from time.time()) seconds after epoch,0.97045505
tensorflow.python.eager.polymorphic_function.eager_function_run.functions_run_eagerly,Returns the value of the `run_functions_eagerly` setting.,torch.distributed.checkpoint.staging.stage,Returns a copy of `state_dict` on the CPU.,0.96980083
tensorflow.python.keras.engine.base_layer_utils.from_saved_model,Returns whether the layer is loaded from a SavedModel.,torch.cuda.tunable.is_enabled,Returns whether the TunableOp feature is enabled.,0.9697702
tensorflow.python.keras.engine.training.fit,"Trains the model for a fixed number of epochs (iterations on a dataset). Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. - A `tf.data` dataset. Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A generator or `keras.utils.Sequence` returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. - A `tf.keras.utils.experimental.DatasetCreator`, which wraps a callable that takes a single argument of type `tf.distribute.InputContext`, and returns a `tf.data.Dataset`. `DatasetCreator` should be used when users prefer to specify the per-replica batching and sharding logic for the `Dataset`. See `tf.keras.utils.experimental.DatasetCreator` doc for more information. A more detailed description of unpacking behavior for iterator types (Dataset, generator, Sequence) is given below. If using `tf.distribute.experimental.ParameterServerStrategy`, only `DatasetCreator` type is supported for `x`. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). If `x` is a dataset, generator, or `keras.utils.Sequence` instance, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or `keras.utils.Sequence` instances (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to b",torch._functorch.apis.chunk_vmap,"chunk_vmap is the vectorizing map (vmap) using chunks of input data. It is a mix of vmap (which vectorizes everything) and map (which executes things sequentially). ``chunk_vmap`` vectorizes the input with number of chunks at a time. For more details about vectorizing map, see :func:`vmap`. .. note:: Please use :func:`vmap` with ``chunk_size`` argument instead of this API. Args: func (function): A Python function that takes one or more arguments. Must return one or more Tensors. in_dims (int or nested structure): Specifies which dimension of the inputs should be mapped over. ``in_dims`` should have a structure like the inputs. If the ``in_dim`` for a particular input is None, then that indicates there is no map dimension. Default: 0. out_dims (int or Tuple[int]): Specifies where the mapped dimension should appear in the outputs. If ``out_dims`` is a Tuple, then it should have one element per output. Default: 0. randomness (str): Specifies whether the randomness in this vmap should be the same or different across batches. If 'different', the randomness for each batch will be different. If 'same', the randomness will be the same across batches. If 'error', any calls to random functions will error. Default: 'error'. WARNING: this flag only applies to random PyTorch operations and does not apply to Python's random module or numpy randomness. chunks (int): Number of chunks to use to split the input data. Default is 2. If equals to 1 then :func:`vmap` is called. Returns: Returns a new ""batched"" function. It takes the same inputs as ``func``, except each input has an extra dimension at the index specified by ``in_dims``. It takes returns the same outputs as ``func``, except each output has an extra dimension at the index specified by ``out_dims``.",0.9697069
tensorflow.python.ops.math_ops.unsorted_segment_sqrt_n,"Computes the sum along segments of a tensor divided by the sqrt(N). Read [the section on segmentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math#about_segmentation) for an explanation of segments. This operator is similar to the `tf.math.unsorted_segment_sum` operator. Additionally to computing the sum over segments, it divides the results by sqrt(N). \\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\\) where the sum is over tuples `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the number of occurrences of id \\i\\. If there is no entry for a given segment ID `i`, it outputs 0. Note that this op only supports floating point and complex dtypes, due to tf.sqrt only supporting these types. If the given segment ID `i` is negative, the value is dropped and will not be added to the sum of the segment. Caution: On CPU, values in `segment_ids` are always validated to be less than `num_segments`, and an error is thrown for out-of-bound indices. On GPU, this does not throw an error for out-of-bound indices. On Gpu, out-of-bound indices result in safe but unspecified behavior, which may include ignoring out-of-bound indices or outputting a tensor with a 0 stored in the first dimension of its shape if `num_segments` is 0. Args: data: A `Tensor` with floating point or complex dtype. segment_ids: An integer tensor whose shape is a prefix of `data.shape`. The values must be in the range `[0, num_segments)`. The values are always validated to be in range on CPU, never validated on GPU. num_segments: An integer scalar `Tensor`. The number of distinct segment IDs. name: A name for the operation (optional). Returns: A `Tensor`. Has same shape as data, except for the first `segment_ids.rank` dimensions, which are replaced with a single dimension which has size `num_segments`.",torch.functional.meshgrid,"Creates grids of coordinates specified by the 1D inputs in `attr`:tensors. This is helpful when you want to visualize data over some range of inputs. See below for a plotting example. Given :math:`N` 1D tensors :math:`T_0 \ldots T_{N-1}` as inputs with corresponding sizes :math:`S_0 \ldots S_{N-1}`, this creates :math:`N` N-dimensional tensors :math:`G_0 \ldots G_{N-1}`, each with shape :math:`(S_0, ..., S_{N-1})` where the output :math:`G_i` is constructed by expanding :math:`T_i` to the result shape. .. note:: 0D inputs are treated equivalently to 1D inputs of a single element. .. warning:: `torch.meshgrid(*tensors)` currently has the same behavior as calling `numpy.meshgrid(*arrays, indexing='ij')`. In the future `torch.meshgrid` will transition to `indexing='xy'` as the default. https://github.com/pytorch/pytorch/issues/50276 tracks this issue with the goal of migrating to NumPy's behavior. .. seealso:: :func:`torch.cartesian_prod` has the same effect but it collects the data in a tensor of vectors. Args: tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size :math:`(1,)` automatically indexing: (str, optional): the indexing mode, either ""xy"" or ""ij"", defaults to ""ij"". See warning for future changes. If ""xy"" is selected, the first dimension corresponds to the cardinality of the second input and the second dimension corresponds to the cardinality of the first input. If ""ij"" is selected, the dimensions are in the same order as the cardinality of the inputs. Returns: seq (sequence of Tensors): If the input has :math:`N` tensors of size :math:`S_0 \ldots S_{N-1}``, then the output will also have :math:`N` tensors, where each tensor is of shape :math:`(S_0, ..., S_{N-1})`. Example:: >>> x = torch.tensor([1, 2, 3]) >>> y = torch.tensor([4, 5, 6]) Observe the element-wise pairings across the grid, (1, 4), (1, 5), ..., (3, 6). This is the same thing as the cartesian product. >>> grid_x, grid_y = torch.meshgrid(x, y",0.969637
tensorflow.python.training.experimental.loss_scale_optimizer.variables,Returns the variables of the Optimizer.,torch.distributions.distribution.mode,Returns the mode of the distribution.,0.9696209
tensorflow.python.framework.func_graph.func_graph_from_py_func,"Returns a `FuncGraph` generated from `python_func`. Args: name: an identifier for the function. python_func: the Python function to trace. args: the positional args with which the Python function should be called; ignored if a signature is provided. kwargs: the keyword args with which the Python function should be called; ignored if a signature is provided. signature: a possibly nested sequence of `TensorSpecs` specifying the shapes and dtypes of the arguments. When a signature is provided, `args` and `kwargs` are ignored, and `python_func` is traced with Tensors conforming to `signature`. If `None`, the shapes and dtypes are inferred from the inputs. func_graph: Optional. An instance of FuncGraph. If provided, we will use this graph else a new one is built and returned. add_control_dependencies: If True, automatically adds control dependencies to ensure program order matches execution order and stateful ops always execute. arg_names: Optional list of argument names, used to give input placeholders recognizable names. op_return_value: Optional. A Tensor. If set and `python_func` returns Operations, those return values will be replaced with this value. If not set, returning an Operation triggers an error. collections: a dictionary of collections this FuncGraph should start with. If not specified (None), the FuncGraph will read (but not write to) the outer graph's collections that are not allowlisted, and both read and write to the outer graph's collections that are allowlisted. The current allowlisted collections are the global variables, the local variables, and the trainable variables. Defaults to None. capture_by_value: An optional boolean. If True, the func graph will capture Variables by value instead of reference. By default inherit from outer graphs, and failing that will default to False. create_placeholders: An optional boolean. If True, then func graph will create placeholders for the inputs as graph ops. If False, the input args and kwargs will be treated ",torch._dynamo.eval_frame.export,"Export an input function f to a format that can be executed outside of PyTorch using the FX graph. Args: f (callable): A PyTorch function to be exported. aten_graph (bool): If True, exports a graph with ATen operators. If False, exports a graph with Python operators. Default is False. pre_dispatch (bool): If True, exports a graph with ATen operators, but before any logic in the PyTorch dispatcher has run. This can be useful if you want to apply further transformations on a graph before running it through autograd, autocast, or any other functionalities that are integrated into the dispatcher. This flag is only valid if aten_graph=True is set. Default is False. decomposition_table (dict): A dictionary that maps operators to their decomposition functions. Required if aten_graph or tracing_mode is specified. Default is None. tracing_mode (str): If ""symbolic"", turn on dynamic shapes support. Default is ""symbolic"". dynamic_shapes: An optional argument where the type should either be: 1) a dict from argument names of ``f`` to their dynamic shape specifications, 2) a tuple that specifies dynamic shape specifications for each input in original order. If you are specifying dynamism on keyword args, you will need to pass them in the order that is defined in the original function signature. The dynamic shape of a tensor argument can be specified as either (1) a dict from dynamic dimension indices to :func:`Dim` types, where it is not required to include static dimension indices in this dict, but when they are, they should be mapped to None; or (2) a tuple / list of :func:`Dim` types or None, where the :func:`Dim` types correspond to dynamic dimensions, and static dimensions are denoted by None. Arguments that are dicts or tuples / lists of tensors are recursively specified by using mappings or sequences of contained specifications. same_signature (bool): If True, rewrite the returned graph's signature to be the same as f. disable_constraint_solver (bool): Whether the dim const",0.9695288
tensorflow.python.ops.nn_ops.softmax_cross_entropy_with_logits_v2_helper,"Computes softmax cross entropy between `logits` and `labels`. Measures the probability error in discrete classification tasks in which the classes are mutually exclusive (each entry is in exactly one class). For example, each CIFAR-10 image is labeled with one and only one label: an image can be a dog or a truck, but not both. **NOTE:** While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of `labels` is a valid probability distribution. If they are not, the computation of the gradient will be incorrect. If using exclusive `labels` (wherein one and only one class is true at a time), see `sparse_softmax_cross_entropy_with_logits`. **WARNING:** This op expects unscaled logits, since it performs a `softmax` on `logits` internally for efficiency. Do not call this op with the output of `softmax`, as it will produce incorrect results. A common use case is to have logits and labels of shape `[batch_size, num_classes]`, but higher dimensions are supported, with the `axis` argument specifying the class dimension. `logits` and `labels` must have the same dtype (either `float16`, `float32`, or `float64`). Backpropagation will happen into both `logits` and `labels`. To disallow backpropagation into `labels`, pass label tensors through `tf.stop_gradient` before feeding it to this function. **Note that to avoid confusion, it is required to pass only named arguments to this function.** Args: labels: Each vector along the class dimension should hold a valid probability distribution e.g. for the case in which labels are of shape `[batch_size, num_classes]`, each row of `labels[i]` must be a valid probability distribution. logits: Unscaled log probabilities. axis: The class dimension. Defaulted to -1 which is the last dimension. name: A name for the operation (optional). dim: Deprecated alias for axis. Returns: A `Tensor` that contains the softmax cross entropy loss. Its type is the same as `logits` and its shape is the same ",torch._functorch.apis.vmap,"vmap is the vectorizing map; ``vmap(func)`` returns a new function that maps ``func`` over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by ``func``, effectively vectorizing those operations. vmap is useful for handling batch dimensions: one can write a function ``func`` that runs on examples and then lift it to a function that can take batches of examples with ``vmap(func)``. vmap can also be used to compute batched gradients when composed with autograd. .. note:: :func:`torch.vmap` is aliased to :func:`torch.func.vmap` for convenience. Use whichever one you'd like. Args: func (function): A Python function that takes one or more arguments. Must return one or more Tensors. in_dims (int or nested structure): Specifies which dimension of the inputs should be mapped over. ``in_dims`` should have a structure like the inputs. If the ``in_dim`` for a particular input is None, then that indicates there is no map dimension. Default: 0. out_dims (int or Tuple[int]): Specifies where the mapped dimension should appear in the outputs. If ``out_dims`` is a Tuple, then it should have one element per output. Default: 0. randomness (str): Specifies whether the randomness in this vmap should be the same or different across batches. If 'different', the randomness for each batch will be different. If 'same', the randomness will be the same across batches. If 'error', any calls to random functions will error. Default: 'error'. WARNING: this flag only applies to random PyTorch operations and does not apply to Python's random module or numpy randomness. chunk_size (None or int): If None (default), apply a single vmap over inputs. If not None, then compute the vmap :attr:`chunk_size` samples at a time. Note that :attr:`chunk_size=1` is equivalent to computing the vmap with a for-loop. If you run into memory issues computing the vmap, please try a non-None chunk_size. Returns: Returns a new ""batched"" function. It takes the same inputs as ``f",0.9694687
tensorflow.python.framework.ops.create_op,"Creates an `Operation` in this graph. This is a low-level interface for creating an `Operation`. Most programs will not call this method directly, and instead use the Python op constructors, such as `tf.constant()`, which add ops to the default graph. Args: op_type: The `Operation` type to create. This corresponds to the `OpDef.name` field for the proto that defines the operation. inputs: A list of `Tensor` objects that will be inputs to the `Operation`. dtypes: (Optional) A list of `DType` objects that will be the types of the tensors that the operation produces. input_types: (Optional.) A list of `DType`s that will be the types of the tensors that the operation consumes. By default, uses the base `DType` of each input in `inputs`. Operations that expect reference-typed inputs must specify `input_types` explicitly. name: (Optional.) A string name for the operation. If not specified, a name is generated based on `op_type`. attrs: (Optional.) A dictionary where the key is the attribute name (a string) and the value is the respective `attr` attribute of the `NodeDef` proto that will represent the operation (an `AttrValue` proto). op_def: (Optional.) The `OpDef` proto that describes the `op_type` that the operation will have. compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always computed). compute_device: (Optional.) If True, device functions will be executed to compute the device property of the Operation. Raises: TypeError: if any of the inputs is not a `Tensor`. ValueError: if colocation conflicts with existing device assignment. Returns: An `Operation` object.",torch.onnx._internal.jit_utils.op,"Creates an ONNX operator ""opname"", taking ""raw_args"" as inputs and ""kwargs"" as attributes. The set of operators and the inputs/attributes they take is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md Args: opname: The ONNX operator name, e.g., `Abs` or `Add`, or an operator qualified with a namespace, e.g., `aten::add`. raw_args: The inputs to the operator; usually provided as arguments to the `symbolic` definition. outputs: The number of outputs this operator returns. By default an operator is assumed to return a single output. If `outputs` is greater than one, this functions returns a tuple of output `Value`, representing each output of the ONNX operator in order. kwargs: The attributes of the ONNX operator, whose keys are named according to the following convention: `alpha_f` indicates the `alpha` attribute with type `f`. The valid type specifiers are `f` (float), `i` (int), `s` (string) or `t` (Tensor). An attribute specified with type float accepts either a single float, or a list of floats (e.g., you would say `dims_i` for a `dims` attribute that takes a list of integers). Returns: The value representing the single output of this operator (see the `outputs` keyword argument for multi-return nodes).",0.9693707
tensorflow.python.feature_column.feature_column_v2.embedding_column,"`DenseColumn` that converts from sparse, categorical input. Use this when your inputs are sparse, but you want to convert them to a dense representation (e.g., to feed to a DNN). Args: categorical_column: A `CategoricalColumn` created by a `categorical_column_with_*` function. This column produces the sparse IDs that are inputs to the embedding lookup. dimension: An integer specifying dimension of the embedding, must be > 0. combiner: A string specifying how to reduce if there are multiple entries in a single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with 'mean' the default. 'sqrtn' often achieves good accuracy, in particular with bag-of-words columns. Each of this can be thought as example level normalizations on the column. For more information, see `tf.embedding_lookup_sparse`. initializer: A variable initializer function to be used in embedding variable initialization. If not specified, defaults to `truncated_normal_initializer` with mean `0.0` and standard deviation `1/sqrt(dimension)`. ckpt_to_load_from: String representing checkpoint name/pattern from which to restore column weights. Required if `tensor_name_in_ckpt` is not `None`. tensor_name_in_ckpt: Name of the `Tensor` in `ckpt_to_load_from` from which to restore the column weights. Required if `ckpt_to_load_from` is not `None`. max_norm: If not `None`, embedding values are l2-normalized to this value. trainable: Whether or not the embedding is trainable. Default is True. use_safe_embedding_lookup: If true, uses safe_embedding_lookup_sparse instead of embedding_lookup_sparse. safe_embedding_lookup_sparse ensures there are no empty rows and all weights and ids are positive at the expense of extra compute cost. This only applies to rank 2 (NxM) shaped input tensors. Defaults to true, consider turning off if the above checks are not needed. Note that having empty rows will not trigger any error though the output result might be 0 or omitted. Returns: `DenseColumn` that converts from sparse inpu",torch.nn.functional.embedding,"Generate a simple lookup table that looks up embeddings in a fixed dictionary and size. This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings. See :class:`torch.nn.Embedding` for more details. .. note:: Note that the analytical gradients of this function with respect to entries in :attr:`weight` at the row specified by :attr:`padding_idx` are expected to differ from the numerical ones. .. note:: Note that `:class:`torch.nn.Embedding` differs from this function in that it initializes the row of :attr:`weight` specified by :attr:`padding_idx` to all zeros on construction. Args: input (LongTensor): Tensor containing indices into the embedding matrix weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated during training, i.e. it remains as a fixed ""pad"". max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm` is renormalized to have norm :attr:`max_norm`. Note: this will modify :attr:`weight` in-place. norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``. scale_grad_by_freq (bool, optional): If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default ``False``. sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under :class:`torch.nn.Embedding` for more details regarding sparse gradients. Shape: - Input: LongTensor of arbitrary shape containing the indices to extract - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`, where V = maximum index",0.96934026
tensorflow.python.data.ops.dataset_ops.from_generator,"Creates a `Dataset` whose elements are generated by `generator`. Note: The current implementation of `Dataset.from_generator()` uses `tf.numpy_function` and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called `Dataset.from_generator()`. In particular, using `from_generator` will preclude the use of tf.data service for scaling out dataset processing. The body of `generator` will not be serialized in a `GraphDef`, and you should not use this method if you need to serialize your model and restore it in a different environment. The `generator` argument must be a callable object that returns an object that supports the `iter()` protocol (e.g. a generator function). The elements generated by `generator` must be compatible with either the given `output_signature` argument or with the given `output_types` and (optionally) `output_shapes` arguments, whichever was specified. The recommended way to call `from_generator` is to use the `output_signature` argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by `tf.TypeSpec` objects from `output_signature` argument: >>> def gen(): ... ragged_tensor = tf.ragged.constant([[1, 2], [3]]) ... yield 42, ragged_tensor >>> >>> dataset = tf.data.Dataset.from_generator( ... gen, ... output_signature=( ... tf.TensorSpec(shape=(), dtype=tf.int32), ... tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32))) >>> >>> list(dataset.take(1)) [(<tf.Tensor: shape=(), dtype=int32, numpy=42>, <tf.RaggedTensor [[1, 2], [3]]>)] There is also a deprecated way to call `from_generator` by either with `output_types` argument alone or together with `output_shapes` argument. In this case the output of the function will be assumed to consist of `tf.Tensor` objects with the types defined by `output_types` and with the shapes which are either unknown or defined by `output_shap",torch.utils.dlpack.from_dlpack,"from_dlpack(ext_tensor) -> Tensor Converts a tensor from an external library into a ``torch.Tensor``. The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine. Args: ext_tensor (object with ``__dlpack__`` attribute, or a DLPack capsule): The tensor or DLPack capsule to convert. If ``ext_tensor`` is a tensor (or ndarray) object, it must support the ``__dlpack__`` protocol (i.e., have a ``ext_tensor.__dlpack__`` method). Otherwise ``ext_tensor`` may be a DLPack capsule, which is an opaque ``PyCapsule`` instance, typically produced by a ``to_dlpack`` function or method. Examples:: >>> import torch.utils.dlpack >>> t = torch.arange(4) # Convert a tensor directly (supported in PyTorch >= 1.10) >>> t2 = torch.from_dlpack(t) >>> t2[:2] = -1 # show that memory is shared >>> t2 tensor([-1, -1, 2, 3]) >>> t tensor([-1, -1, 2, 3]) # The old-style DLPack usage, with an intermediate capsule object >>> capsule = torch.utils.dlpack.to_dlpack(t) >>> capsule <capsule object ""dltensor"" at ...> >>> t3 = torch.from_dlpack(capsule) >>> t3 tensor([-1, -1, 2, 3]) >>> t3[0] = -9 # now we're sharing memory between 3 tensors >>> t3 tensor([-9, -1, 2, 3]) >>> t2 tensor([-9, -1, 2, 3]) >>> t tensor([-9, -1, 2, 3])",0.9693198
tensorflow.python.summary.tb_summary.image,"Write an image summary. See also `tf.summary.scalar`, `tf.summary.SummaryWriter`. Writes a collection of images to the current default summary writer. Data appears in TensorBoard's 'Images' dashboard. Like `tf.summary.scalar` points, each collection of images is associated with a `step` and a `name`. All the image collections with the same `name` constitute a time series of image collections. This example writes 2 random grayscale images: python w = tf.summary.create_file_writer('test/logs') with w.as_default(): image1 = tf.random.uniform(shape=[8, 8, 1]) image2 = tf.random.uniform(shape=[8, 8, 1]) tf.summary.image(""grayscale_noise"", [image1, image2], step=0) To avoid clipping, data should be converted to one of the following: - floating point values in the range [0,1], or - uint8 values in the range [0,255] python # Convert the original dtype=int32 `Tensor` into `dtype=float64`. rgb_image_float = tf.constant([ [[1000, 0, 0], [0, 500, 1000]], ]) / 1000 tf.summary.image(""picture"", [rgb_image_float], step=0) # Convert original dtype=uint8 `Tensor` into proper range. rgb_image_uint8 = tf.constant([ [[1, 1, 0], [0, 0, 1]], ], dtype=tf.uint8) * 255 tf.summary.image(""picture"", [rgb_image_uint8], step=1) Arguments: name: A name for this summary. The summary tag used for TensorBoard will be this name prefixed by any active name scopes. data: A `Tensor` representing pixel data with shape `[k, h, w, c]`, where `k` is the number of images, `h` and `w` are the height and width of the images, and `c` is the number of channels, which should be 1, 2, 3, or 4 (grayscale, grayscale with alpha, RGB, RGBA). Any of the dimensions may be statically unknown (i.e., `None`). Floating point data will be clipped to the range [0,1]. Other data types will be clipped into an allowed range for safe casting to uint8, using `tf.image.convert_image_dtype`. step: Explicit `int64`-castable monotonic step value for this summary. If omitted, this defaults to `tf.summary.experimental.get_step()`, which ",torch.utils.tensorboard.writer.add_image,"Add image data to summary. Note that this requires the ``pillow`` package. Args: tag (str): Data identifier img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data global_step (int): Global step value to record walltime (float): Optional override default walltime (time.time()) seconds after epoch of event dataformats (str): Image data format specification of the form CHW, HWC, HW, WH, etc. Shape: img_tensor: Default is :math:`(3, H, W)`. You can use ``torchvision.utils.make_grid()`` to convert a batch of tensor into 3xHxW format or call ``add_images`` and let us do the job. Tensor with :math:`(1, H, W)`, :math:`(H, W)`, :math:`(H, W, 3)` is also suitable as long as corresponding ``dataformats`` argument is passed, e.g. ``CHW``, ``HWC``, ``HW``. Examples:: from torch.utils.tensorboard import SummaryWriter import numpy as np img = np.zeros((3, 100, 100)) img[0] = np.arange(0, 10000).reshape(100, 100) / 10000 img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC = np.zeros((100, 100, 3)) img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000 writer = SummaryWriter() writer.add_image('my_image', img, 0) # If you have non-default dimension setting, set the dataformats argument. writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC') writer.close() Expected result: .. image:: _static/img/tensorboard/add_image.png :scale: 50 %",0.96841395
tensorflow.python.ops.tensor_array_ops.split,Split the values of a `Tensor` into the TensorArray. Args: value: (N+1)-D. Tensor of type `dtype`. The Tensor to split. lengths: 1-D. int32 vector with the lengths to use when splitting `value` along its first dimension. name: A name for the operation (optional). Returns: A new TensorArray object with flow that ensures the split occurs. Use this object for all subsequent operations. Raises: ValueError: if the shape inference fails.,torch.utils.tensorboard.summary.tensor_proto,"Outputs a `Summary` protocol buffer containing the full tensor. The generated Summary has a Tensor.proto containing the input Tensor. Args: name: A name for the generated node. Will also serve as the series name in TensorBoard. tensor: Tensor to be converted to protobuf Returns: A tensor protobuf in a `Summary` protobuf. Raises: ValueError: If tensor is too big to be converted to protobuf, or tensor data type is not supported",0.96840703
tensorflow.python.framework.test_util.run_in_graph_and_eager_modes,"Execute the decorated test with and without enabling eager execution. This function returns a decorator intended to be applied to test methods in a `tf.test.TestCase` class. Doing so will cause the contents of the test method to be executed twice - once normally, and once with eager execution enabled. This allows unittests to confirm the equivalence between eager and graph execution (see `tf.compat.v1.enable_eager_execution`). For example, consider the following unittest: python class MyTests(tf.test.TestCase): @run_in_graph_and_eager_modes def test_foo(self): x = tf.constant([1, 2]) y = tf.constant([3, 4]) z = tf.add(x, y) self.assertAllEqual([4, 6], self.evaluate(z)) if __name__ == ""__main__"": tf.test.main() This test validates that `tf.add()` has the same behavior when computed with eager execution enabled as it does when constructing a TensorFlow graph and executing the `z` tensor in a session. `deprecated_graph_mode_only`, `run_v1_only`, `run_v2_only`, and `run_in_graph_and_eager_modes` are available decorators for different v1/v2/eager/graph combinations. Args: func: function to be annotated. If `func` is None, this method returns a decorator the can be applied to a function. If `func` is not None this returns the decorator applied to `func`. config: An optional config_pb2.ConfigProto to use to configure the session when executing graphs. use_gpu: If True, attempt to run as many operations as possible on GPU. assert_no_eager_garbage: If True, sets DEBUG_SAVEALL on the garbage collector and asserts that no extra garbage has been created when running the test with eager execution enabled. This will fail if there are reference cycles (e.g. a = []; a.append(a)). Off by default because some tests may create garbage for legitimate reasons (e.g. they define a class which inherits from `object`), and because DEBUG_SAVEALL is sticky in some Python interpreters (meaning that tests which rely on objects being collected elsewhere in the unit test file will not work). Addi",torch.library.opcheck,"Given an operator and some sample arguments, tests if the operator is registered correctly. That is, when you use the torch.library/TORCH_LIBRARY APIs to create a custom op, you specified metadata (e.g. mutability info) about the custom op and these APIs require that the functions you pass them satisfy certain properties (e.g. no data pointer access in the fake/meta/abstract kernel) ``opcheck`` tests these metadata and properties. Concretely, we test the following: - test_schema: if the operator's schema is correct. - test_autograd_registration: if autograd was registered correctly. - test_faketensor: If the operator has a FakeTensor kernel (and if it is correct). The FakeTensor kernel is necessary ( but not sufficient) for the operator to work with PyTorch compilation APIs (torch.compile/export/FX). - test_aot_dispatch_dynamic: If the operator has correct behavior with PyTorch compilation APIs (torch.compile/export/FX). This checks that the outputs (and gradients, if applicable) are the same under eager-mode PyTorch and torch.compile. This test is a superset of ``test_faketensor``. For best results, please call ``opcheck`` multiple times with a representative set of inputs. If your operator supports autograd, please use ``opcheck`` with inputs with ``requires_grad = True``; if your operator supports multiple devices (e.g. CPU and CUDA), please use ``opcheck`` with inputs on all supported devices. Args: op: The operator. Must either be a function decorated with :func:`torch.library.custom_op` or an OpOverload/OpOverloadPacket found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo) args: The args to the operator kwargs: The kwargs to the operator test_utils: Tests that we should run. Default: all of them. Example: (""test_schema"", ""test_faketensor"") raise_exception: If we should raise an exception on the first error. If False, we will return a dict with information on if each test passed or not. .. warning:: opcheck and :func:`torch.autograd.gradcheck` tes",0.9680741
tensorflow.python.autograph.pyct.inspect_utils.isbuiltin,Returns True if the argument is a built-in function.,torch.distributed.pipelining.stage.is_first,Returns true if this stage is the first stage in the pipeline.,0.9679145
tensorflow.python.ops.image_ops_impl.crop_and_resize_v2,"Extracts crops from the input image tensor and resizes them. Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling (possibly with aspect ratio change) to a common output size specified by `crop_size`. This is more general than the `crop_to_bounding_box` op which extracts a fixed size slice from the input image and does not allow resizing or aspect ratio change. The crops occur first and then the resize. Returns a tensor with `crops` from the input `image` at positions defined at the bounding box locations in `boxes`. The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to a fixed `size = [crop_height, crop_width]`. The result is a 4-D tensor `[num_boxes, crop_height, crop_width, depth]`. The resizing is corner aligned. In particular, if `boxes = [[0, 0, 1, 1]]`, the method will give identical results to using `tf.compat.v1.image.resize_bilinear()` or `tf.compat.v1.image.resize_nearest_neighbor()`(depends on the `method` argument) with `align_corners=True`. Args: image: A 4-D tensor of shape `[batch, image_height, image_width, depth]`. Both `image_height` and `image_width` need to be positive. boxes: A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor specifies the coordinates of a box in the `box_ind[i]` image and is specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value of `y` is mapped to the image coordinate at `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image height is mapped to `[0, image_height - 1]` in image height coordinates. We do allow `y1` > `y2`, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the `[0, 1]` range are allowed, in which case we use `extrapolation_value` to extrapolate the input image values. box_indices: A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.",torch.nn.functional.upsample,"Upsample input. Provided tensor is upsampled to either the given :attr:`size` or the given :attr:`scale_factor` .. warning:: This function is deprecated in favor of :func:`torch.nn.functional.interpolate`. This is equivalent with ``nn.functional.interpolate(...)``. Note: {backward_reproducibility_note} The algorithm used for upsampling is determined by :attr:`mode`. Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for upsampling are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only) Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` .. note:: With ``mode='bicubic'``, it's possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call ``result.clamp(min=0, max=255)",0.96780694
tensorflow.python.framework.tensor_util.make_tensor_proto,"Create a TensorProto. In TensorFlow 2.0, representing tensors as protos should no longer be a common workflow. That said, this utility function is still useful for generating TF Serving request protos: python request = tensorflow_serving.apis.predict_pb2.PredictRequest() request.model_spec.name = ""my_model"" request.model_spec.signature_name = ""serving_default"" request.inputs[""images""].CopyFrom(tf.make_tensor_proto(X_new)) `make_tensor_proto` accepts ""values"" of a python scalar, a python list, a numpy ndarray, or a numpy scalar. If ""values"" is a python scalar or a python list, make_tensor_proto first convert it to numpy ndarray. If dtype is None, the conversion tries its best to infer the right numpy data type. Otherwise, the resulting numpy array has a compatible data type with the given dtype. In either case above, the numpy ndarray (either the caller provided or the auto-converted) must have the compatible type with dtype. `make_tensor_proto` then converts the numpy array to a tensor proto. If ""shape"" is None, the resulting tensor proto represents the numpy array precisely. Otherwise, ""shape"" specifies the tensor's shape and the numpy array can not have more elements than what ""shape"" specifies. Args: values: Values to put in the TensorProto. dtype: Optional tensor_pb2 DataType value. shape: List of integers representing the dimensions of tensor. verify_shape: Boolean that enables verification of a shape of values. allow_broadcast: Boolean that enables allowing scalars and 1 length vector broadcasting. Cannot be true when verify_shape is true. Returns: A `TensorProto`. Depending on the type, it may contain data in the ""tensor_content"" attribute, which is not directly useful to Python programs. To access the values you should convert the proto back to a numpy ndarray with `tf.make_ndarray(proto)`. If `values` is a `TensorProto`, it is immediately returned; `dtype` and `shape` are ignored. Raises: TypeError: if unsupported types are provided. ValueError: if argument",torch.onnx.utils.export,"Exports a model into ONNX format. If ``model`` is not a :class:`torch.jit.ScriptModule` nor a :class:`torch.jit.ScriptFunction`, this runs ``model`` once in order to convert it to a TorchScript graph to be exported (the equivalent of :func:`torch.jit.trace`). Thus this has the same limited support for dynamic control flow as :func:`torch.jit.trace`. Args: model (:class:`torch.nn.Module`, :class:`torch.jit.ScriptModule` or :class:`torch.jit.ScriptFunction`): the model to be exported. args (tuple or torch.Tensor): args can be structured either as: 1. ONLY A TUPLE OF ARGUMENTS:: args = (x, y, z) The tuple should contain model inputs such that ``model(*args)`` is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in the tuple. 2. A TENSOR:: args = torch.Tensor([1]) This is equivalent to a 1-ary tuple of that Tensor. 3. A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:: args = ( x, { ""y"": input_y, ""z"": input_z } ) All but the last element of the tuple will be passed as non-keyword arguments, and named arguments will be set from the last element. If a named argument is not present in the dictionary, it is assigned the default value, or None if a default value is not provided. .. note:: If a dictionary is the last element of the args tuple, it will be interpreted as containing named arguments. In order to pass a dict as the last non-keyword arg, provide an empty dict as the last element of the args tuple. For example, instead of:: torch.onnx.export( model, ( x, # WRONG: will be interpreted as named arguments {y: z} ), ""test.onnx.pb"" ) Write:: torch.onnx.export( model, ( x, {y: z}, {} ), ""test.onnx.pb"" ) f: a file-like object (such that ``f.fileno()`` returns a file descriptor) or a string containing a file name. A binary protocol buffer will be written to this file. export_params (bool, default True): if True, all paramet",0.96752936
tensorflow.python.ops.math_ops.accumulate_n,"Returns the element-wise sum of a list of tensors. Optionally, pass `shape` and `tensor_dtype` for shape and type checking, otherwise, these are inferred. For example: >>> a = tf.constant([[1, 2], [3, 4]]) >>> b = tf.constant([[5, 0], [0, 6]]) >>> tf.math.accumulate_n([a, b, a]).numpy() array([[ 7, 4], [ 6, 14]], dtype=int32) >>> # Explicitly pass shape and type >>> tf.math.accumulate_n( ... [a, b, a], shape=[2, 2], tensor_dtype=tf.int32).numpy() array([[ 7, 4], [ 6, 14]], dtype=int32) Note: The input must be a list or tuple. This function does not handle `IndexedSlices` See Also: * `tf.reduce_sum(inputs, axis=0)` - This performe the same mathematical operation, but `tf.add_n` may be more efficient because it sums the tensors directly. `reduce_sum` on the other hand calls `tf.convert_to_tensor` on the list of tensors, unncessairly stacking them into a single tensor before summing. * `tf.add_n` - This is another python wrapper for the same Op. It has nearly identical functionality. Args: inputs: A list of `Tensor` objects, each with same shape and type. shape: Expected shape of elements of `inputs` (optional). Also controls the output shape of this op, which may affect type inference in other ops. A value of `None` means ""infer the input shape from the shapes in `inputs`"". tensor_dtype: Expected data type of `inputs` (optional). A value of `None` means ""infer the input dtype from `inputs[0]`"". name: A name for the operation (optional). Returns: A `Tensor` of same shape and type as the elements of `inputs`. Raises: ValueError: If `inputs` don't all have same shape and dtype or the shape cannot be inferred.",torch.autograd.functional.vhp,"Compute the dot product between vector ``v`` and Hessian of a given scalar function at a specified point. Args: func (function): a Python function that takes Tensor inputs and returns a Tensor with a single element. inputs (tuple of Tensors or Tensor): inputs to the function ``func``. v (tuple of Tensors or Tensor): The vector for which the vector Hessian product is computed. Must be the same size as the input of ``func``. This argument is optional when ``func``'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single ``1``. create_graph (bool, optional): If ``True``, both the output and result will be computed in a differentiable way. Note that when ``strict`` is ``False``, the result can not require gradients or be disconnected from the inputs. Defaults to ``False``. strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to ``False``. Returns: output (tuple): tuple with: func_output (tuple of Tensors or Tensor): output of ``func(inputs)`` vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD) >>> def pow_reducer(x): ... return x.pow(3).sum() >>> inputs = torch.rand(2, 2) >>> v = torch.ones(2, 2) >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> vhp(pow_reducer, inputs, v) (tensor(0.5591), tensor([[1.0689, 1.2431], [3.0989, 4.4456]])) >>> vhp(pow_reducer, inputs, v, create_graph=True) (tensor(0.5591, grad_fn=<SumBackward0>), tensor([[1.0689, 1.2431], [3.0989, 4.4456]], grad_fn=<MulBackward0>)) >>> def pow_adder_reducer(x, y): ... return (2 * x.pow(2) + 3 * y.pow(2)).sum() >>> inputs = (torch.rand(2), torch.rand(2)) >>> v = (torch.zeros(2), torch.ones(2)) >>> vhp(pow_adder_reducer, inputs, v) (tens",0.96749514
tensorflow.python.data.ops.dataset_ops.rejection_resample,"Resamples elements to reach a target distribution. Note: This implementation can reject **or repeat** elements in order to reach the `target_dist`. So, in some cases, the output `Dataset` may be larger than the input `Dataset`. >>> initial_dist = [0.6, 0.4] >>> n = 1000 >>> elems = np.random.choice(len(initial_dist), size=n, p=initial_dist) >>> dataset = tf.data.Dataset.from_tensor_slices(elems) >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n Following from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4. >>> target_dist = [0.5, 0.5] >>> dataset = dataset.rejection_resample( ... class_func=lambda x: x, ... target_dist=target_dist, ... initial_dist=initial_dist) >>> dataset = dataset.map(lambda class_func_result, data: data) >>> zero, one = np.bincount(list(dataset.as_numpy_iterator())) / n Following from `target_dist`, `zero` is ~0.5 and `one` is ~0.5. Args: class_func: A function mapping an element of the input dataset to a scalar `tf.int32` tensor. Values should be in `[0, num_classes)`. target_dist: A floating point type tensor, shaped `[num_classes]`. initial_dist: (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion. seed: (Optional.) Python integer seed for the resampler. name: (Optional.) A name for the tf.data operation. Returns: A new `Dataset` with the transformation applied as described above.",torch.ao.nn.quantized.functional.conv1d,"Applies a 1D convolution over a quantized 1D input composed of several input planes. See :class:`~torch.ao.nn.quantized.Conv1d` for details and output shape. Args: input: quantized input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)` weight: quantized filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , iW)` bias: **non-quantized** bias tensor of shape :math:`(\text{out\_channels})`. The tensor type must be `torch.float`. stride: the stride of the convolving kernel. Can be a single number or a tuple `(sW,)`. Default: 1 padding: implicit paddings on both sides of the input. Can be a single number or a tuple `(padW,)`. Default: 0 dilation: the spacing between kernel elements. Can be a single number or a tuple `(dW,)`. Default: 1 groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the number of groups. Default: 1 padding_mode: the padding mode to use. Only ""zeros"" is supported for quantized convolution at the moment. Default: ""zeros"" scale: quantization scale for the output. Default: 1.0 zero_point: quantization zero_point for the output. Default: 0 dtype: quantization data type to use. Default: ``torch.quint8`` Examples:: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_QENGINE) >>> from torch.ao.nn.quantized import functional as qF >>> filters = torch.randn(33, 16, 3, dtype=torch.float) >>> inputs = torch.randn(20, 16, 50, dtype=torch.float) >>> bias = torch.randn(33, dtype=torch.float) >>> >>> scale, zero_point = 1.0, 0 >>> dtype_inputs = torch.quint8 >>> dtype_filters = torch.qint8 >>> >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters) >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs) >>> qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)",0.9674297
tensorflow.python.distribute.distribute_lib.initializer,Returns a list of ops that initialize the iterator.,torch.distributed.pipelining.stage.get_fwd_recv_ops,Returns a list of ops that are needed to receive the input arguments for this stage.,0.967285
tensorflow.python.framework.test_util.is_ubsan_enabled,Check if UBSAN is enabled.,torch.distributed.distributed_c10d.is_nccl_available,Check if the NCCL backend is available.,0.9672468
tensorflow.python.tools.saved_model_aot_compile.freeze_model,"Freeze a `MetaGraphDef` in preparation for tfcompile`. The graph is always optimized with grappler, and optionally (by default) variables are frozen as constants, before compilation happens. Args: checkpoint_path: Python string. Path to checkpoints/variables. meta_graph_def: Instance of `MetaGraphDef`. output_prefix: Python string. Path prefix for outputs. signature_def_key: String, the signature_def to use in the SavedModel. variables_to_feed: A list of strings, the variables that will be fed by the user; these won't be frozen. If `None`, then we will extract all the variables in the graph and mark them as to-feed. The default behavior is an empty tuple: all variables must be frozen. Returns: a pair containing the path to the frozen model and the path to the config. Raises: RuntimeError: If tensorflow was not built with XLA. ImportError: If tensorflow was built with XLA but there was another issue importing the tfcompile python wrapper. ValueError: If `meta_graph_def.signature_def[signature_def_key]` is missing or has empty outputs.",torch._functorch.aot_autograd.create_aot_dispatcher_function,"Traces the forward and backward graphs of the attr:`flat_fn` to generate a joint graph. The joint graph is an Fx graph with Aten ops. Please refer to the tracing mechanism to understand the graph capturing details. The joint graph is then passed through attr:`partition_fn` to isolate the forward and backward portions, which are then respectively compiled via the provided attr:`fw_compiler` and attr:`bw_compiler`. The resulting compiled forward and backward graphs are then wrapped up in a ``torch.autograd.Function`` object. The calling convention here is that the first aot_config.num_params_buffers inputs in flat_args are parameters and buffers, and the rest are inputs. We use this to assume that parameters/buffer's shapes don't change. Note: this function is used both by aot_function and aot_export (controlled by aot_config.is_export) When aot_config.is_export is True, we return an FX graph + metadata When aot_config.is_export is False, we return an ordinary runtime function",0.9670848
tensorflow.python.client.session.make_callable,"Returns a Python callable that runs a particular step. The returned callable will take `len(feed_list)` arguments whose types must be compatible feed values for the respective elements of `feed_list`. For example, if element `i` of `feed_list` is a `tf.Tensor`, the `i`th argument to the returned callable must be a numpy ndarray (or something convertible to an ndarray) with matching element type and shape. See `tf.Session.run` for details of the allowable feed key and value types. The returned callable will have the same return type as `tf.Session.run(fetches, ...)`. For example, if `fetches` is a `tf.Tensor`, the callable will return a numpy ndarray; if `fetches` is a `tf.Operation`, it will return `None`. Args: fetches: A value or list of values to fetch. See `tf.Session.run` for details of the allowable fetch types. feed_list: (Optional.) A list of `feed_dict` keys. See `tf.Session.run` for details of the allowable feed key types. accept_options: (Optional.) If `True`, the returned `Callable` will be able to accept `tf.compat.v1.RunOptions` and `tf.compat.v1.RunMetadata` as optional keyword arguments `options` and `run_metadata`, respectively, with the same syntax and semantics as `tf.Session.run`, which is useful for certain use cases (profiling and debugging) but will result in measurable slowdown of the `Callable`'s performance. Default: `False`. Returns: A function that when called will execute the step defined by `feed_list` and `fetches` in this session. Raises: TypeError: If `fetches` or `feed_list` cannot be interpreted as arguments to `tf.Session.run`.",torch.nn.modules.module.register_full_backward_hook,"Register a backward hook on the module. The hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr:`grad_input` in subsequent computations. :attr:`grad_input` will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Args: hook (Callable): The user-defined hook to be registered. prepend (bool): If true, the provided ``hook`` will be fired before all existing ``backward`` hooks on this :class:`torch.nn.modules.Module`. Otherwise, the provided ``hook`` will be fired after all existing ``backward`` hooks on this :class:`torch.nn.modules.Module`. Note that global ``backward`` hooks registered with :func:`register_module_full_backward_hook` will fire before all hooks registered by this method. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()``",0.9668505
tensorflow.python.lib.io.file_io.readlines,Returns all lines from the file in a list.,torch._functorch._aot_autograd.logging_utils.get_aot_graph_name,Returns the name of the graph being compiled.,0.9668396
tensorflow.python.checkpoint.checkpoint_view.descendants,Returns a list of trackables by node_id attached to obj.,torch._export.utils.nodes_filter,Returns the nodes that match the node_call_back as a list.,0.96662337
tensorflow.python.ops.distributions.special_math.erfinv,"The inverse function for erf, the error function. Args: x: `Tensor` of type `float32`, `float64`. name: Python string. A name for the operation (default=""erfinv""). Returns: x: `Tensor` with `dtype=x.dtype`. Raises: TypeError: if `x` is not floating-type.",torch.onnx._internal.fx.type_utils.from_python_type_to_onnx_tensor_element_type,"Converts a Python type to the corresponding ONNX tensor element type. For example, `from_python_type_to_onnx_tensor_element_type(float)` returns `onnx.TensorProto.FLOAT`. Args: type (type): The Python type to convert. Returns: int: The corresponding ONNX tensor element type.",0.96658975
tensorflow.python.keras.engine.training_utils_v1.cast_if_floating_dtype,Casts the given data tensors to the default floating point type. Casts only if the input is already a floating point type. Args: x: tensor or list/tuple of tensors. dtype: The dtype to which Tensors should be cast. Returns: Converted input.,torch.nn.parallel.comm.reduce_add,"Sum tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout. Args: inputs (Iterable[Tensor]): an iterable of tensors to add. destination (int, optional): a device on which the output will be placed (default: current device). Returns: A tensor containing an elementwise sum of all inputs, placed on the :attr:`destination` device.",0.966437
tensorflow.python.ops.linalg.linear_operator.tensor_rank_tensor,"Rank (in the sense of tensors) of matrix corresponding to this operator. If this operator acts like the batch matrix `A` with `A.shape = [B1,...,Bb, M, N]`, then this returns `b + 2`. Args: name: A name for this `Op`. Returns: `int32` `Tensor`, determined at runtime.",torch.quasirandom.draw_base2,"Function to draw a sequence of :attr:`2**m` points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is :math:`(2**m, dimension)`. Args: m (Int): The (base2) exponent of the number of points to draw. out (Tensor, optional): The output tensor dtype (:class:`torch.dtype`, optional): the desired data type of the returned tensor. Default: ``None``",0.9664279
tensorflow.python.keras.legacy_tf_layers.core.dropout,"Applies Dropout to the input. Dropout consists in randomly setting a fraction `rate` of input units to 0 at each update during training time, which helps prevent overfitting. The units that are kept are scaled by `1 / (1 - rate)`, so that their sum is unchanged at training time and inference time. Args: inputs: Tensor input. rate: The dropout rate, between 0 and 1. E.g. ""rate=0.1"" would drop out 10% of input units. noise_shape: 1D tensor of type `int32` representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape `(batch_size, timesteps, features)`, and you want the dropout mask to be the same for all timesteps, you can use `noise_shape=[batch_size, 1, features]`. seed: A Python integer. Used to create random seeds. See `tf.compat.v1.set_random_seed` for behavior. training: Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder). Whether to return the output in training mode (apply dropout) or in inference mode (return the input untouched). name: The name of the layer (string). Returns: Output tensor. Raises: ValueError: if eager execution is enabled.",torch.ao.nn.quantized.functional.avg_pool2d,"Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size :math:`sH \times sW` steps. The number of output features is equal to the number of input planes. .. note:: The input quantization parameters propagate to the output. See :class:`~torch.ao.nn.quantized.AvgPool2d` for details and output shape. Args: input: quantized input tensor :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)` kernel_size: size of the pooling region. Can be a single number or a tuple `(kH, kW)` stride: stride of the pooling operation. Can be a single number or a tuple `(sH, sW)`. Default: :attr:`kernel_size` padding: implicit zero paddings on both sides of the input. Can be a single number or a tuple `(padH, padW)`. Default: 0 ceil_mode: when True, will use `ceil` instead of `floor` in the formula to compute the output shape. Default: ``False`` count_include_pad: when True, will include the zero-padding in the averaging calculation. Default: ``True`` divisor_override: if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",0.96640134
tensorflow.python.ops.control_flow_case.case_v2,"Create a case operation. See also `tf.switch_case`. The `pred_fn_pairs` parameter is a list of pairs of size N. Each pair contains a boolean scalar tensor and a python callable that creates the tensors to be returned if the boolean evaluates to True. `default` is a callable generating a list of tensors. All the callables in `pred_fn_pairs` as well as `default` (if provided) should return the same number and types of tensors. If `exclusive==True`, all predicates are evaluated, and an exception is thrown if more than one of the predicates evaluates to `True`. If `exclusive==False`, execution stops at the first predicate which evaluates to True, and the tensors generated by the corresponding function are returned immediately. If none of the predicates evaluate to True, this operation returns the tensors generated by `default`. `tf.case` supports nested structures as implemented in `tf.nest`. All of the callables must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by a callable, they are implicitly unpacked to single values. This behavior is disabled by passing `strict=True`. @compatibility(v2) `pred_fn_pairs` could be a dictionary in v1. However, tf.Tensor and tf.Variable are no longer hashable in v2, so cannot be used as a key for a dictionary. Please use a list or a tuple instead. @end_compatibility **Example 1:** Pseudocode: if (x < y) return 17; else return 23; Expressions: python f1 = lambda: tf.constant(17) f2 = lambda: tf.constant(23) r = tf.case([(tf.less(x, y), f1)], default=f2) **Example 2:** Pseudocode: if (x < y && x > z) raise OpError(""Only one predicate may evaluate to True""); if (x < y) return 17; else if (x > z) return 23; else return -1; Expressions: python def f1(): return tf.constant(17) def f2(): return tf.constant(23) def f3(): return tf.constant(-1) r = tf.case([(tf.less(x, y), f1), (tf.greater(x, z), f2)], default=f3, exclusive=Tru",torch._higher_order_ops.cond.cond,"Conditionally applies `true_fn` or `false_fn`. .. warning:: `torch.cond` is a prototype feature in PyTorch. It has limited support for input and output types and doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch. Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype `cond` is structured control flow operator. That is, it is like a Python if-statement, but has restrictions on `true_fn`, `false_fn`, and `operands` that enable it to be capturable using torch.compile and torch.export. Assuming the constraints on `cond`'s arguments are met, `cond` is equivalent to the following:: def cond(pred, true_branch, false_branch, operands): if pred: return true_branch(*operands) else: return false_branch(*operands) Args: pred (Union[bool, torch.Tensor]): A boolean expression or a tensor with one element, indicating which branch function to apply. true_fn (Callable): A callable function (a -> b) that is within the scope that is being traced. false_fn (Callable): A callable function (a -> b) that is within the scope that is being traced. The true branch and false branch must have consistent input and outputs, meaning the inputs have to be the same, and the outputs have to be the same type and shape. operands (Tuple of possibly nested dict/list/tuple of torch.Tensor): A tuple of inputs to the true/false functions. Example:: def true_fn(x: torch.Tensor): return x.cos() def false_fn(x: torch.Tensor): return x.sin() return cond(x.shape[0] > 4, true_fn, false_fn, (x,)) Restrictions: - The conditional statement (aka `pred`) must meet one of the following constraints: - It's a `torch.Tensor` with only one element, and torch.bool dtype - It's a boolean expression, e.g. `x.shape[0] > 10` or `x.dim() > 1 and x.shape[1] > 10` - The branch function (aka `true_fn`/`false_fn`) must meet all of the following constraints: - The function signature must match with oper",0.96635956
tensorflow.python.ops.control_flow_ops.outer_context,Return the context containing this context.,torch.storage.tolist,Return a list containing the elements of this storage.,0.96634525
tensorflow.python.client.session.run,"Runs operations and evaluates tensors in `fetches`. This method runs one ""step"" of TensorFlow computation, by running the necessary graph fragment to execute every `Operation` and evaluate every `Tensor` in `fetches`, substituting the values in `feed_dict` for the corresponding input values. The `fetches` argument may be a single graph element, or an arbitrarily nested list, tuple, namedtuple, dict, or OrderedDict containing graph elements at its leaves. A graph element can be one of the following types: * A `tf.Operation`. The corresponding fetched value will be `None`. * A `tf.Tensor`. The corresponding fetched value will be a numpy ndarray containing the value of that tensor. * A `tf.sparse.SparseTensor`. The corresponding fetched value will be a `tf.compat.v1.SparseTensorValue` containing the value of that sparse tensor. * A `get_tensor_handle` op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor. * A `string` which is the name of a tensor or operation in the graph. The value returned by `run()` has the same shape as the `fetches` argument, where the leaves are replaced by the corresponding values returned by TensorFlow. Example: python a = tf.constant([10, 20]) b = tf.constant([1.0, 2.0]) # 'fetches' can be a singleton v = session.run(a) # v is the numpy array [10, 20] # 'fetches' can be a list. v = session.run([a, b]) # v is a Python list with 2 numpy arrays: the 1-D array [10, 20] and the # 1-D array [1.0, 2.0] # 'fetches' can be arbitrary lists, tuples, namedtuple, dicts: MyData = collections.namedtuple('MyData', ['a', 'b']) v = session.run({'k1': MyData(a, b), 'k2': [b, a]}) # v is a dict with # v['k1'] is a MyData namedtuple with 'a' (the numpy array [10, 20]) and # 'b' (the numpy array [1.0, 2.0]) # v['k2'] is a list with the numpy array [1.0, 2.0] and the numpy array # [10, 20]. The optional `feed_dict` argument allows the caller to override the value of tensors in the graph. Each key in `feed_dict` can be one ",torch._functorch.eager_transforms.jacrev,"Computes the Jacobian of ``func`` with respect to the arg(s) at index ``argnum`` using reverse mode autodiff .. note:: Using :attr:`chunk_size=1` is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of :func:`vmap` are not applicable. Args: func (function): A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors argnums (int or Tuple[int]): Optional, integer or tuple of integers, saying which arguments to get the Jacobian with respect to. Default: 0. has_aux (bool): Flag indicating that ``func`` returns a ``(output, aux)`` tuple where the first element is the output of the function to be differentiated and the second element is auxiliary objects that will not be differentiated. Default: False. chunk_size (None or int): If None (default), use the maximum chunk size (equivalent to doing a single vmap over vjp to compute the jacobian). If 1, then compute the jacobian row-by-row with a for-loop. If not None, then compute the jacobian :attr:`chunk_size` rows at a time (equivalent to doing multiple vmap over vjp). If you run into memory issues computing the jacobian, please try to specify a non-None chunk_size. Returns: Returns a function that takes in the same inputs as ``func`` and returns the Jacobian of ``func`` with respect to the arg(s) at ``argnums``. If ``has_aux is True``, then the returned function instead returns a ``(jacobian, aux)`` tuple where ``jacobian`` is the Jacobian and ``aux`` is auxiliary objects returned by ``func``. A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian >>> from torch.func import jacrev >>> x = torch.randn(5) >>> jacobian = jacrev(torch.sin)(x) >>> expected = torch.diag(torch.cos(x)) >>> assert torch.allclose(jacobian, expected) If you would like to compute the output of the function as well as the jacobian of the function, use the ``has_aux`` flag to return the output as an auxiliary object: >>> from to",0.9661969
tensorflow.python.ops.linalg.linear_operator_addition.can_add,Returns `True` if this `Adder` can add `op1` and `op2`. Else `False`.,torch.ao.quantization.qconfig.qconfig_equals,"Returns `True` if `q1` equals `q2`, and `False` otherwise.",0.966161
tensorflow.python.ops.nn_ops.max_pool,"Performs the max pooling on the input. Args: value: A 4-D `Tensor` of the format specified by `data_format`. ksize: An int or list of `ints` that has length `1`, `2` or `4`. The size of the window for each dimension of the input tensor. strides: An int or list of `ints` that has length `1`, `2` or `4`. The stride of the sliding window for each dimension of the input tensor. padding: Either the `string` `""SAME""` or `""VALID""` indicating the type of padding algorithm to use, or a list indicating the explicit paddings at the start and end of each dimension. When explicit padding is used and data_format is `""NHWC""`, this should be in the form `[[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]]`. When explicit padding used and data_format is `""NCHW""`, this should be in the form `[[0, 0], [0, 0], [pad_top, pad_bottom], [pad_left, pad_right]]`. When using explicit padding, the size of the paddings cannot be greater than the sliding window size. data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported. name: Optional name for the operation. input: Alias for value. Returns: A `Tensor` of format specified by `data_format`. The max pooled output tensor.",torch.utils.tensorboard.summary.image,"Output a `Summary` protocol buffer with images. The summary has up to `max_images` summary values containing images. The images are built from `tensor` which must be 3-D with shape `[height, width, channels]` and where `channels` can be: * 1: `tensor` is interpreted as Grayscale. * 3: `tensor` is interpreted as RGB. * 4: `tensor` is interpreted as RGBA. The `name` in the outputted Summary.Value protobufs is generated based on the name, with a suffix depending on the max_outputs setting: * If `max_outputs` is 1, the summary value tag is '*name*/image'. * If `max_outputs` is greater than 1, the summary value tags are generated sequentially as '*name*/image/0', '*name*/image/1', etc. Args: tag: A name for the generated node. Will also serve as a series name in TensorBoard. tensor: A 3-D `uint8` or `float32` `Tensor` of shape `[height, width, channels]` where `channels` is 1, 3, or 4. 'tensor' can either have values in [0, 1] (float32) or [0, 255] (uint8). The image() function will scale the image values to [0, 255] by applying a scale factor of either 1 (uint8) or 255 (float32). Out-of-range values will be clipped. Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer.",0.96616054
tensorflow.python.distribute.distribute_lib.update_non_slot,"Runs `fn(*args, **kwargs)` on `colocate_with` devices. Used to update non-slot variables. DEPRECATED: TF 1.x ONLY. Args: colocate_with: Devices returned by `non_slot_devices()`. fn: Function to execute. args: Tuple or list. Positional arguments to pass to `fn()`. kwargs: Dict with keyword arguments to pass to `fn()`. group: Boolean. Defaults to True. If False, the return value will be unwrapped. Returns: Return value of `fn`, possibly merged across devices.",torch.onnx.utils.export_to_pretty_string,"Similar to :func:`export`, but returns a text representation of the ONNX model. Only differences in args listed below. All other args are the same as :func:`export`. Args: add_node_names (bool, default True): Whether or not to set NodeProto.name. This makes no difference unless ``google_printer=True``. google_printer (bool, default False): If False, will return a custom, compact representation of the model. If True will return the protobuf's `Message::DebugString()`, which is more verbose. Returns: A UTF-8 str containing a human-readable representation of the ONNX model.",0.96605355
tensorflow.python.ops.array_ops.identity,"Return a Tensor with the same shape and contents as input. The return value is not the same Tensor as the original, but contains the same values. This operation is fast when used on the same device. For example: >>> a = tf.constant([0.78]) >>> a_identity = tf.identity(a) >>> a.numpy() array([0.78], dtype=float32) >>> a_identity.numpy() array([0.78], dtype=float32) Calling `tf.identity` on a variable will make a Tensor that represents the value of that variable at the time it is called. This is equivalent to calling `<variable>.read_value()`. >>> a = tf.Variable(5) >>> a_identity = tf.identity(a) >>> a.assign_add(1) <tf.Variable ... shape=() dtype=int32, numpy=6> >>> a.numpy() 6 >>> a_identity.numpy() 5 This function can also be used to explicitly transfer tensors between devices. For example, to transfer a tensor in GPU memory back to host memory, one can use: >>> with tf.device(""/gpu:0""): ... x_on_gpu = tf.constant(1) >>> with tf.device(""/cpu:0""): ... x_on_cpu = tf.identity(x_on_gpu) >>> x_on_cpu.device '/job:localhost/replica:0/task:0/device:CPU:0' Args: input: A `Tensor`, a `Variable`, a `CompositeTensor` or anything that can be converted to a tensor using `tf.convert_to_tensor`. name: A name for the operation (optional). Returns: A `Tensor` or CompositeTensor. Has the same type and contents as `input`.",torch._library.abstract_impl.new_dynamic_size,"Constructs a new symint (symbolic int) representing a data-dependent value. This is useful for writing the fake implementation (which is necessary for torch.compile) for a CustomOp where an output Tensor has a size that depends on the data of the input Tensors. Args: min (int): A statically known inclusive lower bound for this symint. Default: 0 max (Optional[int]): A statically known inclusive upper bound for this symint. Default: None .. warning: It is important that the ``min`` and ``max`` (if not None) values are set correctly, otherwise, there will be undefined behavior under torch.compile. The default value of ``min`` is 2 due to torch.compile specializing on 0/1 sizes. You must also verify that your implementation on concrete Tensors (e.g. CPU/CUDA) only returns Tensors where the size that corresponds to the symint also has respects these constraint. The easiest way to do this is to add an assertion in the CPU/CUDA/etc implementation that the size follows these bounds. Example:: >>> # An operator with data-dependent output shape >>> lib = torch.library.Library(""mymodule"", ""FRAGMENT"") >>> lib.define(""mymodule::custom_nonzero(Tensor x) -> Tensor"") >>> >>> @torch.library.register_fake(""mymodule::custom_nonzero"") >>> def _(x): >>> # Number of nonzero-elements is data-dependent. >>> # Since we cannot peek at the data in an fake impl, >>> # we use the ctx object to construct a new symint that >>> # represents the data-dependent size. >>> ctx = torch.library.get_ctx() >>> nnz = ctx.new_dynamic_size() >>> shape = [nnz, x.dim()] >>> result = x.new_empty(shape, dtype=torch.int64) >>> return result >>> >>> @torch.library.impl(lib, ""custom_nonzero"", ""CPU"") >>> def _(x): >>> x_np = x.numpy() >>> res = np.stack(np.nonzero(x_np), axis=1) >>> return torch.tensor(res, device=x.device)",0.9659515
tensorflow.python.tpu.tpu.rewrite_for_inference,"Rewrites `computation` for inference on a TPU system. Other than 'rewriting' the computation to run on a TPU, if using variables in your computation, it moves the ReadVariableOps outside the TPU computation, and adds GuaranteeConst ops just after the ReadVariableOps. This mechanism works only if you are using tf.compat.v1.get_variable() to create and access variables in your tpu computation. You can validate whether this worked, by calling validate_inference_rewrite_for_variables() method immediately after this method to check whether GuaranteeConstOps where added to the graph. Args: computation: A Python function that builds a computation to apply to the input. If the function takes n inputs, 'inputs' should be a list of n tensors. If the function returns m outputs, rewrite will return a list of m tensors. inputs: A list of input tensors or `None` (equivalent to an empty list). infeed_queue: If not `None`, the `InfeedQueue` from which to append a tuple of arguments as inputs to `computation`. device_assignment: if not `None`, a `DeviceAssignment` describing the mapping between logical cores in the computation with physical cores in the TPU topology. May be omitted for a single-core computation, in which case the core attached to task 0, TPU device 0 is used. name: The name of the operator. Returns: A list of output tensors.",torch.distributed.distributed_c10d.all_reduce_coalesced,"WARNING: at this time individual shape checking is not implemented across nodes. For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce operation will proceed without complaint and return erroneous outputs. This lack of shape checking results in significant performance improvements but users of this function should take extra care to ensure that each node passes in tensors whose shapes match across nodes. Reduces each tensor in tensors (residing on the same device) across all machines in such a way that all get the final result. After the call each tensor in tensors is going to bitwise identical in all processes. Complex tensors are supported. Args: tensors (Union[List[Tensor], Tensor]): Input and output of the collective. The function operates in-place. op (Optional[ReduceOp]): One of the values from ``torch.distributed.ReduceOp`` enum. Specifies an operation used for element-wise reductions. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (Optional[bool]): Whether this op should be an async op. Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.",0.9659426
tensorflow.python.ops.variables.gather_nd,"Gather slices from `params` into a Tensor with shape specified by `indices`. See tf.gather_nd for details. Args: indices: A `Tensor`. Must be one of the following types: `int32`, `int64`. Index tensor. name: A name for the operation (optional). Returns: A `Tensor`. Has the same type as `params`.",torch.quasirandom.draw,"Function to draw a sequence of :attr:`n` points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is :math:`(n, dimension)`. Args: n (Int, optional): The length of sequence of points to draw. Default: 1 out (Tensor, optional): The output tensor dtype (:class:`torch.dtype`, optional): the desired data type of the returned tensor. Default: ``None``",0.96591663
tensorflow.python.ops.linalg.linear_operator.range_dimension_tensor,"Dimension (in the sense of vector spaces) of the range of this operator. Determined at runtime. If this operator acts like the batch matrix `A` with `A.shape = [B1,...,Bb, M, N]`, then this returns `M`. Args: name: A name for this `Op`. Returns: `int32` `Tensor`",torch.nn.modules.adaptive.predict,"Return the class with the highest probability for each example in the input minibatch. This is equivalent to ``self.log_prob(input).argmax(dim=1)``, but is more efficient in some cases. Args: input (Tensor): a minibatch of examples Returns: output (Tensor): a class with the highest probability for each example Shape: - Input: :math:`(N, \texttt{in\_features})` - Output: :math:`(N)`",0.96586704
tensorflow.python.kernel_tests.signal.test_util.tflite_convert,"Converts the provided fn to tf.lite model. Args: fn: A callable that expects a list of inputs like input_templates that returns a tensor or structure of tensors. input_templates: A list of Tensors, ndarrays or TensorSpecs describing the inputs that fn expects. The actual values of the Tensors or ndarrays are unused. Returns: The serialized tf.lite model.",torch.onnx._internal.io_adapter.apply,Converts the PyTorch model inputs to exported ONNX model inputs format. Args: model_args: The PyTorch model inputs. model: The PyTorch model. model_kwargs: The PyTorch model keyword inputs. Returns: A sequence of tensors converted from PyTorch model inputs.,0.9658153
tensorflow.python.util.tf_inspect.isanytargetmethod,"Checks if `object` or a TF Decorator wrapped target contains self or cls. This function could be used along with `tf_inspect.getfullargspec` to determine if the first argument of `object` argspec is self or cls. If the first argument is self or cls, it needs to be excluded from argspec when we compare the argspec to the input arguments and, if provided, the tf.function input_signature. Like `tf_inspect.getfullargspec` and python `inspect.getfullargspec`, it does not unwrap python decorators. Args: obj: An method, function, or functool.partial, possibly decorated by TFDecorator. Returns: A bool indicates if `object` or any target along the chain of TF decorators is a method.",torch._numpy.testing.utils.decorate_methods,"Apply a decorator to all methods in a class matching a regular expression. The given decorator is applied to all public methods of `cls` that are matched by the regular expression `testmatch` (``testmatch.search(methodname)``). Methods that are private, i.e. start with an underscore, are ignored. Parameters ---------- cls : class Class whose methods to decorate. decorator : function Decorator to apply to methods testmatch : compiled regexp or str, optional The regular expression. Default value is None, in which case the nose default (``re.compile(r'(?:^|[\b_\.%s-])[Tt]est' % os.sep)``) is used. If `testmatch` is a string, it is compiled to a regular expression first.",0.96575797
tensorflow.python.distribute.cross_device_utils.all_gather,"All-gather a dense tensor. This method must be called inside a tf.function. Args: input_tensor: a dense tensor. It must have the same rank on all replicas, and dimensions other than `axis` need to be the same as well. axis: 0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)). options: an optional tf.distribute.experimental.CommunicationOptions. If provided, it overrides the default options. Returns: The gathered Tensor. Raises: RuntimeError: if called in eager mode.",torch.distributed._shard.sharding_spec.api.shard,"Given a global tensor on src_rank, shard this tensor across ranks within the process group, return a ShardedTensor. Args: tensor (:class:`torch.Tensor`): Tensor needs to be sharded. Keyword args: src_rank (int, optional): The source rank which is used as the ground truth of the data for the parameter that would be sharded and scattered across the rest of the ranks. Default: 0. process_group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Returns: A :class:`ShardedTensor` sharded from the given tensor.",0.9656283
tensorflow.python.framework.test_util.deprecated_graph_mode_only,"Execute the decorated test in graph mode. This is a decorator intended to be applied to tests that are not compatible with eager mode. When this decorator is applied, the test body will be run in an environment where API calls construct graphs instead of executing eagerly. `deprecated_graph_mode_only`, `run_v1_only`, `run_v2_only`, and `run_in_graph_and_eager_modes` are available decorators for different v1/v2/eager/graph combinations. Args: func: function or class to be annotated. If `func` is a function this returns the decorator applied to `func`. If `func` is a unit test class this returns that class with the decorator applied to all test functions within that class. Returns: Returns a function or class that will run the decorated test(s) in graph mode.",torch.testing._internal.common_distributed.run_subtests,"Runs a test function given by ``test_fn`` as a subtest according to the configurations specified by ``subtest_config``. This amortizes the costly setup overhead (including process spawn and initializing the process group) over the subtests. Args: subtest_config (Dict[str, List[Any]]): A mapping from subtest keyword argument name to a list of its possible values. test_fn (Callable): A callable that runs the actual test. test_args: Positional arguments to pass to ``test_fn``. test_kwargs: Keyword arguments to pass to ``test_fn``.",0.96553767
tensorflow.python.eager.context.get_device_name,Get the device name for the current thread. Returns: The device name for the current thread.,torch.utils.checkpoint.get_device_type,Get the current default device type for checkpointing. Returns: str: The current default device type.,0.9655023
tensorflow.python.autograph.pyct.transpiler.transform_function,"Transforms a function. See GenericTranspiler.trasnform_function. This overload wraps the parent's `transform_function`, adding caching and facilities to instantiate the output as a Python object. It also adds facilities to make new symbols available to the generated Python code, visible as local variables - see `get_extra_locals`. Args: fn: A function or lambda. user_context: An opaque object (may be None) that is forwarded to transform_ast, through the ctx.user attribute. Returns: A tuple: * A function or lambda with the same signature and closure as `fn` * The temporary module into which the transformed function was loaded * The source map as a Dict[origin_info.LineLocation, origin_info.OriginInfo]",torch.onnx.utils.register_custom_op_symbolic,"Registers a symbolic function for a custom operator. When the user registers symbolic for custom/contrib ops, it is highly recommended to add shape inference for that operator via setType API, otherwise the exported graph may have incorrect shape inference in some extreme cases. An example of setType is `test_aten_embedding_2` in `test_operators.py`. See ""Custom Operators"" in the module documentation for an example usage. Args: symbolic_name (str): The name of the custom operator in ""<domain>::<op>"" format. symbolic_fn (Callable): A function that takes in the ONNX graph and the input arguments to the current operator, and returns new operator nodes to add to the graph. opset_version (int): The ONNX opset version in which to register.",0.9654986
tensorflow.python.ops.distributions.distribution.copy,"Creates a deep copy of the distribution. Note: the copy distribution may continue to depend on the original initialization arguments. Args: **override_parameters_kwargs: String/value dictionary of initialization arguments to override with new values. Returns: distribution: A new instance of `type(self)` initialized from the union of self.parameters and override_parameters_kwargs, i.e., `dict(self.parameters, **override_parameters_kwargs)`.",torch.distributed.optim.zero_redundancy_optimizer.load_state_dict,"Load the state pertaining to the given rank from the input ``state_dict``, updating the local optimizer as needed. Arguments: state_dict (dict): optimizer state; should be an object returned from a call to :meth:`state_dict`. Raises: RuntimeError: if ``overlap_with_ddp=True`` and this method is called before this :class:`ZeroRedundancyOptimizer` instance has been fully initialized, which happens once :class:`DistributedDataParallel` gradient buckets have been rebuilt.",0.9654522
tensorflow.python.ops.metrics_impl.mean_iou,"Calculate per-step mean Intersection-Over-Union (mIOU). Mean Intersection-Over-Union is a common evaluation metric for semantic image segmentation, which first computes the IOU for each semantic class and then computes the average over classes. IOU is defined as follows: IOU = true_positive / (true_positive + false_positive + false_negative). The predictions are accumulated in a confusion matrix, weighted by `weights`, and mIOU is then calculated from it. For estimation of the metric over a stream of data, the function creates an `update_op` operation that updates these variables and returns the `mean_iou`. If `weights` is `None`, weights default to 1. Use weights of 0 to mask values. Args: labels: A `Tensor` of ground truth labels with shape [batch size] and of type `int32` or `int64`. The tensor will be flattened if its rank > 1. predictions: A `Tensor` of prediction results for semantic labels, whose shape is [batch size] and type `int32` or `int64`. The tensor will be flattened if its rank > 1. num_classes: The possible number of labels the prediction task can have. This value must be provided, since a confusion matrix of dimension = [num_classes, num_classes] will be allocated. weights: Optional `Tensor` whose rank is either 0, or the same rank as `labels`, and must be broadcastable to `labels` (i.e., all dimensions must be either `1`, or the same as the corresponding `labels` dimension). metrics_collections: An optional list of collections that `mean_iou` should be added to. updates_collections: An optional list of collections `update_op` should be added to. name: An optional variable_scope name. Returns: mean_iou: A `Tensor` representing the mean intersection-over-union. update_op: An operation that increments the confusion matrix. Raises: ValueError: If `predictions` and `labels` have mismatched shapes, or if `weights` is not `None` and its shape doesn't match `predictions`, or if either `metrics_collections` or `updates_collections` are not a list or tuple.",torch.autograd.gradcheck.gradgradcheck,"Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in :attr:`inputs` and :attr:`grad_outputs` that are of floating point or complex type and with ``requires_grad=True``. This function checks that backpropagating through the gradients computed to the given :attr:`grad_outputs` are correct. The check between numerical and analytical gradients uses :func:`~torch.allclose`. .. note:: The default values are designed for :attr:`input` and :attr:`grad_outputs` of double precision. This check will likely fail if they are of less precision, e.g., ``FloatTensor``. .. warning:: If any checked tensor in :attr:`input` and :attr:`grad_outputs` has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from :func:`torch.expand`), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address. Args: func (function): a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors inputs (tuple of Tensor or Tensor): inputs to the function grad_outputs (tuple of Tensor or Tensor, optional): The gradients with respect to the function's outputs. eps (float, optional): perturbation for finite differences atol (float, optional): absolute tolerance rtol (float, optional): relative tolerance gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the randomly generated gradient outputs are made to be noncontiguous raise_exception (bool, optional): indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. nondet_tol (float, optional): tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default",0.9653684
tensorflow.python.util.tf_export.get_v1_constants,Get a list of TF 1.* constants in this module. Args: module: TensorFlow module. Returns: List of all API constants under the given module.,torch.utils.cpp_extension.library_paths,"Get the library paths required to build a C++ or CUDA extension. Args: cuda: If `True`, includes CUDA-specific library paths. Returns: A list of library path strings.",0.9653171
tensorflow.python.keras.engine.training.to_yaml,"Returns a yaml string containing the network configuration. Note: Since TF 2.6, this method is no longer supported and will raise a RuntimeError. To load a network from a yaml save file, use `keras.models.model_from_yaml(yaml_string, custom_objects={})`. `custom_objects` should be a dictionary mapping the names of custom losses / layers / etc to the corresponding functions / classes. Args: **kwargs: Additional keyword arguments to be passed to `yaml.dump()`. Returns: A YAML string. Raises: RuntimeError: announces that the method poses a security risk",torch.fx.interpreter.get_attr,"Execute a ``get_attr`` node. In ``Transformer``, this is overridden to insert a new ``get_attr`` node into the output graph. Args: target (Target): The call target for this node. See `Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for details on semantics args (Tuple): Tuple of positional args for this invocation kwargs (Dict): Dict of keyword arguments for this invocation",0.9653001
tensorflow.python.ops.tensor_array_ops.unstack,"Unstack the values of a `Tensor` in the TensorArray. If input value shapes have rank-`R`, then the output TensorArray will contain elements whose shapes are rank-`(R-1)`. Args: value: (N+1)-D. Tensor of type `dtype`. The Tensor to unstack. name: A name for the operation (optional). Returns: A new TensorArray object with flow that ensures the unstack occurs. Use this object for all subsequent operations. Raises: ValueError: if the shape inference fails.",torch.nn.functional.log_softmax,"Apply a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly. See :class:`~torch.nn.LogSoftmax` for more details. Args: input (Tensor): input dim (int): A dimension along which log_softmax will be computed. dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. If specified, the input tensor is cast to :attr:`dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",0.9652933
tensorflow.python.ops.nn_ops.conv_transpose,"The transpose of `convolution`. This operation is sometimes called ""deconvolution"" after (Zeiler et al., 2010), but is really the transpose (gradient) of `conv3d` rather than an actual deconvolution. Args: input: An N+2 dimensional `Tensor` of shape `[batch_size] + input_spatial_shape + [in_channels]` if data_format does not start with ""NC"" (default), or `[batch_size, in_channels] + input_spatial_shape` if data_format starts with ""NC"". It must be one of the following types: `half`, `bfloat16`, `float32`, `float64`. filters: An N+2 dimensional `Tensor` with the same type as `input` and shape `spatial_filter_shape + [in_channels, out_channels]`. output_shape: A 1-D `Tensor` representing the output shape of the deconvolution op. strides: An int or list of `ints` that has length `1`, `N` or `N+2`. The stride of the sliding window for each dimension of `input`. If a single value is given it is replicated in the spatial dimensions. By default the `N` and `C` dimensions are set to 0. The dimension order is determined by the value of `data_format`, see below for details. padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm. See [here](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2) for more information. data_format: A string or None. Specifies whether the channel dimension of the `input` and output is the last dimension (default, or if `data_format` does not start with ""NC""), or the second dimension (if `data_format` starts with ""NC""). For N=1, the valid values are ""NWC"" (default) and ""NCW"". For N=2, the valid values are ""NHWC"" (default) and ""NCHW"". For N=3, the valid values are ""NDHWC"" (default) and ""NCDHW"". dilations: An int or list of `ints` that has length `1`, `N` or `N+2`, defaults to 1. The dilation factor for each dimension of`input`. If a single value is given it is replicated in the spatial dimensions. By default the `N` and `C` dimensions are set to 1. If set to k > 1, there will be k-1 skipped cells between each filter elem",torch.nn.functional.embedding_bag,"Compute sums, means or maxes of `bags` of embeddings. Calculation is done without instantiating the intermediate embeddings. See :class:`torch.nn.EmbeddingBag` for more details. Note: {backward_reproducibility_note} Args: input (LongTensor): Tensor containing bags of indices into the embedding matrix weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines the starting index position of each bag (sequence) in :attr:`input`. max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm` is renormalized to have norm :attr:`max_norm`. Note: this will modify :attr:`weight` in-place. norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option. Default ``2``. scale_grad_by_freq (bool, optional): if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default ``False``. Note: this option is not supported when ``mode=""max""``. mode (str, optional): ``""sum""``, ``""mean""`` or ``""max""``. Specifies the way to reduce the bag. Default: ``""mean""`` sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under :class:`torch.nn.Embedding` for more details regarding sparse gradients. Note: this option is not supported when ``mode=""max""``. per_sample_weights (Tensor, optional): a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights` must have exactly the same shape as input and is treated as having the same :attr:`offsets`, if those are not None. include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1. The last element is the size of the input, or the ending index position of the last bag (sequence). padding_idx (int,",0.9652875
tensorflow.python.ops.linalg.linear_operator_util.convert_nonref_to_tensor,"Converts the given `value` to a `Tensor` if input is nonreference type. This function converts Python objects of various types to `Tensor` objects except if the input has nonreference semantics. Reference semantics are characterized by `is_ref` and is any object which is a `tf.Variable` or instance of `tf.Module`. This function accepts any input which `tf.convert_to_tensor` would also. Note: This function diverges from default Numpy behavior for `float` and `string` types when `None` is present in a Python list or scalar. Rather than silently converting `None` values, an error will be thrown. Args: value: An object whose type has a registered `Tensor` conversion function. dtype: Optional element type for the returned tensor. If missing, the type is inferred from the type of `value`. dtype_hint: Optional element type for the returned tensor, used when dtype is None. In some cases, a caller may not have a dtype in mind when converting to a tensor, so dtype_hint can be used as a soft preference. If the conversion to `dtype_hint` is not possible, this argument has no effect. name: Optional name to use if a new `Tensor` is created. Returns: tensor: A `Tensor` based on `value`. Raises: TypeError: If no conversion function is registered for `value` to `dtype`. RuntimeError: If a registered conversion function returns an invalid value. ValueError: If the `value` is a tensor not of given `dtype` in graph mode. #### Examples: python x = tf.Variable(0.) y = convert_nonref_to_tensor(x) x is y # ==> True x = tf.constant(0.) y = convert_nonref_to_tensor(x) x is y # ==> True x = np.array(0.) y = convert_nonref_to_tensor(x) x is y # ==> False tf.is_tensor(y) # ==> True x = tfp.util.DeferredTensor(13.37, lambda x: x) y = convert_nonref_to_tensor(x) x is y # ==> True tf.is_tensor(y) # ==> False tf.equal(y, 13.37) # ==> True",torch._functorch.eager_transforms.jacfwd,"Computes the Jacobian of ``func`` with respect to the arg(s) at index ``argnum`` using forward-mode autodiff Args: func (function): A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors argnums (int or Tuple[int]): Optional, integer or tuple of integers, saying which arguments to get the Jacobian with respect to. Default: 0. has_aux (bool): Flag indicating that ``func`` returns a ``(output, aux)`` tuple where the first element is the output of the function to be differentiated and the second element is auxiliary objects that will not be differentiated. Default: False. randomness(str): Flag indicating what type of randomness to use. See :func:`vmap` for more detail. Allowed: ""different"", ""same"", ""error"". Default: ""error"" Returns: Returns a function that takes in the same inputs as ``func`` and returns the Jacobian of ``func`` with respect to the arg(s) at ``argnums``. If ``has_aux is True``, then the returned function instead returns a ``(jacobian, aux)`` tuple where ``jacobian`` is the Jacobian and ``aux`` is auxiliary objects returned by ``func``. .. note:: You may see this API error out with ""forward-mode AD not implemented for operator X"". If so, please file a bug report and we will prioritize it. An alternative is to use :func:`jacrev`, which has better operator coverage. A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian >>> from torch.func import jacfwd >>> x = torch.randn(5) >>> jacobian = jacfwd(torch.sin)(x) >>> expected = torch.diag(torch.cos(x)) >>> assert torch.allclose(jacobian, expected) :func:`jacfwd` can be composed with vmap to produce batched Jacobians: >>> from torch.func import jacfwd, vmap >>> x = torch.randn(64, 5) >>> jacobian = vmap(jacfwd(torch.sin))(x) >>> assert jacobian.shape == (64, 5, 5) If you would like to compute the output of the function as well as the jacobian of the function, use the ``has_aux`` flag to return the output as an aux",0.9652477
tensorflow.python.summary.summary.tensor_summary,"Outputs a `Summary` protocol buffer with a serialized tensor.proto. Args: name: A name for the generated node. If display_name is not set, it will also serve as the tag name in TensorBoard. (In that case, the tag name will inherit tf name scopes.) tensor: A tensor of any type and shape to serialize. summary_description: A long description of the summary sequence. Markdown is supported. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. summary_metadata: Optional SummaryMetadata proto (which describes which plugins may use the summary value). family: Optional; if provided, used as the prefix of the summary tag, which controls the name used for display on TensorBoard when display_name is not set. display_name: A string used to name this data in TensorBoard. If this is not set, then the node name will be used instead. Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer.",torch.utils.tensorboard.summary.scalar,Output a `Summary` protocol buffer containing a single scalar value. The generated Summary has a Tensor.proto containing the input Tensor. Args: name: A name for the generated node. Will also serve as the series name in TensorBoard. tensor: A real numeric Tensor containing a single value. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. new_style: Whether to use new style (tensor field) or old style (simple_value field). New style could lead to faster data loading. Returns: A scalar `Tensor` of type `string`. Which contains a `Summary` protobuf. Raises: ValueError: If tensor has the wrong shape or type.,0.9651147
tensorflow.python.ops.logging_ops.Print,"Prints a list of tensors. This is an identity op (behaves like `tf.identity`) with the side effect of printing `data` when evaluating. Note: This op prints to the standard error. It is not currently compatible with jupyter notebook (printing to the notebook *server's* output, not into the notebook). @compatibility(TF2) This API is deprecated. Use `tf.print` instead. `tf.print` does not need the `input_` argument. `tf.print` works in TF2 when executing eagerly and inside a `tf.function`. In TF1-styled sessions, an explicit control dependency declaration is needed to execute the `tf.print` operation. Refer to the documentation of `tf.print` for more details. @end_compatibility Args: input_: A tensor passed through this op. data: A list of tensors to print out when op is evaluated. message: A string, prefix of the error message. first_n: Only log `first_n` number of times. Negative numbers log always; this is the default. summarize: Only print this many entries of each tensor. If None, then a maximum of 3 elements are printed per input tensor. name: A name for the operation (optional). Returns: A `Tensor`. Has the same type and contents as `input_`. python sess = tf.compat.v1.Session() with sess.as_default(): tensor = tf.range(10) print_op = tf.print(tensor) with tf.control_dependencies([print_op]): out = tf.add(tensor, tensor) sess.run(out)",torch.autograd.graph.register_prehook,"Register a backward pre-hook. The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:: hook(grad_outputs: Tuple[Tensor]) -> Tuple[Tensor] or None The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of :attr:`grad_outputs`. This function returns a handle with a method ``handle.remove()`` that removes the hook from the module. .. note:: See :ref:`backward-hooks-execution` for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Example:: >>> a = torch.tensor([0., 0., 0.], requires_grad=True) >>> b = a.clone() >>> assert isinstance(b.grad_fn, torch.autograd.graph.Node) >>> handle = b.grad_fn.register_prehook(lambda gI: (gI[0] * 2,)) >>> b.sum().backward(retain_graph=True) >>> print(a.grad) tensor([2., 2., 2.]) >>> handle.remove() >>> a.grad = None >>> b.sum().backward(retain_graph=True) >>> print(a.grad) tensor([1., 1., 1.])",0.9650782
tensorflow.python.keras.distribute.distribute_coordinator_utils.cluster_spec,Returns a copy of the cluster_spec object.,torch.onnx._internal.diagnostics.infra.context.sarif_log,Returns the SARIF Log object.,0.9649556
tensorflow.python.distribute.coordinator.cluster_coordinator.wait,"Wait for all closures to be finished before returning. If `mark_failed` was called before or during `wait`, the error from the first invocation of `mark_failed` will be raised. Args: timeout: A float specifying a timeout for the wait in seconds. Returns: True unless the given timeout expired, in which case it returns False.",torch.multiprocessing.spawn.join,"Join one or more processes within spawn context. Attempt to join one or more processes in this spawn context. If one of them exited with a non-zero exit status, this function kills the remaining processes and raises an exception with the cause of the first process exiting. Returns ``True`` if all processes have been joined successfully, ``False`` if there are more processes that need to be joined. Args: timeout (float): Wait this long before giving up on waiting.",0.96487164
tensorflow.python.saved_model.save.export_meta_graph,"Exports the MetaGraph proto of the `obj` to a file. This function goes through the same procedures saved_model.save goes to produce the given object's MetaGraph, then saves it to the given file. It skips saving checkpoint information, and is useful when all one wants is the graph defining the model. Args: obj: A trackable object to build the MetaGraph from. filename: The file into which to write the MetaGraph. signatures: Optional, either a `tf.function` with an input signature specified or the result of `f.get_concrete_function` on a `@tf.function`-decorated function `f`, in which case `f` will be used to generate a signature for the SavedModel under the default serving signature key. `signatures` may also be a dictionary, in which case it maps from signature keys to either `tf.function` instances with input signatures or concrete functions. The keys of such a dictionary may be arbitrary strings, but will typically be from the `tf.saved_model.signature_constants` module. options: Optional, `tf.saved_model.SaveOptions` object that specifies options for saving.",torch._custom_op.functional.register_functional_op,"Given a mutable operator, registers the functional variant. This API also correctly links the functional variant with the mutable operator for the purposes of functionalization. All of the new registrations are performed on the ``lib`` passed in. Arguments: lib (Library): Should be a torch.library.Library object that has the same namespace as ``mutable_op``'s namespace. lib will be used to register the new functional op as well as a functionalization kernel for the ``mutable_op`` If you don't have a library handy, use ``torch.library.Library(ns, 'FRAGMENT')`` to construct one. new_op_name (str): The name of the functional operator (without the namespace). If no namespace, the new functional variant will be accessible under ``torch.ops.{lib.ns}.new_op_name``. mutable_op (OpOverload): The mutable custom operator. Note that you may need to add a `.default` to it, like `torch.ops.aten.abs_.default`.",0.9648289
tensorflow.python.keras.engine.base_layer_v1.add_loss,"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: python class MyLayer(tf.keras.layers.Layer): def call(inputs, self): self.add_loss(tf.abs(tf.reduce_mean(inputs)), inputs=True) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(x.kernel)) The `get_losses_for` method allows to retrieve the losses relevant to a specific set of inputs. Args: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. inputs: Ignored when executing eagerly. If anything other tha",torch._functorch.make_functional.make_functional,"make_functional(model, disable_autograd_tracking=False) -> func, params Given a ``torch.nn.Module``, :func:`make_functional` extracts the state (params) and returns a functional version of the model, ``func``. This makes it so that it is possible use transforms over the parameters of ``model``. ``func`` can be invoked as follows: .. code-block:: python import torch import torch.nn as nn from functorch import make_functional x = torch.randn(4, 3) model = nn.Linear(3, 3) func, params = make_functional(model) func(params, x) And here is an example of applying the grad transform over the parameters of a model. .. code-block:: python import torch import torch.nn as nn from functorch import make_functional, grad x = torch.randn(4, 3) t = torch.randn(4, 3) model = nn.Linear(3, 3) func, params = make_functional(model) def compute_loss(params, x, t): y = func(params, x) return nn.functional.mse_loss(y, t) grad_weights = grad(compute_loss)(params, x, t) If the model has any buffers, please use :func:`make_functional_with_buffers` instead. Args: model (torch.nn.Module): Input model. disable_autograd_tracking (bool): Flag to disable gradients tracking for output parameters. The returned params are unrelated to the set of params from the original model. If False (default), the params will have ``requires_grad=True`` on them (aka they will be trackable with regular PyTorch autograd), matching the requires_grad-ness of the params from the original model. Otherwise, the returned params will have ``requires_grad=False``. Default, False. If you plan on using regular PyTorch autograd (e.g., if you want to call ``.backward()`` or ``torch.autograd.grad()``, then set ``disable_autograd_tracking=False``. Otherwise, if you're only planning on using functorch's gradient transforms, then please set ``disable_autograd_tracking=True`` to avoid unnecessarily tracking history with PyTorch autograd.",0.9647695
tensorflow.python.ops.math_ops.cumulative_logsumexp,"Compute the cumulative log-sum-exp of the tensor `x` along `axis`. By default, this op performs an inclusive cumulative log-sum-exp, which means that the first element of the input is identical to the first element of the output. This operation is significantly more numerically stable than the equivalent tensorflow operation `tf.math.log(tf.math.cumsum(tf.math.exp(x)))`, although computes the same result given infinite numerical precision. However, note that in some cases, it may be less stable than `tf.math.reduce_logsumexp` for a given element, as it applies the ""log-sum-exp trick"" in a different way. More precisely, where `tf.math.reduce_logsumexp` uses the following trick: log(sum(exp(x))) == log(sum(exp(x - max(x)))) + max(x) it cannot be directly used here as there is no fast way of applying it to each prefix `x[:i]`. Instead, this function implements a prefix scan using pairwise log-add-exp, which is a commutative and associative (up to floating point precision) operator: log_add_exp(x, y) = log(exp(x) + exp(y)) = log(1 + exp(min(x, y) - max(x, y))) + max(x, y) However, reducing using the above operator leads to a different computation tree (logs are taken repeatedly instead of only at the end), and the maximum is only computed pairwise instead of over the entire prefix. In general, this leads to a different and slightly less precise computation. Args: x: A `Tensor`. Must be one of the following types: `float16`, `float32`, `float64`. axis: A `Tensor` of type `int32` or `int64` (default: 0). Must be in the range `[-rank(x), rank(x))`. exclusive: If `True`, perform exclusive cumulative log-sum-exp. reverse: If `True`, performs the cumulative log-sum-exp in the reverse direction. name: A name for the operation (optional). Returns: A `Tensor`. Has the same shape and type as `x`.",torch.autograd.functional.jvp,"Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector ``v``. Args: func (function): a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. inputs (tuple of Tensors or Tensor): inputs to the function ``func``. v (tuple of Tensors or Tensor): The vector for which the Jacobian vector product is computed. Must be the same size as the input of ``func``. This argument is optional when the input to ``func`` contains a single element and (if it is not provided) will be set as a Tensor containing a single ``1``. create_graph (bool, optional): If ``True``, both the output and result will be computed in a differentiable way. Note that when ``strict`` is ``False``, the result can not require gradients or be disconnected from the inputs. Defaults to ``False``. strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to ``False``. Returns: output (tuple): tuple with: func_output (tuple of Tensors or Tensor): output of ``func(inputs)`` jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output. Note: ``autograd.functional.jvp`` computes the jvp by using the backward of the backward (sometimes called the double backwards trick). This is not the most performant way of computing the jvp. Please consider using :func:`torch.func.jvp` or the :ref:`low-level forward-mode AD API <forward-mode-ad>` instead. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD) >>> def exp_reducer(x): ... return x.exp().sum(dim=1) >>> inputs = torch.rand(4, 4) >>> v = torch.ones(4, 4) >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> jvp(exp_reducer, inputs, v) (tensor([6.3090, 4.6742, 7.9114, 8.2106]), tensor([6.3090, 4.6742, 7.9114, 8.2106])) >>> jvp(exp_r",0.9646524
tensorflow.python.types.core.experimental_get_compiler_ir,"Returns compiler IR for the compiled function. This API is intended *only* for debugging as there are no guarantees on backwards compatibility of returned IR or the allowed values of `stage`. Args: *args: compilation args supports inputs either: (1) all inputs are TensorSpec or (2) all inputs are tf.Tensor/Python variables. **kwargs: Keyword arguments used for compilation. Same requirement as compiliation args. Returns: Function callable with the following kwargs: - `stage` at which the compiler IR should be serialized. Allowed values are: - `hlo`: HLO output after conversion from TF (https://www.tensorflow.org/xla/operation_semantics). - `hlo_serialized`: Like stage=`hlo`, but the output is a serialized HLO module proto (a bytes object). - `optimized_hlo`: HLO after compiler optimizations. - `optimized_hlo_serialized`: Like stage=`optimized_hlo`, but the output is a serialized HLO module proto (a bytes object). - `optimized_hlo_dot`: optimized HLO in DOT format suitable for Graphviz. - `device_name` can be either None, in which case the preferred device is used for compilation, or a device name. It can be a full device name, or a partial one, e.g., `/device:CPU:0`. For example, for python @tf.function(jit_compile=True) def f(x): return x + 1 f.experimental_get_compiler_ir(tf.random.normal([10, 10])(stage='hlo') the output is: HloModule a_inference_f_13__.9 ENTRY %a_inference_f_13__.9 (arg0.1: f32[10,10]) -> f32[10,10] { %arg0.1 = f32[10,10]{1,0} parameter(0), parameter_replication={false} %reshape.2 = f32[10,10]{1,0} reshape(f32[10,10]{1,0} %arg0.1) %constant.3 = f32[] constant(1) %broadcast.4 = f32[10,10]{1,0} broadcast(f32[] %constant.3) %add.5 = f32[10,10]{1,0} add(f32[10,10]{1,0} %reshape.2, f32[10,10]{1,0} %broadcast.4) %reshape.6 = f32[10,10]{1,0} reshape(f32[10,10]{1,0} %add.5) %tuple.7 = (f32[10,10]{1,0}) tuple(f32[10,10]{1,0} %reshape.6) ROOT %get-tuple-element.8 = f32[10,10]{1,0} get-tuple-element((f32[10,10]{1,0}) %tuple.7), index=0 } Here is another exa",torch.distributed.distributed_c10d.batch_isend_irecv,"Send or Receive a batch of tensors asynchronously and return a list of requests. Process each of the operations in ``p2p_op_list`` and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported. Args: p2p_op_list: A list of point-to-point operations(type of each operator is ``torch.distributed.P2POp``). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end. Returns: A list of distributed request objects returned by calling the corresponding op in the op_list. Examples: >>> # xdoctest: +SKIP(""no rank"") >>> send_tensor = torch.arange(2, dtype=torch.float32) + 2 * rank >>> recv_tensor = torch.randn(2, dtype=torch.float32) >>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size) >>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size) >>> reqs = batch_isend_irecv([send_op, recv_op]) >>> for req in reqs: >>> req.wait() >>> recv_tensor tensor([2, 3]) # Rank 0 tensor([0, 1]) # Rank 1 .. note:: Note that when this API is used with the NCCL PG backend, users must set the current GPU device with `torch.cuda.set_device`, otherwise it will lead to unexpected hang issues. In addition, if this API is the first collective call in the ``group`` passed to ``dist.P2POp``, all ranks of the ``group`` must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the ``group``, batched P2P operations involving only a subset of ranks of the ``group`` are allowed.",0.96462315
tensorflow.python.saved_model.save.save,"Exports a [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module) (and subclasses) `obj` to [SavedModel format](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk). The `obj` must inherit from the [`Trackable` class](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/trackable/base.py#L278). Example usage: >>> class Adder(tf.Module): ... @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)]) ... def add(self, x): ... return x + x >>> model = Adder() >>> tf.saved_model.save(model, '/tmp/adder') The resulting SavedModel is then servable with an input named ""x"", a scalar with dtype float32. _Signatures_ Signatures define the input and output types for a computation. The optional save `signatures` argument controls which methods in `obj` will be available to programs which consume `SavedModel`s, for example, serving APIs. Python functions may be decorated with `@tf.function(input_signature=...)` and passed as signatures directly, or lazily with a call to `get_concrete_function` on the method decorated with `@tf.function`. Example: >>> class Adder(tf.Module): ... @tf.function ... def add(self, x): ... return x + x >>> model = Adder() >>> tf.saved_model.save( ... model, '/tmp/adder',signatures=model.add.get_concrete_function( ... tf.TensorSpec([], tf.float32))) If a `@tf.function` does not have an input signature and `get_concrete_function` is not called on that method, the function will not be directly callable in the restored SavedModel. Example: >>> class Adder(tf.Module): ... @tf.function ... def add(self, x): ... return x + x >>> model = Adder() >>> tf.saved_model.save(model, '/tmp/adder') >>> restored = tf.saved_model.load('/tmp/adder') >>> restored.add(1.) Traceback (most recent call last): ... ValueError: Found zero restored functions for caller function. If the `signatures` argument is omitted, `obj` will be searched for `@tf.function`-decorated methods. If exactly one traced `@tf.function` ",torch.onnx._internal.exporter.enable_fake_mode,"Enable fake mode for the duration of the context. Internally it instantiates a :class:`torch._subclasses.fake_tensor.FakeTensorMode` context manager that converts user input and model parameters into :class:`torch._subclasses.fake_tensor.FakeTensor`. A :class:`torch._subclasses.fake_tensor.FakeTensor` is a :class:`torch.Tensor` with the ability to run PyTorch code without having to actually do computation through tensors allocated on a ``meta`` device. Because there is no actual data being allocated on the device, this API allows for exporting large models without the actual memory footprint needed for executing it. It is highly recommended to enable fake mode when exporting models that are too large to fit into memory. Returns: A :class:`ONNXFakeContext` object that must be passed to :func:`dynamo_export` through the :attr:`ExportOptions.fake_context` argument. Example:: # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX) >>> import torch >>> import torch.onnx >>> class MyModel(torch.nn.Module): # Dummy model ... def __init__(self) -> None: ... super().__init__() ... self.linear = torch.nn.Linear(2, 2) ... def forward(self, x): ... out = self.linear(x) ... return out >>> with torch.onnx.enable_fake_mode() as fake_context: ... my_nn_module = MyModel() ... arg1 = torch.randn(2, 2, 2) # positional input 1 >>> export_options = torch.onnx.ExportOptions(fake_context=fake_context) >>> onnx_program = torch.onnx.dynamo_export( ... my_nn_module, ... arg1, ... export_options=export_options ... ) >>> # Saving model WITHOUT initializers >>> onnx_program.save(""my_model_without_initializers.onnx"") >>> # Saving model WITH initializers >>> onnx_program.save(""my_model_with_initializers.onnx"", model_state=MyModel().state_dict()) .. warning:: This API is experimental and is *NOT* backward-compatible.",0.964614
tensorflow.python.ops.nn_ops.conv3d_transpose_v2,"The transpose of `conv3d`. This operation is sometimes called ""deconvolution"" after (Zeiler et al., 2010), but is really the transpose (gradient) of `conv3d` rather than an actual deconvolution. Args: input: A 5-D `Tensor` of type `float` and shape `[batch, depth, height, width, in_channels]` for `NDHWC` data format or `[batch, in_channels, depth, height, width]` for `NCDHW` data format. filters: A 5-D `Tensor` with the same type as `input` and shape `[depth, height, width, output_channels, in_channels]`. `filter`'s `in_channels` dimension must match that of `input`. output_shape: A 1-D `Tensor` representing the output shape of the deconvolution op. strides: An int or list of `ints` that has length `1`, `3` or `5`. The stride of the sliding window for each dimension of `input`. If a single value is given it is replicated in the `D`, `H` and `W` dimension. By default the `N` and `C` dimensions are set to 0. The dimension order is determined by the value of `data_format`, see below for details. padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm. See [here](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2) for more information. data_format: A string. 'NDHWC' and 'NCDHW' are supported. dilations: An int or list of `ints` that has length `1`, `3` or `5`, defaults to 1. The dilation factor for each dimension of`input`. If a single value is given it is replicated in the `D`, `H` and `W` dimension. By default the `N` and `C` dimensions are set to 1. If set to k > 1, there will be k-1 skipped cells between each filter element on that dimension. The dimension order is determined by the value of `data_format`, see above for details. Dilations in the batch and depth dimensions if a 5-d tensor must be 1. name: Optional name for the returned tensor. Returns: A `Tensor` with the same type as `input`. References: Deconvolutional Networks: [Zeiler et al., 2010] (https://ieeexplore.ieee.org/abstract/document/5539957) ([pdf] (http://citeseerx.ist",torch.nn.functional.interpolate,"Down/up samples the input. Tensor interpolated to either the given :attr:`size` or the given :attr:`scale_factor` The algorithm used for interpolation is determined by :attr:`mode`. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`, `nearest-exact` Args: input (Tensor): the input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. If `scale_factor` is a tuple, its length has to match the number of spatial dimensions; `input.dim() - 2`. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'linear'`` | ``'bilinear'`` | ``'bicubic'`` | ``'trilinear'`` | ``'area'`` | ``'nearest-exact'``. Default: ``'nearest'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'`` or ``'trilinear'``. Default: ``False`` recompute_scale_factor (bool, optional): recompute the scale_factor for use in the interpolation calculation. If `recompute_scale_factor` is ``True``, then `scale_factor` must be passed in and `scale_factor` is used to compute the output `size`. The computed output ",0.9645641
tensorflow.python.summary.writer.writer.add_run_metadata,Adds a metadata information for a single session.run() call. Args: run_metadata: A `RunMetadata` protobuf object. tag: The tag name for this metadata. global_step: Number. Optional global step counter to record with the StepStats. Raises: ValueError: If the provided tag was already used for this type of event.,torch.utils.tensorboard.writer.add_event,Add an event to the event file. Args: event: An `Event` protocol buffer. step: Number. Optional global step value for training process to record with the event. walltime: float. Optional walltime to override the default (current) walltime (from time.time()) seconds after epoch,0.9645509
tensorflow.python.data.experimental.ops.io.save,"Saves the content of the given dataset. Example usage: >>> import tempfile >>> path = os.path.join(tempfile.gettempdir(), ""saved_data"") >>> # Save a dataset >>> dataset = tf.data.Dataset.range(2) >>> tf.data.experimental.save(dataset, path) >>> new_dataset = tf.data.experimental.load(path) >>> for elem in new_dataset: ... print(elem) tf.Tensor(0, shape=(), dtype=int64) tf.Tensor(1, shape=(), dtype=int64) The saved dataset is saved in multiple file ""shards"". By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the `shard_func` function. For example, you can save the dataset to using a single shard as follows: python dataset = make_dataset() def custom_shard_func(element): return np.int64(0) dataset = tf.data.experimental.save( path=""/path/to/data"", ..., shard_func=custom_shard_func) To enable checkpointing, pass in `checkpoint_args` to the `save` method as follows: python dataset = tf.data.Dataset.range(100) save_dir = ""..."" checkpoint_prefix = ""..."" step_counter = tf.Variable(0, trainable=False) checkpoint_args = { ""checkpoint_interval"": 50, ""step_counter"": step_counter, ""directory"": checkpoint_prefix, ""max_to_keep"": 20, } dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args) NOTE: The directory layout and file format used for saving the dataset is considered an implementation detail and may change. For this reason, datasets saved through `tf.data.experimental.save` should only be consumed through `tf.data.experimental.load`, which is guaranteed to be backwards compatible. Args: dataset: The dataset to save. path: Required. A directory to use for saving the dataset. compression: Optional. The algorithm to use to compress data when writing it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`. shard_func: Optional. A function to control the mapping of dataset elements to file shards. The function is expected to map elements of the input dataset to int64 shard IDs. If present, the fu",torch.onnx._internal.exporter.adapt_torch_inputs_to_onnx,"Converts the PyTorch model inputs to exported ONNX model inputs format. Due to design differences, input/output format between PyTorch model and exported ONNX model are often not the same. E.g., None is allowed for PyTorch model, but are not supported by ONNX. Nested constructs of tensors are allowed for PyTorch model, but only flattened tensors are supported by ONNX, etc. The actual adapting steps are associated with each individual export. It depends on the PyTorch model, the particular set of model_args and model_kwargs used for the export, and export options. This method replays the adapting steps recorded during export. Args: model_args: The PyTorch model inputs. model_with_state_dict: The PyTorch model to get extra state from. If not specified, the model used during export is used. Required when :func:`enable_fake_mode` is used to extract real initializers as needed by the ONNX graph. model_kwargs: The PyTorch model keyword inputs. Returns: A sequence of tensors converted from PyTorch model inputs. Example:: # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX) >>> import torch >>> import torch.onnx >>> from typing import Dict, Tuple >>> def func_nested_input( ... x_dict: Dict[str, torch.Tensor], ... y_tuple: Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]] ... ): ... if ""a"" in x_dict: ... x = x_dict[""a""] ... elif ""b"" in x_dict: ... x = x_dict[""b""] ... else: ... x = torch.randn(3) ... ... y1, (y2, y3) = y_tuple ... ... return x + y1 + y2 + y3 >>> x_dict = {""a"": torch.tensor(1.)} >>> y_tuple = (torch.tensor(2.), (torch.tensor(3.), torch.tensor(4.))) >>> onnx_program = torch.onnx.dynamo_export(func_nested_input, x_dict, y_tuple) >>> print(x_dict, y_tuple) {'a': tensor(1.)} (tensor(2.), (tensor(3.), tensor(4.))) >>> print(onnx_program.adapt_torch_inputs_to_onnx(x_dict, y_tuple, model_with_state_dict=func_nested_input)) (tensor(1.), tensor(2.), tensor(3.), tensor(4.)) .. warning:: This API is experimental and is *NOT* backward-compatible.",0.9645039
tensorflow.python.data.ops.dataset_ops.make_one_shot_iterator,"Creates an iterator for elements of this dataset. Note: The returned iterator will be initialized automatically. A ""one-shot"" iterator does not currently support re-initialization. For that see `make_initializable_iterator`. Example: python # Building graph ... dataset = ... next_value = dataset.make_one_shot_iterator().get_next() # ... from within a session ... try: while True: value = sess.run(next_value) ... except tf.errors.OutOfRangeError: pass Returns: An `tf.data.Iterator` for elements of this dataset.",torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook,"Call ``allreduce`` using ``GradBucket`` tensors. Once gradient tensors are aggregated across all workers, its ``then`` callback takes the mean and returns the result. If user registers this DDP communication hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won't change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior. Example:: >>> # xdoctest: +SKIP >>> ddp_model.register_comm_hook(process_group, allreduce_hook)",0.964497
tensorflow.python.ops.ragged.ragged_map_ops.map_fn,"map on the list of tensors unpacked from `elems` on dimension 0. The simplest version of `map_fn` repeatedly applies the callable `fn` to a sequence of elements from first to last. The elements are made of the tensors unpacked from `elems`. `dtype` is the data type of the return value of `fn`. Users must provide `dtype` if it is different from the data type of `elems`. Suppose that `elems` is unpacked into `values`, a list of tensors. The shape of the result tensor is `[values.shape[0]] + fn(values[0]).shape`. This method also allows multi-arity `elems` and output of `fn`. If `elems` is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of `fn` may match the structure of `elems`. That is, if `elems` is `(t1, [t2, t3, [t4, t5]])`, then an appropriate signature for `fn` is: `fn = lambda (t1, [t2, t3, [t4, t5]]):`. Furthermore, `fn` may emit a different structure than its input. For example, `fn` may look like: `fn = lambda t1: return (t1 + 1, t1 - 1)`. In this case, the `dtype` parameter is not optional: `dtype` must be a type or (possibly nested) tuple of types matching the output of `fn`. To apply a functional operation to the nonzero elements of a SparseTensor one of the following methods is recommended. First, if the function is expressible as TensorFlow ops, use python result = SparseTensor(input.indices, fn(input.values), input.dense_shape) If, however, the function is not expressible as a TensorFlow op, then use python result = SparseTensor( input.indices, map_fn(fn, input.values), input.dense_shape) instead. When executing eagerly, map_fn does not execute in parallel even if `parallel_iterations` is set to a value > 1. You can still get the performance benefits of running a function in parallel by using the `tf.contrib.eager.defun` decorator, python # Assume the function being used in map_fn is fn. # To ensure map_fn calls fn in parallel, use the defun decorator. @tf.contrib.eag",torch._functorch.eager_transforms.jvp,"Standing for the Jacobian-vector product, returns a tuple containing the output of `func(*primals)` and the ""Jacobian of ``func`` evaluated at ``primals``"" times ``tangents``. This is also known as forward-mode autodiff. Args: func (function): A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors primals (Tensors): Positional arguments to ``func`` that must all be Tensors. The returned function will also be computing the derivative with respect to these arguments tangents (Tensors): The ""vector"" for which Jacobian-vector-product is computed. Must be the same structure and sizes as the inputs to ``func``. has_aux (bool): Flag indicating that ``func`` returns a ``(output, aux)`` tuple where the first element is the output of the function to be differentiated and the second element is other auxiliary objects that will not be differentiated. Default: False. Returns: Returns a ``(output, jvp_out)`` tuple containing the output of ``func`` evaluated at ``primals`` and the Jacobian-vector product. If ``has_aux is True``, then instead returns a ``(output, jvp_out, aux)`` tuple. .. note:: You may see this API error out with ""forward-mode AD not implemented for operator X"". If so, please file a bug report and we will prioritize it. jvp is useful when you wish to compute gradients of a function R^1 -> R^N >>> from torch.func import jvp >>> x = torch.randn([]) >>> f = lambda x: x * torch.tensor([1., 2., 3]) >>> value, grad = jvp(f, (x,), (torch.tensor(1.),)) >>> assert torch.allclose(value, f(x)) >>> assert torch.allclose(grad, torch.tensor([1., 2, 3])) :func:`jvp` can support functions with multiple inputs by passing in the tangents for each of the inputs >>> from torch.func import jvp >>> x = torch.randn(5) >>> y = torch.randn(5) >>> f = lambda x, y: (x * y) >>> _, output = jvp(f, (x, y), (torch.ones(5), torch.ones(5))) >>> assert torch.allclose(output, x + y)",0.964438
tensorflow.python.keras.optimizer_v2.optimizer_v2.name_scope_only_in_function_or_graph,"Internal-only entry point for `name_scope*`. Enters a compat.v1.name_scope only when in a function or graph, not when running fully eagerly. Args: name: The name argument that is passed to the op function. Returns: `name_scope*` context manager.",torch.distributed.fsdp.fully_sharded_data_parallel.named_parameters,"Return an iterator over module parameters, yielding both the name of the parameter and the parameter itself. Intercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix when inside the :meth:`summon_full_params` context manager.",0.9644364
tensorflow.python.summary.writer.writer_cache.get,"Returns the FileWriter for the specified directory. Args: logdir: str, name of the directory. Returns: A `FileWriter`.",torch.package.file_structure_representation.has_file,Checks if a file is present in a :class:`Directory`. Args: filename (str): Path of file to search for. Returns: bool: If a :class:`Directory` contains the specified file.,0.96419466
tensorflow.python.ops.signal.spectral_ops.stft,"Computes the [Short-time Fourier Transform][stft] of `signals`. Implemented with TPU/GPU-compatible ops and supports gradients. Args: signals: A `[..., samples]` `float32`/`float64` `Tensor` of real-valued signals. frame_length: An integer scalar `Tensor`. The window length in samples. frame_step: An integer scalar `Tensor`. The number of samples to step. fft_length: An integer scalar `Tensor`. The size of the FFT to apply. If not provided, uses the smallest power of 2 enclosing `frame_length`. window_fn: A callable that takes a window length and a `dtype` keyword argument and returns a `[window_length]` `Tensor` of samples in the provided datatype. If set to `None`, no windowing is used. pad_end: Whether to pad the end of `signals` with zeros when the provided frame length and step produces a frame that lies partially past its end. name: An optional name for the operation. Returns: A `[..., frames, fft_unique_bins]` `Tensor` of `complex64`/`complex128` STFT values where `fft_unique_bins` is `fft_length // 2 + 1` (the unique components of the FFT). Raises: ValueError: If `signals` is not at least rank 1, `frame_length` is not scalar, or `frame_step` is not scalar. [stft]: https://en.wikipedia.org/wiki/Short-time_Fourier_transform",torch.nn.functional.max_pool1d_with_indices,"max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False) Applies a 1D max pooling over an input signal composed of several input planes. .. note:: The order of :attr:`ceil_mode` and :attr:`return_indices` is different from what seen in :class:`~torch.nn.MaxPool1d`, and will change in a future release. See :class:`~torch.nn.MaxPool1d` for details. Args: input: input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)`, minibatch dim optional. kernel_size: the size of the window. Can be a single number or a tuple `(kW,)` stride: the stride of the window. Can be a single number or a tuple `(sW,)`. Default: :attr:`kernel_size` padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2. dilation: The stride between elements within a sliding window, must be > 0. ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window. return_indices: If ``True``, will return the argmax along with the max values. Useful for :class:`torch.nn.functional.max_unpool1d` later",0.9641243
tensorflow.python.training.supervisor.request_stop,"Request that the coordinator stop the threads. See `Coordinator.request_stop()`. Args: ex: Optional `Exception`, or Python `exc_info` tuple as returned by `sys.exc_info()`. If this is the first call to `request_stop()` the corresponding exception is recorded and re-raised from `join()`.",torch.nn.modules.module.register_module_backward_hook,Register a backward hook common to all the modules. This function is deprecated in favor of :func:`torch.nn.modules.module.register_module_full_backward_hook` and the behavior of this function will change in future versions. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()``,0.96409786
tensorflow.python.eager.polymorphic_function.eager_function_run.run_functions_eagerly,"Enables / disables eager execution of `tf.function`s. Calling `tf.config.run_functions_eagerly(True)` will make all invocations of `tf.function` run eagerly instead of running as a traced graph function. This can be useful for debugging. As the code now runs line-by-line, you can add arbitrary `print` messages or pdb breakpoints to monitor the inputs/outputs of each Tensorflow operation. However, you should avoid using this for actual production because it significantly slows down execution. >>> def my_func(a): ... print(f'a: {a}') ... return a + a >>> a_fn = tf.function(my_func) >>> # A side effect the first time the function is traced >>> # In tracing time, `a` is printed with shape and dtype only >>> a_fn(tf.constant(1)) a: Tensor(""a:0"", shape=(), dtype=int32) <tf.Tensor: shape=(), dtype=int32, numpy=2> >>> # `print` is a python side effect, it won't execute as the traced function >>> # is called >>> a_fn(tf.constant(2)) <tf.Tensor: shape=(), dtype=int32, numpy=4> >>> # Now, switch to eager running >>> tf.config.run_functions_eagerly(True) >>> # The code now runs eagerly and the actual value of `a` is printed >>> a_fn(tf.constant(2)) a: 2 <tf.Tensor: shape=(), dtype=int32, numpy=4> >>> # Turn this back off >>> tf.config.run_functions_eagerly(False) Note: This flag has no effect on functions passed into tf.data transformations as arguments. tf.data functions are never executed eagerly and are always executed as a compiled Tensorflow Graph. Args: run_eagerly: Boolean. Whether to run functions eagerly.",torch.nn.utils.stateless.functional_call,"Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. .. warning:: This API is deprecated as of PyTorch 2.0 and will be removed in a future version of PyTorch. Please use :func:`torch.func.functional_call` instead, which is a drop-in replacement for this API. .. note:: If the module has active parametrizations, passing a value in the :attr:`parameters_and_buffers` argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as ``{submodule_name}.parametrizations.{parameter_name}.original``. .. note:: If the module performs in-place operations on parameters/buffers, these will be reflected in the `parameters_and_buffers` input. Example:: >>> a = {'foo': torch.zeros(())} >>> # xdoctest: +SKIP >>> mod = Foo() # does self.foo = self.foo + 1 >>> print(mod.foo) # tensor(0.) >>> functional_call(mod, a, torch.ones(())) >>> print(mod.foo) # tensor(0.) >>> print(a['foo']) # tensor(1.) .. note:: If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag. Example:: >>> a = {'foo': torch.zeros(())} >>> # xdoctest: +SKIP >>> mod = Foo() # has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied >>> print(mod.foo) # tensor(1.) >>> mod(torch.zeros(())) # tensor(2.) >>> functional_call(mod, a, torch.zeros(())) # tensor(0.) since it will change self.foo_tied too >>> functional_call(mod, a, torch.zeros(()), tie_weights=False) # tensor(1.)--self.foo_tied is not updated >>> new_a = {'foo': torch.zeros(()), 'foo_tied': torch.zeros(())} >>> functional_call(mod, new_a, torch.zeros()) # tensor(0.) Args: module (torch.nn.Module): the module to call parameters_and_buffers (dict of str and Tensor): the parameters that will be used in the module call. args (Any or tuple): arguments to be passed to the module call. If no",0.9640963
tensorflow.python.ops.check_ops.ensure_shape,"Updates the shape of a tensor and checks at runtime that the shape holds. When executed, this operation asserts that the input tensor `x`'s shape is compatible with the `shape` argument. See `tf.TensorShape.is_compatible_with` for details. >>> x = tf.constant([[1, 2, 3], ... [4, 5, 6]]) >>> x = tf.ensure_shape(x, [2, 3]) Use `None` for unknown dimensions: >>> x = tf.ensure_shape(x, [None, 3]) >>> x = tf.ensure_shape(x, [2, None]) If the tensor's shape is not compatible with the `shape` argument, an error is raised: >>> x = tf.ensure_shape(x, [5]) Traceback (most recent call last): ... tf.errors.InvalidArgumentError: Shape of tensor dummy_input [3] is not compatible with expected shape [5]. [Op:EnsureShape] During graph construction (typically tracing a `tf.function`), `tf.ensure_shape` updates the static-shape of the **result** tensor by merging the two shapes. See `tf.TensorShape.merge_with` for details. This is most useful when **you** know a shape that can't be determined statically by TensorFlow. The following trivial `tf.function` prints the input tensor's static-shape before and after `ensure_shape` is applied. >>> @tf.function ... def f(tensor): ... print(""Static-shape before:"", tensor.shape) ... tensor = tf.ensure_shape(tensor, [None, 3]) ... print(""Static-shape after:"", tensor.shape) ... return tensor This lets you see the effect of `tf.ensure_shape` when the function is traced: >>> cf = f.get_concrete_function(tf.TensorSpec([None, None])) Static-shape before: (None, None) Static-shape after: (None, 3) >>> cf(tf.zeros([3, 3])) # Passes >>> cf(tf.constant([1, 2, 3])) # fails Traceback (most recent call last): ... InvalidArgumentError: Shape of tensor x [3] is not compatible with expected shape [3,3]. The above example raises `tf.errors.InvalidArgumentError`, because `x`'s shape, `(3,)`, is not compatible with the `shape` argument, `(None, 3)` Inside a `tf.function` or `v1.Graph` context it checks both the buildtime and runtime shapes. This is stricter than `",torch.autograd.graph.register_multi_grad_hook,"Register a multi-grad backward hook. There are two supported modes: ``""all""`` and ``""any""``. Under the ``""all""`` mode, the hook will be called after gradients with respect to every tensor in :attr:`tensors` have been computed. If a tensor is in :attr:`tensors` but is not part of the graph, or if a tensor is not needed to compute the gradients for any ``inputs`` specified for the current ``.backward()`` or ``.grad()`` call, this tensor will be ignored and the hook will not wait for its gradient to be computed. After every non-ignored tensor's gradient has been computed, :attr:`fn` will be called with those gradients. ``None`` will be passed for tensors that did not have their gradients computed. Under the ``""any""`` mode, the hook will be called after the first gradient with respect to a tensor in :attr:`tensors` has been computed. The hook will be called with that gradient as its argument. The hook should not modify its arguments. This function returns a handle with a method ``handle.remove()`` that removes the hook. .. note:: See :ref:`backward-hooks-execution` for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Example:: >>> import torch >>> >>> a = torch.rand(2, 3, requires_grad=True) >>> b = torch.rand(2, 3, requires_grad=True) >>> c = a * b >>> d = a * b >>> >>> def fn(grads): ... print([g is not None for g in grads]) ... >>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn) >>> >>> c.sum().backward(retain_graph=True) [True, True, True, False] >>> c.sum().backward(inputs=(a,), retain_graph=True) [True, False, True, False] >>>",0.96408737
tensorflow.python.keras.utils.generic_utils.serialize_keras_object,"Serialize a Keras object into a JSON-compatible representation. Calls to `serialize_keras_object` while underneath the `SharedObjectSavingScope` context manager will cause any objects re-used across multiple layers to be saved with a special shared object ID. This allows the network to be re-created properly during deserialization. Args: instance: The object to serialize. Returns: A dict-like, JSON-compatible representation of the object's config.",torch.distributed.elastic.rendezvous.api.use_agent_store,Indicates that store reference returned by :py:meth:`next_rendezvous` can be shared with user applications and will be available during application lifecyle. Rendezous handler impl will share store details as instance of :py:class:`RendezvousStoreInfo`. Applications as a convention use `MASTER_ADDR`/`MASTER_PORT` env variables to lookup the store.,0.96407825
tensorflow.python.distribute.distribute_lib.experimental_distribute_dataset,"Creates `tf.distribute.DistributedDataset` from `tf.data.Dataset`. The returned `tf.distribute.DistributedDataset` can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a `tf.distribute.DistributedDataset`. You can only create an iterator or examine the `tf.TypeSpec` of the data generated by it. See API docs of `tf.distribute.DistributedDataset` to learn more. The following is an example: >>> global_batch_size = 2 >>> # Passing the devices is optional. ... strategy = tf.distribute.MirroredStrategy(devices=[""GPU:0"", ""GPU:1""]) >>> # Create a dataset ... dataset = tf.data.Dataset.range(4).batch(global_batch_size) >>> # Distribute that dataset ... dist_dataset = strategy.experimental_distribute_dataset(dataset) >>> @tf.function ... def replica_fn(input): ... return input*2 >>> result = [] >>> # Iterate over the `tf.distribute.DistributedDataset` ... for x in dist_dataset: ... # process dataset elements ... result.append(strategy.run(replica_fn, args=(x,))) >>> print(result) [PerReplica:{ 0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>, 1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])> }, PerReplica:{ 0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>, 1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])> }] Three key actions happening under the hood of this method are batching, sharding, and prefetching. In the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a `tf.distribute.DistributedValues` containing data for all replicas, and each replica gets data of the new batch size. `tf.distribute.Strategy.run` will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica. Sharding contains autosharding acros",torch.distributed.tensor.parallel.loss.loss_parallel,"A context manager that enables loss parallelism, where efficient parallelized loss computation can be performed when the input is sharded on the class dimension. Currently only the cross-entropy loss is supported. Within this context manager, one can use :func:`~torch.nn.functional.cross_entropy` or :class:`~torch.nn.CrossEntropyLoss` as usual, with the following assumptions on the input parameters. The corresponding ``backward()`` call, if any, also needs to happen under this context manager. Args: input (:class:`DTensor`): Input logits. Assumed to be sharded on the class dimension. target (Union[:class:`torch.Tensor`, :class:`DTensor`]): Must be ground truth class indices (class probabilities currently not supported). Assumed to be replicated across the ``DeviceMesh``. weight (Union[:class:`torch.Tensor`, :class:`DTensor`], optional): If given, assumed to be replicated across the ``DeviceMesh``. label_smoothing: Currently not supported. Returns: A replicated :class:`DTensor`. Example: A sharded DTensor is manually created here to showcase the usage. In practice, it is usually the output of a TP module. >>> # xdoctest: +SKIP(""distributed"") >>> from torch.distributed.tensor.parallel import loss_parallel >>> from torch.distributed.device_mesh import init_device_mesh >>> ... >>> device_mesh = init_device_mesh(""cuda"", (8,)) >>> input = torch.randn(4, 16, device=""cuda"", requires_grad=True) >>> dist_input = distribute_tensor(input, device_mesh, placements=[Shard(1)]) >>> target = torch.randint(16, (4,), device=""cuda"") >>> with loss_parallel(): >>> loss = F.cross_entropy(dist_input, target, reduction=""mean"") >>> loss.backward() >>> ...",0.96406186
tensorflow.python.autograph.pyct.parser.parse_expression,Returns the AST of given identifier. Args: src: A piece of code that represents a single Python expression Returns: A gast.AST object. Raises: ValueError: if src does not consist of a single Expression.,torch.onnx._internal.registration.custom_onnx_symbolic,Registers a custom symbolic function. Args: name: the qualified name of the function. opset: the opset version of the function. decorate: a sequence of decorators to apply to the function. Returns: The decorator. Raises: ValueError: If the separator '::' is not in the name.,0.963916
tensorflow.python.ops.parsing_ops.parse_single_example_v2,"Parses a single `Example` proto. Similar to `parse_example`, except: For dense tensors, the returned `Tensor` is identical to the output of `parse_example`, except there is no batch dimension, the output shape is the same as the shape given in `dense_shape`. For `SparseTensor`s, the first (batch) column of the indices matrix is removed (the indices matrix is a column vector), the values vector is unchanged, and the first (`batch_size`) entry of the shape vector is removed (it is now a single element vector). One might see performance advantages by batching `Example` protos with `parse_example` instead of using this function directly. Args: serialized: A scalar string Tensor, a single serialized Example. features: A mapping of feature keys to `FixedLenFeature` or `VarLenFeature` values. example_names: (Optional) A scalar string Tensor, the associated name. name: A name for this operation (optional). Returns: A `dict` mapping feature keys to `Tensor` and `SparseTensor` values. Raises: ValueError: if any feature is invalid.",torch.nn.utils.clip_grad.clip_grad_norm_,"Clip the gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place. Args: parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a single Tensor that will have gradients normalized max_norm (float): max norm of the gradients norm_type (float): type of the used p-norm. Can be ``'inf'`` for infinity norm. error_if_nonfinite (bool): if True, an error is thrown if the total norm of the gradients from :attr:`parameters` is ``nan``, ``inf``, or ``-inf``. Default: False (will switch to True in the future) foreach (bool): use the faster foreach-based implementation. If ``None``, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types. Default: ``None`` Returns: Total norm of the parameter gradients (viewed as a single vector).",0.96387434
tensorflow.python.ops.summary_ops_v2.set_step,"Sets the default summary step for the current thread. For convenience, this function sets a default value for the `step` parameter used in summary-writing functions elsewhere in the API so that it need not be explicitly passed in every such invocation. The value can be a constant or a variable, and can be retrieved via `tf.summary.experimental.get_step()`. Note: when using this with @tf.functions, the step value will be captured at the time the function is traced, so changes to the step outside the function will not be reflected inside the function unless using a `tf.Variable` step. Args: step: An `int64`-castable default step value, or None to unset.",torch.fx.graph.placeholder,"Insert a ``placeholder`` node into the Graph. A ``placeholder`` represents a function input. Args: name (str): A name for the input value. This corresponds to the name of the positional argument to the function this ``Graph`` represents. type_expr (Optional[Any]): an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation). default_value (Any): The default value this function argument should take on. NOTE: to allow for `None` as a default value, `inspect.Signature.empty` should be passed as this argument to specify that the parameter does _not_ have a default value. .. note:: The same insertion point and type expression rules apply for this method as ``Graph.create_node``.",0.96387297
tensorflow.python.keras.utils.data_utils.urlretrieve,"Replacement for `urlretrieve` for Python 2. Under Python 2, `urlretrieve` relies on `FancyURLopener` from legacy `urllib` module, known to have issues with proxy management. Args: url: url to retrieve. filename: where to store the retrieved data locally. reporthook: a hook function that will be called once on establishment of the network connection and once after each block read thereafter. The hook will be passed three arguments; a count of blocks transferred so far, a block size in bytes, and the total size of the file. data: `data` argument passed to `urlopen`.",torch.distributed.checkpoint.storage.read_data,Read all items from ``plan`` using ``planner`` to resolve the data. A subclass should call ``LoadPlanner::load_bytes`` to deserialize a BytesIO object into the right place. A subclass should call ``LoadPlanner::resolve_tensor`` to get access to the tensors that in should load data into. It's the StorageLayer responsibility to properly schedule any cross device copies required. Args: plan (LoadPlan): The local plan to execute on planner (LoadPlanner): The planner object to use to resolve items. Returns: A future that completes once all reads are finished.,0.9638667
tensorflow.python.ops.array_ops.concat,"Concatenates tensors along one dimension. See also `tf.tile`, `tf.stack`, `tf.repeat`. Concatenates the list of tensors `values` along dimension `axis`. If `values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated result has shape [D0, D1, ... Raxis, ...Dn] where Raxis = sum(Daxis(i)) That is, the data from the input tensors is joined along the `axis` dimension. The number of dimensions of the input tensors must match, and all dimensions except `axis` must be equal. For example: >>> t1 = [[1, 2, 3], [4, 5, 6]] >>> t2 = [[7, 8, 9], [10, 11, 12]] >>> tf.concat([t1, t2], 0) <tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)> >>> tf.concat([t1, t2], 1) <tf.Tensor: shape=(2, 6), dtype=int32, numpy= array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]], dtype=int32)> As in Python, the `axis` could also be negative numbers. Negative `axis` are interpreted as counting from the end of the rank, i.e., `axis + rank(values)`-th dimension. For example: >>> t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]] >>> t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]] >>> tf.concat([t1, t2], -1) <tf.Tensor: shape=(2, 2, 4), dtype=int32, numpy= array([[[ 1, 2, 7, 4], [ 2, 3, 8, 4]], [[ 4, 4, 2, 10], [ 5, 3, 15, 11]]], dtype=int32)> Note: If you are concatenating along a new axis consider using stack. E.g. python tf.concat([tf.expand_dims(t, axis) for t in tensors], axis) can be rewritten as python tf.stack(tensors, axis=axis) Args: values: A list of `Tensor` objects or a single `Tensor`. axis: 0-D `int32` `Tensor`. Dimension along which to concatenate. Must be in the range `[-rank(values), rank(values))`. As in Python, indexing for axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers to `axis`-th dimension. And negative axis refers to `axis + rank(values)`-th dimension. name: A name for the operation (optional). Returns: A `Tensor` resulting from concatenation of the input tensors.",torch.functional.unravel_index,"Converts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape. Args: indices (Tensor): An integer tensor containing indices into the flattened version of an arbitrary tensor of shape :attr:`shape`. All elements must be in the range ``[0, prod(shape) - 1]``. shape (int, sequence of ints, or torch.Size): The shape of the arbitrary tensor. All elements must be non-negative. Returns: tuple of Tensors: Each ``i``-th tensor in the output corresponds with dimension ``i`` of :attr:`shape`. Each tensor has the same shape as ``indices`` and contains one index into dimension ``i`` for each of the flat indices given by ``indices``. Example:: >>> import torch >>> torch.unravel_index(torch.tensor(4), (3, 2)) (tensor(2), tensor(0)) >>> torch.unravel_index(torch.tensor([4, 1]), (3, 2)) (tensor([2, 0]), tensor([0, 1])) >>> torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2)) (tensor([0, 0, 1, 1, 2, 2]), tensor([0, 1, 0, 1, 0, 1])) >>> torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10)) (tensor([1, 5]), tensor([2, 6]), tensor([3, 7]), tensor([4, 8])) >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10)) (tensor([[1], [5]]), tensor([[2], [6]]), tensor([[3], [7]]), tensor([[4], [8]])) >>> torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100)) (tensor([[12], [56]]), tensor([[34], [78]]))",0.9636613
tensorflow.python.keras.engine.training.to_json,"Returns a JSON string containing the network configuration. To load a network from a JSON save file, use `keras.models.model_from_json(json_string, custom_objects={})`. Args: **kwargs: Additional keyword arguments to be passed to `json.dumps()`. Returns: A JSON string.",torch.ao.quantization.qconfig.get_default_qconfig,"Returns the default PTQ qconfig for the specified backend. Args: * `backend` (str): a string representing the target backend. Currently supports `x86` (default), `fbgemm`, `qnnpack` and `onednn`. Return: qconfig",0.96362203
tensorflow.python.ops.lookup_ops.index_table_from_tensor,"Returns a lookup table that converts a string tensor into int64 IDs. This operation constructs a lookup table to convert tensor of strings into int64 IDs. The mapping can be initialized from a string `vocabulary_list` 1-D tensor where each element is a key and corresponding index within the tensor is the value. Any lookup of an out-of-vocabulary token will return a bucket ID based on its hash if `num_oov_buckets` is greater than zero. Otherwise it is assigned the `default_value`. The bucket ID range is `[vocabulary list size, vocabulary list size + num_oov_buckets - 1]`. The underlying table must be initialized by calling `session.run(tf.compat.v1.tables_initializer())` or `session.run(table.init())` once. Elements in `vocabulary_list` cannot have duplicates, otherwise when executing the table initializer op, it will throw a `FailedPreconditionError`. Sample Usages: python vocabulary_list = tf.constant([""emerson"", ""lake"", ""palmer""]) table = tf.lookup.index_table_from_tensor( vocabulary_list=vocabulary_list, num_oov_buckets=1, default_value=-1) features = tf.constant([""emerson"", ""lake"", ""and"", ""palmer""]) ids = table.lookup(features) ... tf.compat.v1.tables_initializer().run() ids.eval() ==> [0, 1, 4, 2] Args: vocabulary_list: A 1-D `Tensor` that specifies the mapping of keys to indices. The type of this object must be castable to `dtype`. num_oov_buckets: The number of out-of-vocabulary buckets. default_value: The value to use for out-of-vocabulary feature values. Defaults to -1. hasher_spec: A `HasherSpec` to specify the hash function to use for assignment of out-of-vocabulary buckets. dtype: The type of values passed to `lookup`. Only string and integers are supported. name: A name for this op (optional). Returns: The lookup table to map an input `Tensor` to index `int64` `Tensor`. Raises: ValueError: If `vocabulary_list` is invalid. ValueError: If `num_oov_buckets` is negative.",torch.nn.modules.module.to,"Move and/or cast the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth:`torch.Tensor.to`, but only accepts floating point or complex :attr:`dtype`\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr:`dtype` (if given). The integral parameters and buffers will be moved :attr:`device`, if that is given, but with dtypes unchanged. When :attr:`non_blocking` is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class:`torch.device`): the desired device of the parameters and buffers in this module dtype (:class:`torch.dtype`): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class:`torch.memory_format`): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1) >>> gpu1 = torch.device(""cuda:1"") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Par",0.9635998
tensorflow.python.ops.cond.cond,"Return `true_fn()` if the predicate `pred` is true else `false_fn()`. `true_fn` and `false_fn` both return lists of output tensors. `true_fn` and `false_fn` must have the same non-zero number and type of outputs. **WARNING**: Any Tensors or Operations created outside of `true_fn` and `false_fn` will be executed regardless of which branch is selected at runtime. Although this behavior is consistent with the dataflow model of TensorFlow, it has frequently surprised users who expected a lazier semantics. Consider the following simple program: python z = tf.multiply(a, b) result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y)) If `x < y`, the `tf.add` operation will be executed and `tf.square` operation will not be executed. Since `z` is needed for at least one branch of the `cond`, the `tf.multiply` operation is always executed, unconditionally. Note that `cond` calls `true_fn` and `false_fn` *exactly once* (inside the call to `cond`, and not at all during `Session.run()`). `cond` stitches together the graph fragments created during the `true_fn` and `false_fn` calls with some additional graph nodes to ensure that the right branch gets executed depending on the value of `pred`. `tf.cond` supports nested structures as implemented in `tensorflow.python.util.nest`. Both `true_fn` and `false_fn` must return the same (possibly nested) value structure of lists, tuples, and/or named tuples. Singleton lists and tuples form the only exceptions to this: when returned by `true_fn` and/or `false_fn`, they are implicitly unpacked to single values. This behavior is disabled by passing `strict=True`. Args: pred: A scalar determining whether to return the result of `true_fn` or `false_fn`. true_fn: The callable to be performed if pred is true. false_fn: The callable to be performed if pred is false. strict: A boolean that enables/disables 'strict' mode; see above. name: Optional name prefix for the returned tensors. Returns: Tensors returned by the call to either `true_fn`",torch.autograd.functional.hessian,"Compute the Hessian of a given scalar function. Args: func (function): a Python function that takes Tensor inputs and returns a Tensor with a single element. inputs (tuple of Tensors or Tensor): inputs to the function ``func``. create_graph (bool, optional): If ``True``, the Hessian will be computed in a differentiable manner. Note that when ``strict`` is ``False``, the result can not require gradients or be disconnected from the inputs. Defaults to ``False``. strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to ``False``. vectorize (bool, optional): This feature is experimental. Please consider using :func:`torch.func.hessian` instead if you are looking for something less experimental and more performant. When computing the hessian, usually we invoke ``autograd.grad`` once per row of the hessian. If this flag is ``True``, we use the vmap prototype feature as the backend to vectorize calls to ``autograd.grad`` so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use `torch._C._debug_only_display_vmap_fallback_warnings(True)` to show any performance warnings and file us issues if warnings exist for your use case. Defaults to ``False``. outer_jacobian_strategy (str, optional): The Hessian is computed by computing the Jacobian of a Jacobian. The inner Jacobian is always computed in reverse-mode AD. Setting strategy to ``""forward-mode""`` or ``""reverse-mode""`` determines whether the outer Jacobian will be computed with forward or reverse mode AD. Currently, computing the outer Jacobian in ``""forward-mode""`` requires ``vectorized=True``. Defaults to ``""reverse-mode""``. Returns: Hessian (Tensor or a tup",0.963584
tensorflow.python.ops.array_ops.transpose_v2,"Transposes `a`, where `a` is a Tensor. Permutes the dimensions according to the value of `perm`. The returned tensor's dimension `i` will correspond to the input dimension `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is the rank of the input tensor. Hence, by default, this operation performs a regular matrix transpose on 2-D input Tensors. If conjugate is `True` and `a.dtype` is either `complex64` or `complex128` then the values of `a` are conjugated and transposed. @compatibility(numpy) In `numpy` transposes are memory-efficient constant time operations as they simply return a new view of the same data with adjusted `strides`. TensorFlow does not support strides, so `transpose` returns a new tensor with the items permuted. @end_compatibility For example: >>> x = tf.constant([[1, 2, 3], [4, 5, 6]]) >>> tf.transpose(x) <tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 4], [2, 5], [3, 6]], dtype=int32)> Equivalently, you could call `tf.transpose(x, perm=[1, 0])`. If `x` is complex, setting conjugate=True gives the conjugate transpose: >>> x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j], ... [4 + 4j, 5 + 5j, 6 + 6j]]) >>> tf.transpose(x, conjugate=True) <tf.Tensor: shape=(3, 2), dtype=complex128, numpy= array([[1.-1.j, 4.-4.j], [2.-2.j, 5.-5.j], [3.-3.j, 6.-6.j]])> 'perm' is more useful for n-dimensional tensors where n > 2: >>> x = tf.constant([[[ 1, 2, 3], ... [ 4, 5, 6]], ... [[ 7, 8, 9], ... [10, 11, 12]]]) As above, simply calling `tf.transpose` will default to `perm=[2,1,0]`. To take the transpose of the matrices in dimension-0 (such as when you are transposing matrices where 0 is the batch dimension), you would set `perm=[0,2,1]`. >>> tf.transpose(x, perm=[0, 2, 1]) <tf.Tensor: shape=(2, 3, 2), dtype=int32, numpy= array([[[ 1, 4], [ 2, 5], [ 3, 6]], [[ 7, 10], [ 8, 11], [ 9, 12]]], dtype=int32)> Note: This has a shorthand `linalg.matrix_transpose`): Args: a: A `Tensor`. perm: A permutation of the dimensions of `a`. This should be a vector.",torch.functional.chain_matmul,"Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N` needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If :math:`N` is 1, then this is a no-op - the original matrix is returned as is. .. warning:: :func:`torch.chain_matmul` is deprecated and will be removed in a future PyTorch release. Use :func:`torch.linalg.multi_dot` instead, which accepts a list of two or more tensors rather than multiple arguments. Args: matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined. out (Tensor, optional): the output tensor. Ignored if :attr:`out` = ``None``. Returns: Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \times p_{i + 1}`, then the product would be of dimensions :math:`p_{1} \times p_{N + 1}`. Example:: >>> # xdoctest: +SKIP >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> a = torch.randn(3, 4) >>> b = torch.randn(4, 5) >>> c = torch.randn(5, 6) >>> d = torch.randn(6, 7) >>> # will raise a deprecation warning >>> torch.chain_matmul(a, b, c, d) tensor([[ -2.3375, -3.9790, -4.1119, -6.6577, 9.5609, -11.5095, -3.2614], [ 21.4038, 3.3378, -8.4982, -5.2457, -10.2561, -2.4684, 2.7163], [ -0.9647, -5.8917, -2.3213, -5.2284, 12.8615, -12.2816, -2.5095]]) .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition",0.9635834
tensorflow.python.debug.wrappers.framework.prepare_run_debug_urls,Abstract method to be implemented by concrete subclasses. This method prepares the run-specific debug URL(s). Args: fetches: Same as the `fetches` argument to `Session.run()` feed_dict: Same as the `feed_dict` argument to `Session.run()` Returns: debug_urls: (`str` or `list` of `str`) Debug URLs to be used in this `Session.run()` call.,torch.distributed.fsdp.fully_sharded_data_parallel.sharded_optim_state_dict,"Return the optimizer state-dict in its sharded form. The API is similar to :meth:`full_optim_state_dict` but this API chunks all non-zero-dimension states to :class:`ShardedTensor` to save memory. This API should only be used when the model ``state_dict`` is derived with the context manager ``with state_dict_type(SHARDED_STATE_DICT):``. For the detailed usage, refer to :meth:`full_optim_state_dict`. .. warning:: The returned state dict contains ``ShardedTensor`` and cannot be directly used by the regular ``optim.load_state_dict``.",0.9635308
tensorflow.python.data.ops.dataset_ops.prefetch,"Creates a `Dataset` that prefetches elements from this dataset. Most dataset input pipelines should end with a call to `prefetch`. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements. Note: Like other `Dataset` methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. `examples.prefetch(2)` will prefetch two elements (2 examples), while `examples.batch(20).prefetch(2)` will prefetch 2 elements (2 batches, of 20 examples each). >>> dataset = tf.data.Dataset.range(3) >>> dataset = dataset.prefetch(2) >>> list(dataset.as_numpy_iterator()) [0, 1, 2] Args: buffer_size: A `tf.int64` scalar `tf.Tensor`, representing the maximum number of elements that will be buffered when prefetching. If the value `tf.data.AUTOTUNE` is used, then the buffer size is dynamically tuned. name: Optional. A name for the tf.data transformation. Returns: A new `Dataset` with the transformation applied as described above.",torch.optim.swa_utils.update_bn,"Updates BatchNorm running_mean, running_var buffers in the model. It performs one pass over data in `loader` to estimate the activation statistics for BatchNorm layers in the model. Args: loader (torch.utils.data.DataLoader): dataset loader to compute the activation statistics on. Each data batch should be either a tensor, or a list/tuple whose first element is a tensor containing data. model (torch.nn.Module): model for which we seek to update BatchNorm statistics. device (torch.device, optional): If set, data will be transferred to :attr:`device` before being passed into :attr:`model`. Example: >>> # xdoctest: +SKIP(""Undefined variables"") >>> loader, model = ... >>> torch.optim.swa_utils.update_bn(loader, model) .. note:: The `update_bn` utility assumes that each data batch in :attr:`loader` is either a tensor or a list or tuple of tensors; in the latter case it is assumed that :meth:`model.forward()` should be called on the first element of the list or tuple corresponding to the data batch.",0.96342576
tensorflow.python.debug.lib.grpc_debug_test_server.query_op_traceback,"Query the traceback of an op. Args: op_name: Name of the op to query. Returns: The traceback of the op, as a list of 3-tuples: (filename, lineno, function_name) Raises: ValueError: If the op cannot be found in the tracebacks received by the server so far.",torch.onnx._internal.exporter.register_op,"Registers a custom operator: torch.ops.<namespace>.<op_name>.<overload>. Args: function: The onnx-sctip function to register. namespace: The namespace of the operator to register. op_name: The name of the operator to register. overload: The overload of the operator to register. If it's default overload, leave it to None. is_complex: Whether the function is a function that handles complex valued inputs. Raises: ValueError: If the name is not in the form of 'namespace::op'.",0.96326876
tensorflow.python.framework.override_binary_operator.maybe_promote_tensors,"Promotes tensors if numpy style promotion is enabled. This function promotes `tensors` according to numpy promotion rules if numpy style promotion is enabled. Otherwise, if `force_same_dtype` is `True`, it force-casts `tensors[1:]` to `tensor[0]`'s dtype. Note that this force-cast can be problematic. For example, when some `tensors[1:]` elements can be silently downcasted. Args: *tensors: the list of tensors to promote. force_same_dtype: bool (optional, default to `False`). When numpy style promotion is disabled and `force_same_dtype` is `True`, this function will force-casts `tensors[1:]` to `tensor[0]`'s dtype (which could be problematic). Returns: The promoted list of tensors.",torch.distributed._shard.sharded_tensor.api.gather,"Creates a full :class:`Tensor` on rank ``dst`` by gathering all shards of the sharded tensor. The API needs to be called on all ranks in SPMD fashion. All ranks should have the same ``dst``. ``out`` should be a tensor of the same size as the overall size of the sharded tensor on ``dst`` and ``None`` on all other ranks. Args: dst(int): The rank where full tensor is constructed. Default: 0 out (:class `torch.Tensor`, optional): The output full tensor. Must to be provided ONLY on ``dst`` rank. Default: ``None`` enforce_dtype (bool): Deprecated, please use dtype instead. Force the gathered tensors to be the same type as input and output. dtype (torch.dtype): Force the gathered tensors to be this dtype. Default: ``None``",0.9631831
tensorflow.python.util.dispatch.dispatch_for_unary_elementwise_apis,"Decorator to override default implementation for unary elementwise APIs. The decorated function (known as the ""elementwise api handler"") overrides the default implementation for any unary elementwise API whenever the value for the first argument (typically named `x`) matches the type annotation `x_type`. The elementwise api handler is called with two arguments: `elementwise_api_handler(api_func, x)` Where `api_func` is a function that takes a single parameter and performs the elementwise operation (e.g., `tf.abs`), and `x` is the first argument to the elementwise api. The following example shows how this decorator can be used to update all unary elementwise operations to handle a `MaskedTensor` type: >>> class MaskedTensor(tf.experimental.ExtensionType): ... values: tf.Tensor ... mask: tf.Tensor >>> @dispatch_for_unary_elementwise_apis(MaskedTensor) ... def unary_elementwise_api_handler(api_func, x): ... return MaskedTensor(api_func(x.values), x.mask) >>> mt = MaskedTensor([1, -2, -3], [True, False, True]) >>> abs_mt = tf.abs(mt) >>> print(f""values={abs_mt.values.numpy()}, mask={abs_mt.mask.numpy()}"") values=[1 2 3], mask=[ True False True] For unary elementwise operations that take extra arguments beyond `x`, those arguments are *not* passed to the elementwise api handler, but are automatically added when `api_func` is called. E.g., in the following example, the `dtype` parameter is not passed to `unary_elementwise_api_handler`, but is added by `api_func`. >>> ones_mt = tf.ones_like(mt, dtype=tf.float32) >>> print(f""values={ones_mt.values.numpy()}, mask={ones_mt.mask.numpy()}"") values=[1.0 1.0 1.0], mask=[ True False True] Args: x_type: A type annotation indicating when the api handler should be called. See `dispatch_for_api` for a list of supported annotation types. Returns: A decorator. #### Registered APIs The unary elementwise APIs are: <<API_LIST>>",torch._functorch.apis.grad,"``grad`` operator helps computing gradients of ``func`` with respect to the input(s) specified by ``argnums``. This operator can be nested to compute higher-order gradients. Args: func (Callable): A Python function that takes one or more arguments. Must return a single-element Tensor. If specified ``has_aux`` equals ``True``, function can return a tuple of single-element Tensor and other auxiliary objects: ``(output, aux)``. argnums (int or Tuple[int]): Specifies arguments to compute gradients with respect to. ``argnums`` can be single integer or tuple of integers. Default: 0. has_aux (bool): Flag indicating that ``func`` returns a tensor and other auxiliary objects: ``(output, aux)``. Default: False. Returns: Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified ``has_aux`` equals ``True``, tuple of gradients and output auxiliary objects is returned. If ``argnums`` is a tuple of integers, a tuple of output gradients with respect to each ``argnums`` value is returned. Example of using ``grad``: >>> # xdoctest: +SKIP >>> from torch.func import grad >>> x = torch.randn([]) >>> cos_x = grad(lambda x: torch.sin(x))(x) >>> assert torch.allclose(cos_x, x.cos()) >>> >>> # Second-order gradients >>> neg_sin_x = grad(grad(lambda x: torch.sin(x)))(x) >>> assert torch.allclose(neg_sin_x, -x.sin()) When composed with ``vmap``, ``grad`` can be used to compute per-sample-gradients: >>> # xdoctest: +SKIP >>> from torch.func import grad, vmap >>> batch_size, feature_size = 3, 5 >>> >>> def model(weights, feature_vec): >>> # Very simple linear model with activation >>> assert feature_vec.dim() == 1 >>> return feature_vec.dot(weights).relu() >>> >>> def compute_loss(weights, example, target): >>> y = model(weights, example) >>> return ((y - target) ** 2).mean() # MSELoss >>> >>> weights = torch.randn(feature_size, requires_grad=True) >>> examples = torch.randn(batch_s",0.96314967
tensorflow.python.ops.array_ops.one_hot,"Returns a one-hot tensor. See also `tf.fill`, `tf.eye`. The locations represented by indices in `indices` take value `on_value`, while all other locations take value `off_value`. `on_value` and `off_value` must have matching data types. If `dtype` is also provided, they must be the same data type as specified by `dtype`. If `on_value` is not provided, it will default to the value `1` with type `dtype` If `off_value` is not provided, it will default to the value `0` with type `dtype` If the input `indices` is rank `N`, the output will have rank `N+1`. The new axis is created at dimension `axis` (default: the new axis is appended at the end). If `indices` is a scalar the output shape will be a vector of length `depth` If `indices` is a vector of length `features`, the output shape will be: features x depth if axis == -1 depth x features if axis == 0 If `indices` is a matrix (batch) with shape `[batch, features]`, the output shape will be: batch x features x depth if axis == -1 batch x depth x features if axis == 1 depth x batch x features if axis == 0 If `indices` is a RaggedTensor, the 'axis' argument must be positive and refer to a non-ragged axis. The output will be equivalent to applying 'one_hot' on the values of the RaggedTensor, and creating a new RaggedTensor from the result. If `dtype` is not provided, it will attempt to assume the data type of `on_value` or `off_value`, if one or both are passed in. If none of `on_value`, `off_value`, or `dtype` are provided, `dtype` will default to the value `tf.float32`. Note: If a non-numeric data type output is desired (`tf.string`, `tf.bool`, etc.), both `on_value` and `off_value` _must_ be provided to `one_hot`. For example: python indices = [0, 1, 2] depth = 3 tf.one_hot(indices, depth) # output: [3 x 3] # [[1., 0., 0.], # [0., 1., 0.], # [0., 0., 1.]] indices = [0, 2, -1, 1] depth = 3 tf.one_hot(indices, depth, on_value=5.0, off_value=0.0, axis=-1) # output: [4 x 3] # [[5.0, 0.0, 0.0], # one_hot(0) # [0.0, 0.0, 5.0],",torch.distributed._tensor.experimental.local_map.local_map,"``local_map`` is an experimental API that allows users to apply on :class:`DTensors` a function that is written to be applied on :class:`~torch.Tensors`. Args: func (Callable): the function to be applied on each local shard of :class:`DTensor`s. out_placements (Union[`PlacementType`, Tuple[`PlacementType`, ...]]): the desired placements of the :class:`DTensor`s in `func`'s flattened output. If the flattened `output` is a single value, the `out_placements` should be of type `PlacementType`. Otherwise if the flattened `output` has multiple values, the `out_placements` should be a tuple of `PlacementType` values 1:1 mapping to the flattened `output`. Besides, for :class:`Tensor` output, we use `PlacementType` as its placements (a `Tuple[Placement]` value). For non-:class:`Tensor` output, the `PlacementType` should be `None`. Note that the only exception is when no :class:`DTensor` argument is passed in. In this case, even if `out_placements` is not `None`, the result function should ignore the desired placements because the application is not on :class:`DTensors`. in_placements (Tuple[`PlacementType`, ...], optional): the required placements of the :class:`DTensor`s in `func`'s flattened input. If `in_placements` is specified, `local_map` would examine whether the placements of each :class:`DTensor` argument is the same as the required placements or not. If the placements are not the same and `redistribute_inputs` is `False`, an exception will be raised. Otherwise if `redistribute_inputs` is `True`, the argument will be first redistributed to the required sharding placements before passing its local tensor to `func`. The only exception is when required placements are not `None` and the argument is a :class:`torch.Tensor`. In this case, the placements examination will be skipped and the argument will be directly passed to `func`. If `in_placements` is `None`, no placements examination will be performed. Default: `None` device_mesh (:class:`DeviceMesh`, optional): the de",0.9631106
tensorflow.python.framework.type_spec_registry.register,"Decorator used to register a globally unique name for a TypeSpec subclass. Args: name: The name of the type spec. Must be globally unique. Must have the form `""{project_name}.{type_name}""`. E.g. `""my_project.MyTypeSpec""`. Returns: A class decorator that registers the decorated class with the given name.",torch.package.package_importer.load_pickle,"Unpickles the resource from the package, loading any modules that are needed to construct the objects using :meth:`import_module`. Args: package (str): The name of module package (e.g. ``""my_package.my_subpackage""``). resource (str): The unique name for the resource. map_location: Passed to `torch.load` to determine how tensors are mapped to devices. Defaults to ``None``. Returns: Any: The unpickled object.",0.96310186
tensorflow.python.debug.lib.debug_utils.add_debug_tensor_watch,"Add watch on a `Tensor` to `RunOptions`. N.B.: 1. Under certain circumstances, the `Tensor` may not get actually watched (e.g., if the node of the `Tensor` is constant-folded during runtime). 2. For debugging purposes, the `parallel_iteration` attribute of all `tf.while_loop`s in the graph are set to 1 to prevent any node from being executed multiple times concurrently. This change does not affect subsequent non-debugged runs of the same `tf.while_loop`s. Args: run_options: An instance of `config_pb2.RunOptions` to be modified. node_name: (`str`) name of the node to watch. output_slot: (`int`) output slot index of the tensor from the watched node. debug_ops: (`str` or `list` of `str`) name(s) of the debug op(s). Can be a `list` of `str` or a single `str`. The latter case is equivalent to a `list` of `str` with only one element. For debug op types with customizable attributes, each debug op string can optionally contain a list of attribute names, in the syntax of: debug_op_name(attr_name_1=attr_value_1;attr_name_2=attr_value_2;...) debug_urls: (`str` or `list` of `str`) URL(s) to send debug values to, e.g., `file:///tmp/tfdbg_dump_1`, `grpc://localhost:12345`. tolerate_debug_op_creation_failures: (`bool`) Whether to tolerate debug op creation failures by not throwing exceptions. global_step: (`int`) Optional global_step count for this debug tensor watch.",torch.distributed.checkpoint.state_dict_saver.save,"Save a distributed model in SPMD style. This function is different from ``torch.save()`` as it handles ``ShardedTensor`` , and ``DTensor`` by having each rank only save their local shards. For each ``Stateful`` object (having both a ``state_dict`` and a ``load_state_dict``), save will call ``state_dict`` before serialization. .. warning:: There is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts. .. warning:: If using the `process_group` argument, make sure that only its ranks call `save_state_dict` and that all data in state_dict belong to it. .. note:: When saving checkpoint for FSDP's `ShardingStrategy.HYBRID_SHARD`, only one of the shard_group should be calling `save_state_dict` and the corresponding process group needs to be passed in. .. note:: If no process group is available, this function assumes the intention is to save the state_dict in the local process. .. note: Rank 0 is assumed to be the coordinator rank. Args: state_dict (Dict[str, Any]): The state_dict to save. checkpoint_id (Union[str, os.PathLike, None]): The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: ``None``) storage_writer (Optional[StorageWriter]): Instance of StorageWriter used to perform writes. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: ``None``) planner (Optional[SavePlanner]): Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: ``None``) process_group (Optional[ProcessGroup]): ProcessGroup to be used for cross-rank synchronization. (Default: ``None``) Returns: Metadata: Metadata object for the saved checkpoint. Example: >>> # xdoctest: +SKIP >>> my_model = MyModule() >>> state_dict = {""model"": my_model} >>> fs_storage_writer = torch.di",0.9630312
tensorflow.python.keras.saving.hdf5_format.load_attributes_from_hdf5_group,Loads attributes of the specified name from the HDF5 group. This method deals with an inherent problem of HDF5 file which is not able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes. Args: group: A pointer to a HDF5 group. name: A name of the attributes to load. Returns: data: Attributes data.,torch.onnx._internal.onnx_proto_utils.export_data,"Export data to ONNX protobuf format. Args: data: The data to export, nested data structure of numpy.ndarray. value_info_proto: The ValueInfoProto of the data. The type of the ValueInfoProto determines how the data is stored. f: The file to write the data to.",0.96299034
tensorflow.python.keras.callbacks.on_test_batch_begin,"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Args: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.test_step`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`.",torch.distributed.optim.zero_redundancy_optimizer.state_dict,"Return the last global optimizer state known to this rank. .. warning: If the state has not been consolidated to this rank, this raises a runtime error, and even if it has, the state may not be up-to-date, depending on when :meth:`consolidate_state_dict` was last called. Raises: RuntimeError: if ``overlap_with_ddp=True`` and this method is called before this :class:`ZeroRedundancyOptimizer` instance has been fully initialized, which happens once :class:`DistributedDataParallel` gradient buckets have been rebuilt; or if this method is called without a preceding call to :meth:`consolidate_state_dict`.",0.9628908
tensorflow.python.framework.tfrt_utils.enabled,Returns true if TFRT should be enabled.,torch.distributed.pipelining.stage.is_last,Returns true if this stage is the last stage in the pipeline.,0.9626802
tensorflow.python.ops.losses.losses_impl.sparse_softmax_cross_entropy,"Cross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`. `weights` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `weights` is a tensor of shape `[batch_size]`, then the loss weights apply to each corresponding sample. Args: labels: `Tensor` of shape `[d_0, d_1, ..., d_{r-1}]` (where `r` is rank of `labels` and result) and dtype `int32` or `int64`. Each entry in `labels` must be an index in `[0, num_classes)`. Other values will raise an exception when this op is run on CPU, and return `NaN` for corresponding loss and gradient rows on GPU. logits: Unscaled log probabilities of shape `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float16`, `float32` or `float64`. weights: Coefficients for the loss. This must be scalar or broadcastable to `labels` (i.e. same rank and each dimension is either 1 or the same). scope: the scope for the operations performed in computing the loss. loss_collection: collection to which the loss will be added. reduction: Type of reduction to apply to loss. Returns: Weighted loss `Tensor` of the same type as `logits`. If `reduction` is `NONE`, this has the same shape as `labels`; otherwise, it is scalar. Raises: ValueError: If the shapes of `logits`, `labels`, and `weights` are incompatible, or if any of them are None. @compatibility(eager) The `loss_collection` argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a `tf.keras.Model`. @end_compatibility",torch.nn.utils.prune.l1_unstructured,"Prune tensor by removing units with the lowest L1-norm. Prunes tensor corresponding to parameter called ``name`` in ``module`` by removing the specified `amount` of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called ``name+'_mask'`` corresponding to the binary mask applied to the parameter ``name`` by the pruning method. 2) replacing the parameter ``name`` by its pruned version, while the original (unpruned) parameter is stored in a new parameter named ``name+'_orig'``. Args: module (nn.Module): module containing the tensor to prune name (str): parameter name within ``module`` on which pruning will act. amount (int or float): quantity of parameters to prune. If ``float``, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If ``int``, it represents the absolute number of parameters to prune. importance_scores (torch.Tensor): tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place. Returns: module (nn.Module): modified (i.e. pruned) version of the input module Examples: >>> # xdoctest: +SKIP >>> m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2) >>> m.state_dict().keys() odict_keys(['bias', 'weight_orig', 'weight_mask'])",0.96267843
tensorflow.python.tpu.tpu_embedding_v3.apply_gradients,"Applies the gradient update to the embedding tables. If a gradient of `None` is passed in any position of the nested structure, then a gradient update with a zero gradient is applied for that feature. For optimizers like SGD or Adagrad, this is the same as applying no update at all. For lazy Adam and other sparsely applied optimizers with decay, ensure you understand the effect of applying a zero gradient. Args: gradients: A nested structure of gradients, with structure matching the `feature_config` passed to this object. preserved_outputs: A dicts of PartitionedCsrFormatTensor, coming from the second output of the embedding lookup call. Raises: RuntimeError: if not built. ValueError: If a non-`tf.Tensor` non-`None` gradient is passed in, or a `tf.Tensor` of the incorrect shape is passed in. Also if the size of any sequence in `gradients` does not match corresponding sequence in `feature_config`. TypeError: If the type of any sequence in `gradients` does not match corresponding sequence in `feature_config`.",torch.distributed._tensor.api.to_local,"Get the local tensor of this DTensor on its current rank. For sharding it returns a local shard of the logical tensor view, for replication it returns the replica on its current rank. Keyword args: grad_placements (List[:class:`Placement`], optional): the placements describes the future layout of any gradient layout of the Tensor returned from this function. `to_local` converts DTensor to local tensor and the returned local tensor might not be used as the original DTensor layout later in the code. This argument is the hint that user can give to autograd in case the gradient layout of the returned tensor does not match the original DTensor layout. If not specified, we will assume the gradient layout remains the same as the original DTensor and use that for gradient computation. Returns: A :class:`torch.Tensor` or `AsyncCollectiveTensor` object. it represents the local tensor on its current rank. .. note:: `to_local` is differentiable, the `requires_grad` of the local tensor returned will depend on if the `DTensor` requires_grad or not.",0.9626309
tensorflow.python.ops.tensor_array_ops.write,Write `value` into index `index` of the TensorArray. Args: index: 0-D. int32 scalar with the index to write to. value: N-D. Tensor of type `dtype`. The Tensor to write to this index. name: A name for the operation (optional). Returns: A new TensorArray object with flow that ensures the write occurs. Use this object for all subsequent operations. Raises: ValueError: if there are more writers than specified.,torch.distributed.nn.functional.scatter,"Scatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the ``tensor`` argument. Arguments: tensors (list[Tensor]): List of tensors to scatter on the source rank. Receivers must pass ``None`. src (int, optional): Source rank (default is 0). group (ProcessGroup, optional): The process group to work on. Returns: Tensor: Output tensor from the scatter operation.",0.96259314
tensorflow.python.util.type_annotations.is_generic_list,Returns true if `tp` is a parameterized typing.List value.,torch.nn.utils.rnn.is_pinned,Return true if `self.data` stored on in pinned memory.,0.9625624
tensorflow.python.keras.legacy_tf_layers.core.flatten,"Flattens an input tensor while preserving the batch axis (axis 0). Args: inputs: Tensor input. name: The name of the layer (string). data_format: A string, one of `channels_last` (default) or `channels_first`. The ordering of the dimensions in the inputs. `channels_last` corresponds to inputs with shape `(batch, height, width, channels)` while `channels_first` corresponds to inputs with shape `(batch, channels, height, width)`. Returns: Reshaped tensor. Examples: x = tf.compat.v1.placeholder(shape=(None, 4, 4), dtype='float32') y = flatten(x) # now `y` has shape `(None, 16)` x = tf.compat.v1.placeholder(shape=(None, 3, None), dtype='float32') y = flatten(x) # now `y` has shape `(None, None)`",torch.nn.utils._expanded_weights.expanded_weights_utils.sum_over_all_but_batch_and_last_n,"Calculate the sum over all dimensions, except the first (batch dimension), and excluding the last n_dims. This function will ignore the first dimension and it will not aggregate over the last n_dims dimensions. Args: tensor: An input tensor of shape ``(B, ..., X[n_dims-1])``. n_dims: Number of dimensions to keep. Example: >>> tensor = torch.ones(1, 2, 3, 4, 5) >>> sum_over_all_but_batch_and_last_n(tensor, n_dims=2).shape torch.Size([1, 4, 5]) Returns: A tensor of shape ``(B, ..., X[n_dims-1])``",0.9625589
tensorflow.python.keras.saving.saved_model.base_serialization.functions_to_serialize,"Returns extra functions to include when serializing a Keras object. Normally, when calling exporting an object to SavedModel, only the functions and objects defined by the user are saved. For example: obj = tf.Module() obj.v = tf.Variable(1.) @tf.function def foo(...): ... obj.foo = foo w = tf.Variable(1.) tf.saved_model.save(obj, 'path/to/saved/model') loaded = tf.saved_model.load('path/to/saved/model') loaded.v # Variable with the same value as obj.v loaded.foo # Equivalent to obj.foo loaded.w # AttributeError Assigning trackable objects to attributes creates a graph, which is used for both checkpointing and SavedModel serialization. When the graph generated from attribute tracking is insufficient, extra objects and functions may be added at serialization time. For example, most models do not have their call function wrapped with a @tf.function decorator. This results in `model.call` not being saved. Since Keras objects should be revivable from the SavedModel format, the call function is added as an extra function to serialize. This function and `objects_to_serialize` is called multiple times when exporting to SavedModel. Please use the cache to avoid generating new functions and objects. A fresh cache is created for each SavedModel export. Args: serialization_cache: Dictionary passed to all objects in the same object graph during serialization. Returns: A dictionary mapping attribute names to `Function` or `ConcreteFunction`.",torch.onnx._internal.fx.serialization.save_model_with_external_data,"Load PyTorch tensors from files and add to ""onnx_model"" as external initializers. Output files: ONNX model file path: ONNX initializer folder: os.path.join(basepath, initializer_location) After running this function, you can do ort_sess = onnxruntime.InferenceSession(os.path.join(basepath, model_location)) to execute the model. Arguments: basepath: Base path of the ONNX external data file (e.g., ""/path/to/large_model/""). model_location: Relative location of the ONNX model file. E.g., ""model.onnx"" so that the model file is saved to ""<basepath>/model.onnx"". initializer_location: Relative location of the ONNX initializer folder. E.g., ""initializers"" so that the initializers are saved to ""<basepath>/initializers/"". Note: When initializers are >2GB, must be the same as `model_location`. torch_state_dicts: Dictionaries or files which contain PyTorch tensors to be saved as ONNX initializers. For non-dict arguments, `torch.load` will be used to load them from file-like objects. onnx_model: ONNX model to be saved with external initializers. If an input name matches a tensor loaded from ""torch_state_dicts"", the tensor will be saved as that input's external initializer. rename_initializer: Replaces ""."" by ""_"" for all ONNX initializer names. Not needed by the official torch.onnx.dynamo_export. This is a hack for supporting `FXSymbolicTracer` tracer with fake tensor mode. In short, `FXSymbolicTracer` lifts FX parameters (self.linear_weight) as inputs (`def forward(self, linear_weight)`) and therefore, `.` cannot be used.",0.9625387
tensorflow.python.ops.string_ops.string_format,"Formats a string template using a list of tensors. Formats a string template using a list of tensors, abbreviating tensors by only printing the first and last `summarize` elements of each dimension (recursively). If formatting only one tensor into a template, the tensor does not have to be wrapped in a list. Example: Formatting a single-tensor template: >>> tensor = tf.range(5) >>> tf.strings.format(""tensor: {}, suffix"", tensor) <tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'> Formatting a multi-tensor template: >>> tensor_a = tf.range(2) >>> tensor_b = tf.range(1, 4, 2) >>> tf.strings.format(""a: {}, b: {}, suffix"", (tensor_a, tensor_b)) <tf.Tensor: shape=(), dtype=string, numpy=b'a: [0 1], b: [1 3], suffix'> Args: template: A string template to format tensor values into. inputs: A list of `Tensor` objects, or a single Tensor. The list of tensors to format into the template string. If a solitary tensor is passed in, the input tensor will automatically be wrapped as a list. placeholder: An optional `string`. Defaults to `{}`. At each placeholder occurring in the template, a subsequent tensor will be inserted. summarize: An optional `int`. Defaults to `3`. When formatting the tensors, show the first and last `summarize` entries of each tensor dimension (recursively). If set to -1, all elements of the tensor will be shown. name: A name for the operation (optional). Returns: A scalar `Tensor` of type `string`. Raises: ValueError: if the number of placeholders does not match the number of inputs.",torch.distributed.distributed_c10d.all_gather_into_tensor,"Gather tensors from all ranks and put them in a single output tensor. Args: output_tensor (Tensor): Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of ""concatenation"", see ``torch.cat()``; (ii) a stack of all the input tensors along the primary dimension; for definition of ""stack"", see ``torch.stack()``. Examples below may better explain the supported output forms. input_tensor (Tensor): Tensor to be gathered from current rank. Different from the ``all_gather`` API, the input tensors in this API must have the same size across all ranks. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group Examples: >>> # xdoctest: +SKIP(""need process group init"") >>> # All tensors below are of torch.int64 dtype and on CUDA devices. >>> # We have two ranks. >>> device = torch.device(f'cuda:{rank}') >>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank >>> tensor_in tensor([1, 2], device='cuda:0') # Rank 0 tensor([3, 4], device='cuda:1') # Rank 1 >>> # Output in concatenation form >>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device) >>> dist.all_gather_into_tensor(tensor_out, tensor_in) >>> tensor_out tensor([1, 2, 3, 4], device='cuda:0') # Rank 0 tensor([1, 2, 3, 4], device='cuda:1') # Rank 1 >>> # Output in stack form >>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device) >>> dist.all_gather_into_tensor(tensor_out2, tensor_in) >>> tensor_out2 tensor([[1, 2], [3, 4]], device='cuda:0') # Rank 0 tensor([[1, 2], [3, 4]], device='cuda:1') # Rank 1 .. warning:: The Gloo backend does not support this API.",0.9625163
tensorflow.python.ops.variables.initialize_local_variables,See `tf.compat.v1.local_variables_initializer`.,torch.autograd.function.saved_tensors,See :meth:`Function.saved_tensors`.,0.96250725
tensorflow.python.autograph.impl.api.tf_convert,"Decorator that applies AutoGraph to a function. Use in internal APIs. This API is suitable for high order functions internal to the TensorFlow API, and more generally any function to which AutoGraph is not applied. Guidance: `convert` was a decorator meant for use directly by developers, but most of today's uses go through `tf.function`. `tf_convert` is to be called from high order functions internal to TF. By default, all the internal TensorFlow functions are skipped when AutoGraph processes the code. This may lead to user-supplied functions to be incorrectly skipped as well. `tf_convert` helps avoid that. See the following example for more details. =====tf_internal_module.py===== def unconverted(input_fn): return input_fn() def converted(input_fn): return tf.__internal__.autograph.tf_convert( input_fn, ctx=tf.__internal__.autograph.control_status_ctx())() ======user_module.py====== @tf.function def foo(input_fn) return unconverted(input_fn) @tf.function def bar(input_fn) return converted(input_fn) @tf.function(autograph=False) def baz(input_fn) return converted(input_fn) The `foo` method above will execute the `input_fn` without autograph conversion, while the `bar` method will run an autographed `input_fn`. The `baz` method will run an unconverted `input_fn`, since `tf_convert` respect the control status context. Note that both methods in `tf_internal_module` are skipped by autograph when tracing the `tf.function`. The configuration of whether a module/package should be skipped by autograph is controlled in tensorflow/python/autograph/core/config.py. Args: f: Callable. ctx: ag_ctx.ControlStatusCtx, the Autograph context in which `f` is used. convert_by_default: bool, whether to use AutoGraph when the context doesn't specify. user_requested: bool, whether to ignore the conversion allowlist. See ConversionOptions.user_requested. Returns: Either `f or the converted version of `f`.",torch.distributed.checkpoint.state_dict.get_state_dict,"Return the model state_dict and optimizers state_dict. ``get_state_dict`` can process any module that is parallelized by PyTorch FSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any combination of these parallelisms. The main functions of ``get_state_dict`` are: 1.) returning a model and optimizer state_dict that can be resharded with a different number of trainers and/or different parallelisms. 2.) hiding the parallelism-specific state_dict APIs. Users don't have to call these APIs. 3.) sanity checking the result state_dict. The keys of the result state dictionary are the canonical FQNs (Fully Qualified Names). A canonical FQN refers to the FQN based on a parameter's position in an nn.Module hierarchy. More specifically, a canonical FQN to a parameter is the FQN returned by ``module.named_parameters()`` or ``module.named_buffers()`` when the module is not distributed by any parallelisms. Since the optimizer internally uses parameter IDs to represent a parameter, there will be a conversion from the parameter IDs to the canonical FQNs when calling this API. ``get_state_dict`` can also process a module that is not parallelized. In such a case, ``get_state_dict`` only performs one function -- converting the optimizer parameter IDs to the canonical FQNs. Example: >>> # xdoctest: +SKIP >>> import torch >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP >>> from torch.nn.parallel import DistributedDataParallel as DDP >>> from torch.distributed.checkpoint.state_dict import get_state_dict >>> fsdp_model = FSDP(copy.deepcopy(model)) >>> fsdp_optim = torch.optim.Adam(model.parameters(), lr=1e-3) >>> ddp_model = DDP(copy.deepcopy(model)) >>> ddp_optim = torch.optim.Adam(model.parameters(), lr=1e-3) >>> ddp_state_dict, ddp_optim_state_dict = get_state_dict(ddp_model, ddp_optim) >>> fsdp_state_dict, fsdp_optim_state_dict = get_state_dict(fsdp_model, fsdp_optim) >>> # if we simply call ddp_model.state_dict() and fsdp_model.state_dict()",0.9624038
tensorflow.python.data.ops.iterator_ops.from_structure,"Creates a new, uninitialized `Iterator` with the given structure. This iterator-constructing method can be used to create an iterator that is reusable with many different datasets. The returned iterator is not bound to a particular dataset, and it has no `initializer`. To initialize the iterator, run the operation returned by `Iterator.make_initializer(dataset)`. The following is an example python iterator = Iterator.from_structure(tf.int64, tf.TensorShape([])) dataset_range = Dataset.range(10) range_initializer = iterator.make_initializer(dataset_range) dataset_evens = dataset_range.filter(lambda x: x % 2 == 0) evens_initializer = iterator.make_initializer(dataset_evens) # Define a model based on the iterator; in this example, the model_fn # is expected to take scalar tf.int64 Tensors as input (see # the definition of 'iterator' above). prediction, loss = model_fn(iterator.get_next()) # Train for `num_epochs`, where for each epoch, we first iterate over # dataset_range, and then iterate over dataset_evens. for _ in range(num_epochs): # Initialize the iterator to `dataset_range` sess.run(range_initializer) while True: try: pred, loss_val = sess.run([prediction, loss]) except tf.errors.OutOfRangeError: break # Initialize the iterator to `dataset_evens` sess.run(evens_initializer) while True: try: pred, loss_val = sess.run([prediction, loss]) except tf.errors.OutOfRangeError: break Args: output_types: A (nested) structure of `tf.DType` objects corresponding to each component of an element of this dataset. output_shapes: (Optional.) A (nested) structure of `tf.TensorShape` objects corresponding to each component of an element of this dataset. If omitted, each component will have an unconstrainted shape. shared_name: (Optional.) If non-empty, this iterator will be shared under the given name across multiple sessions that share the same devices (e.g. when using a remote server). output_classes: (Optional.) A (nested) structure of Python `type` objects corresponding to ea",torch.ao.quantization.quantize_jit.quantize_jit,"Quantize the input float TorchScript model with post training static quantization. First it will prepare the model for calibration, then it calls `run_fn` which will run the calibration step, after that we will convert the model to a quantized model. Args: `model`: input float TorchScript model `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and qconfig for that module as value, empty key means the qconfig will be applied to whole model unless it's overwritten by more specific configurations, the qconfig for each module is either found in the dictionary or fallback to the qconfig of parent module. Right now qconfig_dict is the only way to configure how the model is quantized, and it is done in the granularity of module, that is, we only support one type of qconfig for each torch.nn.Module, and the qconfig for sub module will override the qconfig for parent module, empty string means global configuration. `run_fn`: a calibration function for calibrating the prepared model `run_args`: positional arguments for `run_fn` `inplace`: carry out model transformations in-place, the original module is mutated `debug`: flag for producing a debug friendly model (preserve weight attribute) Return: Quantized TorchSciprt model. Example: python import torch from torch.ao.quantization import get_default_qconfig from torch.ao.quantization import quantize_jit ts_model = torch.jit.script(float_model.eval()) # or torch.jit.trace(float_model, input) qconfig = get_default_qconfig('fbgemm') def calibrate(model, data_loader): model.eval() with torch.no_grad(): for image, target in data_loader: model(image) quantized_model = quantize_jit( ts_model, {'': qconfig}, calibrate, [data_loader_test])",0.96238434
tensorflow.python.tpu.tpu_embedding_v2.embedding_tables,"Returns a dict of embedding tables, keyed by `TableConfig`. This property only works when the `TPUEmbedding` object is created under a non-TPU strategy. This is intended to be used to for CPU based lookup when creating a serving checkpoint. Returns: A dict of embedding tables, keyed by `TableConfig`. Raises: RuntimeError: If object was created under a `TPUStrategy`.",torch.distributed.optim.zero_redundancy_optimizer.join_hook,Return the ZeRO join hook. It enables training on uneven inputs by shadowing the collective communications in the optimizer step. Gradients must be properly set before this hook is called. Arguments: kwargs (dict): a :class:`dict` containing any keyword arguments to modify the behavior of the join hook at run time; all :class:`Joinable` instances sharing the same join context manager are forwarded the same value for ``kwargs``. This hook does not support any keyword arguments; i.e. ``kwargs`` is unused.,0.9623313
tensorflow.python.summary.writer.writer.add_graph,"Adds a `Graph` to the event file. The graph described by the protocol buffer will be displayed by TensorBoard. Most users pass a graph in the constructor instead. Args: graph: A `Graph` object, such as `sess.graph`. global_step: Number. Optional global step counter to record with the graph. graph_def: DEPRECATED. Use the `graph` parameter instead. Raises: ValueError: If both graph and graph_def are passed to the method.",torch.utils.tensorboard.writer.add_graph,"Add graph data to summary. Args: model (torch.nn.Module): Model to draw. input_to_model (torch.Tensor or list of torch.Tensor): A variable or a tuple of variables to be fed. verbose (bool): Whether to print graph structure in console. use_strict_trace (bool): Whether to pass keyword argument `strict` to `torch.jit.trace`. Pass False when you want the tracer to record your mutable container types (list, dict)",0.96229976
tensorflow.python.keras.backend.set_value,"Sets the value of a variable, from a Numpy array. `backend.set_value` is the complement of `backend.get_value`, and provides a generic interface for assigning to variables while abstracting away the differences between TensorFlow 1.x and 2.x semantics. {snippet} Args: x: Variable to set to a new value. value: Value to set the tensor to, as a Numpy array (of the same shape).",torch.amp.autocast_mode.is_autocast_available,"Return a bool indicating if autocast is available on :attr:`device_type`. Args: device_type(str): Device type to use. Possible values are: 'cuda', 'cpu', 'xpu' and so on. The type is the same as the `type` attribute of a :class:`torch.device`. Thus, you may obtain the device type of a tensor using `Tensor.device.type`.",0.9622574
tensorflow.python.data.kernel_tests.checkpoint_test_base.verify_error_on_save,Attempts to save a non-saveable iterator. Args: ds_fn: 0-argument function that returns a Dataset. num_outputs: Total number of outputs expected from this Dataset. error: Declared error when trying to save iterator. break_point: Break point. Optional. Defaults to num_outputs/2. sparse_tensors: Whether dataset is built from SparseTensor(s). assert_items_equal: Tests the output has the expected elements regardless of order. Raises: AssertionError if any test fails.,torch.testing._internal.dist_utils.wait_until_node_failure,"Loops until an RPC to the given rank fails. This is used to indicate that the node has failed in unit tests. Args: rank (int): Rank of the node expected to fail expected_error_regex (optional, str): Regex of exception message expected. Useful to ensure a specific failure occurs, not just any.",0.96223515
tensorflow.python.training.training_util.get_or_create_global_step,"Returns and create (if necessary) the global step tensor. Args: graph: The graph in which to create the global step tensor. If missing, use default graph. Returns: The global step tensor. @compatibility(TF2) With the deprecation of global graphs, TF no longer tracks variables in collections. In other words, there are no global variables in TF2. Thus, the global step functions have been removed (`get_or_create_global_step`, `create_global_step`, `get_global_step`) . You have two options for migrating: 1. Create a Keras optimizer, which generates an `iterations` variable. This variable is automatically incremented when calling `apply_gradients`. 2. Manually create and increment a `tf.Variable`. Below is an example of migrating away from using a global step to using a Keras optimizer: Define a dummy model and loss: >>> def compute_loss(x): ... v = tf.Variable(3.0) ... y = x * v ... loss = x * 5 - x * v ... return loss, [v] Before migrating: >>> g = tf.Graph() >>> with g.as_default(): ... x = tf.compat.v1.placeholder(tf.float32, []) ... loss, var_list = compute_loss(x) ... global_step = tf.compat.v1.train.get_or_create_global_step() ... global_init = tf.compat.v1.global_variables_initializer() ... optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1) ... train_op = optimizer.minimize(loss, global_step, var_list) >>> sess = tf.compat.v1.Session(graph=g) >>> sess.run(global_init) >>> print(""before training:"", sess.run(global_step)) before training: 0 >>> sess.run(train_op, feed_dict={x: 3}) >>> print(""after training:"", sess.run(global_step)) after training: 1 Migrating to a Keras optimizer: >>> optimizer = tf.keras.optimizers.SGD(.01) >>> print(""before training:"", optimizer.iterations.numpy()) before training: 0 >>> with tf.GradientTape() as tape: ... loss, var_list = compute_loss(3) ... grads = tape.gradient(loss, var_list) ... optimizer.apply_gradients(zip(grads, var_list)) >>> print(""after training:"", optimizer.iterations.numpy()) after training: 1 @end_compatib",torch.onnx._internal.exporter.model_signature,"The model signature for the exported ONNX graph. This information is relevant because ONNX specification often differs from PyTorch's, resulting in a ONNX graph with input and output schema different from the actual PyTorch model implementation. By using the model signature, the users can understand the inputs and outputs differences and properly execute the model in ONNX Runtime. NOTE: Model signature is only available when the ONNX graph was exported from a :class:`torch.export.ExportedProgram` object. NOTE: Any transformation done to the model that changes the model signature must be accompanied by updates to this model signature as well through :class:`InputAdaptStep` and/or :class:`OutputAdaptStep`. Example: The following model produces different sets of inputs and outputs. The first 4 inputs are model parameters (namely conv1.weight, conv2.weight, fc1.weight, fc2.weight), and the next 2 inputs are registered buffers (namely my_buffer2, my_buffer1) and finally the last 2 inputs are user inputs (namely x and b). The first output is a buffer mutation (namely my_buffer2) and the last output is the actual model output. >>> import pprint >>> class CustomModule(torch.nn.Module): ... def __init__(self): ... super().__init__() ... self.my_parameter = torch.nn.Parameter(torch.tensor(2.0)) ... self.register_buffer(""my_buffer1"", torch.tensor(3.0)) ... self.register_buffer(""my_buffer2"", torch.tensor(4.0)) ... self.conv1 = torch.nn.Conv2d(1, 32, 3, 1, bias=False) ... self.conv2 = torch.nn.Conv2d(32, 64, 3, 1, bias=False) ... self.fc1 = torch.nn.Linear(9216, 128, bias=False) ... self.fc2 = torch.nn.Linear(128, 10, bias=False) ... def forward(self, x, b): ... tensor_x = self.conv1(x) ... tensor_x = torch.nn.functional.sigmoid(tensor_x) ... tensor_x = self.conv2(tensor_x) ... tensor_x = torch.nn.functional.sigmoid(tensor_x) ... tensor_x = torch.nn.functional.max_pool2d(tensor_x, 2) ... tensor_x = torch.flatten(tensor_x, 1) ... tensor_x = self.fc1(tensor_x) ... tensor_x = torch",0.9622331
tensorflow.python.feature_column.feature_column_v2.get_variable,Returns an existing variable. Args: feature_column: A `FeatureColumn` object this variable corresponds to. name: variable name.,torch.distributed.elastic.utils.api.get_env_variable_or_raise,Tries to retrieve environment variable. Raises ``ValueError`` if no environment variable found. Args: env_name (str): Name of the env variable,0.96218115
tensorflow.python.ops.resource_variable_ops.scatter_div,"Divide this variable by `tf.IndexedSlices`. Args: sparse_delta: `tf.IndexedSlices` to divide this variable by. use_locking: If `True`, use locking during the operation. name: the name of the operation. Returns: The updated variable. Raises: TypeError: if `sparse_delta` is not an `IndexedSlices`.",torch.distributed.distributed_c10d.get_group_rank,Translate a global rank into a group rank. ``global_rank`` must be part of ``group`` otherwise this raises RuntimeError. Args: group (ProcessGroup): ProcessGroup to find the relative rank. global_rank (int): Global rank to query. Returns: Group rank of ``global_rank`` relative to ``group`` N.B. calling this function on the default process group returns identity,0.9621702
tensorflow.python.tpu.tpu_name_util.core,"Returns the device name for a core in a replicated TPU computation. Args: num: the virtual core number within each replica to which operators should be assigned. Returns: A device name, suitable for passing to `tf.device()`.",torch.storage.is_pinned,Determine whether the CPU storage is already pinned on device. Args: device (str or torch.device): The device to pin memory on. Default: ``'cuda'``. Returns: A boolean variable.,0.9621647
tensorflow.python.keras.engine.training.compile,"Configures the model for training. Args: optimizer: String (name of optimizer) or optimizer instance. See `tf.keras.optimizers`. loss: String (name of objective function), objective function or `tf.keras.losses.Loss` instance. See `tf.keras.losses`. An objective function is any callable with the signature `loss = fn(y_true, y_pred)`, where y_true = ground truth values with shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]`. y_pred = predicted values with shape = `[batch_size, d0, .. dN]`. It returns a weighted loss float tensor. If a custom `Loss` instance is used and reduction is set to `None`, return value has the shape `[batch_size, d0, .. dN-1]` i.e. per-sample or per-timestep loss values; otherwise, it is a scalar. If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses, unless `loss_weights` is specified. metrics: List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a `tf.keras.metrics.Metric` instance. See `tf.keras.metrics`. Typically you will use `metrics=['accuracy']`. A function is any callable with the signature `result = fn(y_true, y_pred)`. To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as `metrics={'output_a': 'accuracy', 'output_b': ['accuracy', 'mse']}`. You can also pass a list to specify a metric or a list of metrics for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the strings 'accuracy' or 'acc', we convert this to one of `tf.keras.metrics.BinaryAccuracy`, `tf.keras.metrics.CategoricalAccuracy`, `tf.keras.metrics.SparseCategoricalAccuracy` based on th",torch.optim.optimizer.state_dict,"Returns the state of the optimizer as a :class:`dict`. It contains two entries: * ``state``: a Dict holding current optimization state. Its content differs between optimizer classes, but some common characteristics hold. For example, state is saved per parameter, and the parameter itself is NOT saved. ``state`` is a Dictionary mapping parameter ids to a Dict with state corresponding to each parameter. * ``param_groups``: a List containing all parameter groups where each parameter group is a Dict. Each parameter group contains metadata specific to the optimizer, such as learning rate and weight decay, as well as a List of parameter IDs of the parameters in the group. NOTE: The parameter IDs may look like indices but they are just IDs associating state with param_group. When loading from a state_dict, the optimizer will zip the param_group ``params`` (int IDs) and the optimizer ``param_groups`` (actual ``nn.Parameter`` s) in order to match state WITHOUT additional verification. A returned state dict might look something like: .. code-block:: text { 'state': { 0: {'momentum_buffer': tensor(...), ...}, 1: {'momentum_buffer': tensor(...), ...}, 2: {'momentum_buffer': tensor(...), ...}, 3: {'momentum_buffer': tensor(...), ...} }, 'param_groups': [ { 'lr': 0.01, 'weight_decay': 0, ... 'params': [0] }, { 'lr': 0.001, 'weight_decay': 0.5, ... 'params': [1, 2, 3] } ] }",0.9620854
tensorflow.python.tpu.tpu_embedding_v2.apply_gradients,"Applies the gradient update to the embedding tables. If a gradient of `None` is passed in any position of the nested structure, then an gradient update with a zero gradient is applied for that feature. For optimizers like SGD or Adagrad, this is the same as applying no update at all. For lazy Adam and other sparsely applied optimizers with decay, ensure you understand the effect of applying a zero gradient. python strategy = tf.distribute.TPUStrategy(...) with strategy.scope(): embedding = tf.tpu.experimental.embedding.TPUEmbedding(...) distributed_dataset = ( strategy.distribute_datasets_from_function( dataset_fn=..., options=tf.distribute.InputOptions( experimental_fetch_to_device=False)) dataset_iterator = iter(distributed_dataset) @tf.function def training_step(): def tpu_step(tpu_features): with tf.GradientTape() as tape: activations = embedding.dequeue() tape.watch(activations) loss = ... # some computation involving activations embedding_gradients = tape.gradient(loss, activations) embedding.apply_gradients(embedding_gradients) embedding_features, tpu_features = next(dataset_iterator) embedding.enqueue(embedding_features, training=True) strategy.run(tpu_step, args=(tpu_features, )) training_step() Args: gradients: A nested structure of gradients, with structure matching the `feature_config` passed to this object. name: A name for the underlying op. Raises: RuntimeError: If called when object wasn't created under a `TPUStrategy` or if not built (either by manually calling build or calling enqueue). ValueError: If a non-`tf.Tensor` non-`None` gradient is passed in, or a `tf.Tensor` of the incorrect shape is passed in. Also if the size of any sequence in `gradients` does not match corresponding sequence in `feature_config`. TypeError: If the type of any sequence in `gradients` does not match corresponding sequence in `feature_config`.",torch._higher_order_ops.while_loop.while_loop,"Run body_fn(*carried_inputs) while cond_fn(*carried_inputs) returns a True scalar tensor. Returns the output of body_fn or initial carried_inputs. .. warning:: `torch.while_loop` is a prototype feature in PyTorch. It has limited support for input and output types and doesn't support training currently. Please look forward to a more stable implementation in a future version of PyTorch. Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype `while_loop` is a structured control flow operator. It preserves the loop semantic across the torch.compile and torch.export. `while_loop` is equivalent to the following: def while_loop(cond_fn, body_fn, carried_inputs): val = carried_inputs while cond_fn(*val): val = body_fn(*val) return val Args: cond_fn (Callable): A callable function that returns a boolean Scalar tensor. body_fn (Callable): A callable function that takes the same inputs as `cond_fn` and returns a tuple of tensors carried_inputs (Tuple of possibly nested dict/list/tuple of tensors): A tuple of inputs to cond_fn and body_fn. It's also the initial value of states that are carried across iterations. Example: def cond_fn(iter, x): return iter.sum() < 10 def body_fn(iter, x): return iter + 1, x.sin() while_loop(cond_fn, body_fn, (torch.zeros(1), torch.randn(3, 4))) Restrictions: - body_fn must return tensors with the same metadata (e.g.shape, dtype) as inputs. - body_fn and cond_fn must not in-place mutate the carried_inputs. A clone before the mutation is required. - body_fn and cond_fn must not mutate python varialbles (e.g. list/dict) created outside of the body_fn. - body_fn and cond_fn's output cannot aliase any of the inputs. A clone is required. .. warning:: Temporal Limitations: - 'while_loop' only supports **inference** right now. Autograd will be supported in the future.",0.9620824
tensorflow.python.eager.polymorphic_function.atomic_function.from_func_graph,"Initializes an AtomicFunction from FuncGraph. Args: name: str, the name for the created function. graph: Graph, the graph containing the operations in the function attrs: dict mapping names of attributes to their AttrValue values function_type: known FunctionType to use, otherwise one is derived. overwrite: overwrites function definition in the current context if needed Returns: An AtomicFunction instance.",torch.jit.frontend.get_default_args_for_class,"Get default arguments for all methods in a class (except for static methods). Args: cls: type - The class type to inspect for default arguments. Returns: A Dict[str, Dict[str, Any]] which maps each method name to a Dict[str, Any] that maps each argument name to its default value.",0.962081
tensorflow.python.tpu.tensor_tracer_flags.is_flag_on,Returns True if the given flag is on.,torch.distributed.pipelining.stage.has_backward,Returns true if this stage has a backward pass.,0.96207845
tensorflow.python.keras.utils.generic_utils.get_registered_name,"Returns the name registered to an object within the Keras framework. This function is part of the Keras serialization and deserialization framework. It maps objects to the string names associated with those objects for serialization/deserialization. Args: obj: The object to look up. Returns: The name associated with the object, or the default Python name if the object is not registered.",torch._inductor.codegen.cuda.gemm_template.gen_ops,Creates a list of Cutlass GemmOperation instances that match the operation this template is designed to represent. The matching is carried out with respect to the input and output specifications of the operation. No function arguments. Returns: List[cutlass_gemm_op.GemmOperation]: A list of GemmOperation instances that are compatible with the operation requirements of this template.,0.9619709
tensorflow.python.data.experimental.ops.resampling.rejection_resample,"A transformation that resamples a dataset to achieve a target distribution. **NOTE** Resampling is performed via rejection sampling; some fraction of the input values will be dropped. Args: class_func: A function mapping an element of the input dataset to a scalar `tf.int32` tensor. Values should be in `[0, num_classes)`. target_dist: A floating point type tensor, shaped `[num_classes]`. initial_dist: (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion. seed: (Optional.) Python integer seed for the resampler. Returns: A `Dataset` transformation function, which can be passed to `tf.data.Dataset.apply`.",torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook,"Compress by casting ``GradBucket`` to ``torch.float16`` divided by process group size. This DDP communication hook implements a simple gradient compression approach that casts ``GradBucket`` tensor to half-precision floating-point format (``torch.float16``) and then divides it by the process group size. It allreduces those ``float16`` gradient tensors. Once compressed gradient tensors are allreduced, the chained callback ``decompress`` casts it back to the input data type (such as ``float32``). Example:: >>> # xdoctest: +SKIP >>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)",0.9619387
tensorflow.python.framework.func_graph.control_dependencies,Handles control dependencies. FuncGraph wraps Graph's control_dependencies logic by first filtering out any external tensors / operations and storing them in the graph's control_captures member. Any consumers of this function graph must then decide how to handle the control captures. Args: control_inputs: A list of `Operation` or `Tensor` objects which must be executed or computed before running the operations defined in the context. Can also be `None` to clear the control dependencies. Returns: A context manager that specifies control dependencies for all operations constructed within the context. Raises: TypeError: If `control_inputs` is not a list of `Operation` or `Tensor` objects.,torch.fx.interpreter.run,"Run `module` via interpretation and return the result. Args: *args: The arguments to the Module to run, in positional order initial_env (Optional[Dict[Node, Any]]): An optional starting environment for execution. This is a dict mapping `Node` to any value. This can be used, for example, to pre-populate results for certain `Nodes` so as to do only partial evaluation within the interpreter. enable_io_processing (bool): If true, we process the inputs and outputs with graph's process_inputs and process_outputs function first before using them. Returns: Any: The value returned from executing the Module",0.9619105
tensorflow.python.distribute.cluster_resolver.tfconfig_cluster_resolver.master,"Returns the master address to use when creating a TensorFlow session. Note: this is only useful for TensorFlow 1.x. Args: task_type: (String, optional) Overrides and sets the task_type of the master. task_id: (Integer, optional) Overrides and sets the task id of the master. rpc_layer: (String, optional) Overrides and sets the protocol over which TensorFlow nodes communicate with each other. Returns: The address of the master. Raises: RuntimeError: If the task_type or task_id is not specified and the `TF_CONFIG` environment variable does not contain a task section.",torch.distributed.distributed_c10d.isend,"Send a tensor asynchronously. .. warning:: Modifying ``tensor`` before the request completes causes undefined behavior. .. warning:: ``tag`` is not supported with the NCCL backend. Args: tensor (Tensor): Tensor to send. dst (int): Destination rank on global process group (regardless of ``group`` argument) group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. tag (int, optional): Tag to match send with remote recv Returns: A distributed request object. None, if not part of the group",0.96191025
tensorflow.python.ops.rnn.static_state_saving_rnn,"RNN that accepts a state saver for time-truncated RNN calculation. Args: cell: An instance of `RNNCell`. inputs: A length T list of inputs, each a `Tensor` of shape `[batch_size, input_size]`. state_saver: A state saver object with methods `state` and `save_state`. state_name: Python string or tuple of strings. The name to use with the state_saver. If the cell returns tuples of states (i.e., `cell.state_size` is a tuple) then `state_name` should be a tuple of strings having the same length as `cell.state_size`. Otherwise it should be a single string. sequence_length: (optional) An int32/int64 vector size [batch_size]. See the documentation for rnn() for more details about sequence_length. scope: VariableScope for the created subgraph; defaults to ""rnn"". Returns: A pair (outputs, state) where: outputs is a length T list of outputs (one for each input) states is the final state Raises: TypeError: If `cell` is not an instance of RNNCell. ValueError: If `inputs` is `None` or an empty list, or if the arity and type of `state_name` does not match that of `cell.state_size`.",torch.nn.modules.module.register_buffer,"Add a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's ``running_mean`` is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:`persistent` to ``False``. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:`state_dict`. Buffers can be accessed as attributes using given names. Args: name (str): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If ``None``, then operations that run on buffers, such as :attr:`cuda`, are ignored. If ``None``, the buffer is **not** included in the module's :attr:`state_dict`. persistent (bool): whether the buffer is part of this module's :attr:`state_dict`. Example:: >>> # xdoctest: +SKIP(""undefined vars"") >>> self.register_buffer('running_mean', torch.zeros(num_features))",0.96189606
tensorflow.python.ops.math_ops.multiply,"Returns an element-wise x * y. For example: >>> x = tf.constant(([1, 2, 3, 4])) >>> tf.math.multiply(x, x) <tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1, 4, 9, 16], dtype=int32)> Since `tf.math.multiply` will convert its arguments to `Tensor`s, you can also pass in non-`Tensor` arguments: >>> tf.math.multiply(7,6) <tf.Tensor: shape=(), dtype=int32, numpy=42> If `x.shape` is not the same as `y.shape`, they will be broadcast to a compatible shape. (More about broadcasting [here](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).) For example: >>> x = tf.ones([1, 2]); >>> y = tf.ones([2, 1]); >>> x * y # Taking advantage of operator overriding <tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1., 1.], [1., 1.]], dtype=float32)> The reduction version of this elementwise operation is `tf.math.reduce_prod` Args: x: A Tensor. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`. y: A `Tensor`. Must have the same type as `x`. name: A name for the operation (optional). Returns: A `Tensor`. Has the same type as `x`. Raises: * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.",torch.utils.data._utils.collate.default_convert,"Convert each NumPy array element into a :class:`torch.Tensor`. If the input is a `Sequence`, `Collection`, or `Mapping`, it tries to convert each element inside to a :class:`torch.Tensor`. If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both `batch_sampler` and `batch_size` are NOT defined in :class:`~torch.utils.data.DataLoader`. The general input type to output type mapping is similar to that of :func:`~torch.utils.data.default_collate`. See the description there for more details. Args: data: a single data point to be converted Examples: >>> # xdoctest: +SKIP >>> # Example with `int` >>> default_convert(0) 0 >>> # Example with NumPy array >>> default_convert(np.array([0, 1])) tensor([0, 1]) >>> # Example with NamedTuple >>> Point = namedtuple('Point', ['x', 'y']) >>> default_convert(Point(0, 0)) Point(x=0, y=0) >>> default_convert(Point(np.array(0), np.array(0))) Point(x=tensor(0), y=tensor(0)) >>> # Example with List >>> default_convert([np.array([0, 1]), np.array([2, 3])]) [tensor([0, 1]), tensor([2, 3])]",0.9617997
tensorflow.python.training.optimizer.target,Returns the optimization target for this variable.,torch.onnx._internal.diagnostics.infra.utils.function_location,Returns a Location for the given function.,0.9617757
tensorflow.python.keras.utils.generic_utils.class_and_config_for_serialized_keras_object,Returns the class name and config for a serialized keras object.,torch._dynamo.utils.fqn,Returns the fully qualified name of the object.,0.96173733
tensorflow.python.keras.saving.saved_model.save.save,"Saves a model as a SavedModel to the filepath. Args: model: Keras model instance to be saved. filepath: String path to save the model. overwrite: whether to overwrite the existing filepath. include_optimizer: If True, save the model's optimizer state. signatures: Signatures to save with the SavedModel. Applicable to the 'tf' format only. Please see the `signatures` argument in `tf.saved_model.save` for details. options: (only applies to SavedModel format) `tf.saved_model.SaveOptions` object that specifies options for saving to SavedModel. save_traces: (only applies to SavedModel format) When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to `True`. Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a `get_config()` method. Raises: ValueError: if the model's inputs have not been defined.",torch.onnx._internal.exporter.save,"Saves the in-memory ONNX model to ``destination`` using specified ``serializer``. Args: destination: The destination to save the ONNX model. It can be either a string or a file-like object. When used with ``model_state``, it must be a string with a full path to the destination. If `destination` is a string, besides saving the ONNX model into a file, model weights are also stored in separate files in the same directory as the ONNX model. E.g. for `destination=""/path/model.onnx""`, the initializers are saved in ""/path/"" folder along with ""onnx.model"". include_initializers: Whether to include initializers in the ONNX graph as external data. Cannot be combined with `model_state_dict`. model_state: The state_dict of the PyTorch model containing all weights on it. It can be either a string with the path to a checkpoint or a dictionary with the actual model state. The supported file formats are the same as those supported by `torch.load` and `safetensors.safe_open`. Required when :func:`enable_fake_mode` is used but real initializers are needed on the ONNX graph. serializer: The serializer to use. If not specified, the model will be serialized as Protobuf.",0.9617119
tensorflow.python.summary.summary.histogram,"Outputs a `Summary` protocol buffer with a histogram. Adding a histogram summary makes it possible to visualize your data's distribution in TensorBoard. You can see a detailed explanation of the TensorBoard histogram dashboard [here](https://www.tensorflow.org/get_started/tensorboard_histograms). The generated [`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto) has one summary value containing a histogram for `values`. This op reports an `InvalidArgument` error if any value is not finite. Args: name: A name for the generated node. Will also serve as a series name in TensorBoard. values: A real numeric `Tensor`. Any shape. Values to use to build the histogram. collections: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to `[GraphKeys.SUMMARIES]`. family: Optional; if provided, used as the prefix of the summary tag name, which controls the tab name used for display on Tensorboard. Returns: A scalar `Tensor` of type `string`. The serialized `Summary` protocol buffer. @compatibility(TF2) For compatibility purposes, when invoked in TF2 where the outermost context is eager mode, this API will check if there is a suitable TF2 summary writer context available, and if so will forward this call to that writer instead. A ""suitable"" writer context means that the writer is set as the default writer, and there is an associated non-empty value for `step` (see `tf.summary.SummaryWriter.as_default`, `tf.summary.experimental.set_step` or alternatively `tf.compat.v1.train.create_global_step`). For the forwarded call, the arguments here will be passed to the TF2 implementation of `tf.summary.histogram`, and the return value will be an empty bytestring tensor, to avoid duplicate summary writing. This forwarding is best-effort and not all arguments will be preserved. To migrate to TF2, please use `tf.summary.histogram` instead. Please check [Migrating tf.summary usage to TF 2.0](https://www.tensorflow.o",torch._library.custom_ops.custom_op,"Wraps a function into custom operator. Reasons why you may want to create a custom op include: - Wrapping a third-party library or custom kernel to work with PyTorch subsystems like Autograd. - Preventing torch.compile/export/FX tracing from peeking inside your function. This API is used as a decorator around a function (please see examples). The provided function must have type hints; these are needed to interface with PyTorch's various subsystems. Args: name (str): A name for the custom op that looks like ""{namespace}::{name}"", e.g. ""mylib::my_linear"". The name is used as the op's stable identifier in PyTorch subsystems (e.g. torch.export, FX graphs). To avoid name collisions, please use your project name as the namespace; e.g. all custom ops in pytorch/fbgemm use ""fbgemm"" as the namespace. mutates_args (Iterable[str]): The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. device_types (None | str | Sequence[str]): The device type(s) the function is valid for. If no device type is provided, then the function is used as the default implementation for all device types. Examples: ""cpu"", ""cuda"". schema (None | str): A schema string for the operator. If None (recommended) we'll infer a schema for the operator from its type annotations. We recommend letting us infer a schema unless you have a specific reason not to. Example: ""(Tensor x, int y) -> (Tensor, Tensor)"". .. note:: We recommend not passing in a ``schema`` arg and instead letting us infer it from the type annotations. It is error-prone to write your own schema. You may wish to provide your own schema if our interpretation of the type annotation is not what you want. For more info on how to write a schema string, see `here <https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/native/README.md#func>`_ Examples:: >>> import torch >>> from torch import Tensor >>> from torch.library import custom_op >>> import numpy as np >>> >>> @custom_op(""mylib::numpy_sin""",0.961689
tensorflow.python.util.all_util.reveal_undocumented,"Reveals a symbol that was previously removed by `remove_undocumented`. This should be used by tensorflow internal tests only. It explicitly defeats the encapsulation afforded by `remove_undocumented`. It throws an exception when the symbol was not hidden in the first place. Args: symbol_name: a string representing the full absolute path of the symbol. target_module: if specified, the module in which to restore the symbol.",torch.utils.cpp_extension.get_compiler_abi_compatibility_and_version,"Determine if the given compiler is ABI-compatible with PyTorch alongside its version. Args: compiler (str): The compiler executable name to check (e.g. ``g++``). Must be executable in a shell process. Returns: A tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch, followed by a `TorchVersion` string that contains the compiler version separated by dots.",0.96157074
tensorflow.python.eager.context.config,Return the ConfigProto with all runtime deltas applied.,torch.cuda.tunable.get_validators,Return the TunableOp validators.,0.96153986
tensorflow.python.ops.control_flow_ops.AddForwardLoopCounter,Adds a loop that counts the number of iterations. This is added to the forward loop at the time when we start to create the loop for backprop gradient computation. Called in the outer context of this forward context. The pseudocode is: `n = 0; while (_pivot) { n++; }` Note that a control dependency is added to `n` to ensure the correct execution order of stack push ops. Args: outer_grad_state: The outer grad state. None if not nested. Returns: The number of iterations taken by the forward loop and the loop index.,torch.distributed.optim.zero_redundancy_optimizer.consolidate_state_dict,"Consolidate a list of ``state_dict`` s (one per rank) on the target rank. Arguments: to (int): the rank that receives the optimizer states (default: 0). Raises: RuntimeError: if ``overlap_with_ddp=True`` and this method is called before this :class:`ZeroRedundancyOptimizer` instance has been fully initialized, which happens once :class:`DistributedDataParallel` gradient buckets have been rebuilt. .. warning:: This needs to be called on all ranks.",0.961498
tensorflow.python.keras.backend.function,"Instantiates a Keras function. Args: inputs: List of placeholder tensors. outputs: List of output tensors. updates: List of update ops. name: String, name of function. **kwargs: Passed to `tf.Session.run`. Returns: Output values as Numpy arrays. Raises: ValueError: if invalid kwargs are passed in or if in eager execution.",torch.distributed.distributed_c10d.is_backend_available,Check backend availability. Checks if the given backend is available and supports the built-in backends or third-party backends through function ``Backend.register_backend``. Args: backend (str): Backend name. Returns: bool: Returns true if the backend is available otherwise false.,0.96148115
tensorflow.python.ops.signal.dct_ops.idct,"Computes the 1D [Inverse Discrete Cosine Transform (DCT)][idct] of `input`. Currently Types I, II, III, IV are supported. Type III is the inverse of Type II, and vice versa. Note that you must re-normalize by 1/(2n) to obtain an inverse if `norm` is not `'ortho'`. That is: `signal == idct(dct(signal)) * 0.5 / signal.shape[-1]`. When `norm='ortho'`, we have: `signal == idct(dct(signal, norm='ortho'), norm='ortho')`. @compatibility(scipy) Equivalent to [scipy.fftpack.idct] (https://docs.scipy.org/doc/scipy-1.4.0/reference/generated/scipy.fftpack.idct.html) for Type-I, Type-II, Type-III and Type-IV DCT. @end_compatibility Args: input: A `[..., samples]` `float32`/`float64` `Tensor` containing the signals to take the DCT of. type: The IDCT type to perform. Must be 1, 2, 3 or 4. n: For future expansion. The length of the transform. Must be `None`. axis: For future expansion. The axis to compute the DCT along. Must be `-1`. norm: The normalization to apply. `None` for no normalization or `'ortho'` for orthonormal normalization. name: An optional name for the operation. Returns: A `[..., samples]` `float32`/`float64` `Tensor` containing the IDCT of `input`. Raises: ValueError: If `type` is not `1`, `2` or `3`, `n` is not `None, `axis` is not `-1`, or `norm` is not `None` or `'ortho'`. [idct]: https://en.wikipedia.org/wiki/Discrete_cosine_transform#Inverse_transforms",torch.nn.utils.weight_norm.weight_norm,"Apply weight normalization to a parameter in the given module. .. math:: \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|} Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by :attr:`name` (e.g. ``'weight'``) with two parameters: one specifying the magnitude (e.g. ``'weight_g'``) and one specifying the direction (e.g. ``'weight_v'``). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every :meth:`~Module.forward` call. By default, with ``dim=0``, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use ``dim=None``. See https://arxiv.org/abs/1602.07868 .. warning:: This function is deprecated. Use :func:`torch.nn.utils.parametrizations.weight_norm` which uses the modern parametrization API. The new ``weight_norm`` is compatible with ``state_dict`` generated from old ``weight_norm``. Migration guide: * The magnitude (``weight_g``) and direction (``weight_v``) are now expressed as ``parametrizations.weight.original0`` and ``parametrizations.weight.original1`` respectively. If this is bothering you, please comment on https://github.com/pytorch/pytorch/issues/102999 * To remove the weight normalization reparametrization, use :func:`torch.nn.utils.parametrize.remove_parametrizations`. * The weight is no longer recomputed once at module forward; instead, it will be recomputed on every access. To restore the old behavior, use :func:`torch.nn.utils.parametrize.cached` before invoking the module in question. Args: module (Module): containing module name (str, optional): name of weight parameter dim (int, optional): dimension over which to compute the norm Returns: The original module with the weight norm hook Example:: >>> m = weight_norm(nn.Linear(20, 40), name='weight') >>> m Linear(in_features=20, out_features=40, bias=True) >>> m.weight_g.size() ",0.9613948
tensorflow.python.util.all_util.remove_undocumented,"Removes symbols in a module that are not referenced by a docstring. Args: module_name: the name of the module (usually `__name__`). allowed_exception_list: a list of names that should not be removed. doc_string_modules: a list of modules from which to take the docstrings. If None, then a list containing only the module named `module_name` is used. Furthermore, if a symbol previously added with `add_to_global_allowlist`, then it will always be allowed. This is useful for internal tests. Returns: None",torch.distributed.fsdp.fully_sharded_data_parallel.fsdp_modules,"Return all nested FSDP instances. This possibly includes ``module`` itself and only includes FSDP root modules if ``root_only=True``. Args: module (torch.nn.Module): Root module, which may or may not be an ``FSDP`` module. root_only (bool): Whether to return only FSDP root modules. (Default: ``False``) Returns: List[FullyShardedDataParallel]: FSDP modules that are nested in the input ``module``.",0.9613943
tensorflow.python.framework.ops.gradient_override_map,"EXPERIMENTAL: A context manager for overriding gradient functions. This context manager can be used to override the gradient function that will be used for ops within the scope of the context. For example: python @tf.RegisterGradient(""CustomSquare"") def _custom_square_grad(op, grad): # ... with tf.Graph().as_default() as g: c = tf.constant(5.0) s_1 = tf.square(c) # Uses the default gradient for tf.square. with g.gradient_override_map({""Square"": ""CustomSquare""}): s_2 = tf.square(s_2) # Uses _custom_square_grad to compute the # gradient of s_2. Args: op_type_map: A dictionary mapping op type strings to alternative op type strings. Returns: A context manager that sets the alternative op type to be used for one or more ops created in that context. Raises: TypeError: If `op_type_map` is not a dictionary mapping strings to strings.",torch.onnx._internal.registration.onnx_symbolic,"Registers a symbolic function. Usage:: @onnx_symbolic(""aten::symbolic_b"", opset=10, decorate=[quantized_aten_handler(scale=1/128, zero_point=0)]) @symbolic_helper.parse_args(""v"", ""v"", ""b"") def symbolic_b(g: _C.Graph, x: _C.Value, y: _C.Value, arg1: bool) -> _C.Value: ... Args: name: The qualified name of the function in the form of 'domain::op'. E.g. 'aten::add'. opset: The opset versions of the function to register at. decorate: A sequence of decorators to apply to the function. custom: Whether the function is a custom symbolic function. Raises: ValueError: If the separator '::' is not in the name.",0.9613428
tensorflow.python.framework.stack.get_default_session,"Returns the default session for the current thread. The returned `Session` will be the innermost session on which a `Session` or `Session.as_default()` context has been entered. NOTE: The default session is a property of the current thread. If you create a new thread, and wish to use the default session in that thread, you must explicitly add a `with sess.as_default():` in that thread's function. Returns: The default `Session` being used in the current thread.",torch.onnx._internal.onnxruntime.get_cached_instance_for_options,"Returns a possibly cached instance of an ``OrtBackend``. If an existing backend was created previously through this function with the same options, it will be returned. Otherwise a new backend will be created, cached, and returned. Note: if ``options`` sets ``ort_session_options``, a new ``OrtBackend`` will always be returned, since ``onnxruntime.SessionOptions`` cannot participate in caching.",0.9613287
tensorflow.python.tools.optimize_for_inference_lib.optimize_for_inference,"Applies a series of inference optimizations on the input graph. Args: input_graph_def: A GraphDef containing a training model. input_node_names: A list of names of the nodes that are fed inputs during inference. output_node_names: A list of names of the nodes that produce the final results. placeholder_type_enum: The AttrValue enum for the placeholder data type, or a list that specifies one value per input node name. toco_compatible: Boolean, if True, only runs optimizations that result in TOCO compatible graph operations (default=False). placeholder_to_const_names: A list of names of the PlaceholderWithDefault nodes to be converted to Constant. Returns: An optimized version of the input graph.",torch._export.passes.lift_constants_pass.lift_constants_pass,"Takes a graph module, graph signature, and modifies them implace to lift any constants (tensors or custom classes) as inputs to the graph. Returns a dictionary of names to constants. Arguments: gm (torch.fx.GraphModule): The graph module containing the graph and constants to lift. graph_signature (ExportGraphSignature): This graph signature will be mutated to add additional CONSTANT_TENSOR and CUSTOM_OBJ inputs. constant_attrs (ConstantAttr): A mapping from a constant value to its fully-qualified path in `gm`. This is used to maintain consistent location of constants between the original module and the exported version. Returns: A dictionary of fqn => constant value.",0.9613162
tensorflow.python.data.util.convert.partial_shape_to_tensor,"Returns a `tf.Tensor` that represents the given shape. Args: shape_like: A value that can be converted to a `tf.TensorShape` or a `tf.Tensor`. Returns: A 1-D `tf.Tensor` of `tf.int64` elements representing the given shape, where `-1` is substituted for any unknown dimensions.",torch.nn.utils.parametrize.right_inverse,"Call the ``right_inverse`` methods of the parametrizations in the inverse registration order. Then, it stores the result in ``self.original`` if ``right_inverse`` outputs one tensor or in ``self.original0``, ``self.original1``, ... if it outputs several. Args: value (Tensor): Value to which initialize the module",0.9613043
tensorflow.python.ops.image_ops_impl.sobel_edges,"Returns a tensor holding Sobel edge maps. Example usage: For general usage, `image` would be loaded from a file as below: python image_bytes = tf.io.read_file(path_to_image_file) image = tf.image.decode_image(image_bytes) image = tf.cast(image, tf.float32) image = tf.expand_dims(image, 0) But for demo purposes, we are using randomly generated values for `image`: >>> image = tf.random.uniform( ... maxval=255, shape=[1, 28, 28, 3], dtype=tf.float32) >>> sobel = tf.image.sobel_edges(image) >>> sobel_y = np.asarray(sobel[0, :, :, :, 0]) # sobel in y-direction >>> sobel_x = np.asarray(sobel[0, :, :, :, 1]) # sobel in x-direction For displaying the sobel results, PIL's [Image Module]( https://pillow.readthedocs.io/en/stable/reference/Image.html) can be used: python # Display edge maps for the first channel (at index 0) Image.fromarray(sobel_y[..., 0] / 4 + 0.5).show() Image.fromarray(sobel_x[..., 0] / 4 + 0.5).show() Args: image: Image tensor with shape [batch_size, h, w, d] and type float32 or float64. The image(s) must be 2x2 or larger. Returns: Tensor holding edge maps for each channel. Returns a tensor with shape [batch_size, h, w, d, 2] where the last two dimensions hold [[dy[0], dx[0]], [dy[1], dx[1]], ..., [dy[d-1], dx[d-1]]] calculated using the Sobel filter.",torch.utils.tensorboard.writer.add_mesh,"Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage. Args: tag (str): Data identifier vertices (torch.Tensor): List of the 3D coordinates of vertices. colors (torch.Tensor): Colors for each vertex faces (torch.Tensor): Indices of vertices within each triangle. (Optional) config_dict: Dictionary with ThreeJS classes names and configuration. global_step (int): Global step value to record walltime (float): Optional override default walltime (time.time()) seconds after epoch of event Shape: vertices: :math:`(B, N, 3)`. (batch, number_of_vertices, channels) colors: :math:`(B, N, 3)`. The values should lie in [0, 255] for type `uint8` or [0, 1] for type `float`. faces: :math:`(B, N, 3)`. The values should lie in [0, number_of_vertices] for type `uint8`. Examples:: from torch.utils.tensorboard import SummaryWriter vertices_tensor = torch.as_tensor([ [1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1], ], dtype=torch.float).unsqueeze(0) colors_tensor = torch.as_tensor([ [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 0, 255], ], dtype=torch.int).unsqueeze(0) faces_tensor = torch.as_tensor([ [0, 2, 3], [0, 3, 1], [0, 1, 2], [1, 3, 2], ], dtype=torch.int).unsqueeze(0) writer = SummaryWriter() writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor) writer.close()",0.96118855
tensorflow.python.training.input.string_input_producer,"Output strings (e.g. filenames) to a queue for an input pipeline. Note: if `num_epochs` is not `None`, this function creates local counter `epochs`. Use `local_variables_initializer()` to initialize local variables. Args: string_tensor: A 1-D string tensor with the strings to produce. num_epochs: An integer (optional). If specified, `string_input_producer` produces each string from `string_tensor` `num_epochs` times before generating an `OutOfRange` error. If not specified, `string_input_producer` can cycle through the strings in `string_tensor` an unlimited number of times. shuffle: Boolean. If true, the strings are randomly shuffled within each epoch. seed: An integer (optional). Seed used if shuffle == True. capacity: An integer. Sets the queue capacity. shared_name: (optional). If set, this queue will be shared under the given name across multiple sessions. All sessions open to the device which has this queue will be able to access it via the shared_name. Using this in a distributed setting means each name will only be seen by one of the sessions which has access to this operation. name: A name for the operations (optional). cancel_op: Cancel op for the queue (optional). Returns: A queue with the output strings. A `QueueRunner` for the Queue is added to the current `Graph`'s `QUEUE_RUNNER` collection. Raises: ValueError: If the string_tensor is a null Python list. At runtime, will fail with an assertion if string_tensor becomes a null tensor. @compatibility(eager) Input pipelines based on Queues are not supported when eager execution is enabled. Please use the `tf.data` API to ingest data under eager execution. @end_compatibility",torch.nn.modules.module.register_full_backward_pre_hook,"Register a backward pre-hook on the module. The hook will be called every time the gradients for the module are computed. The hook should have the following signature:: hook(module, grad_output) -> tuple[Tensor] or None The :attr:`grad_output` is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of :attr:`grad_output` in subsequent computations. Entries in :attr:`grad_output` will be ``None`` for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs inplace is not allowed when using backward hooks and will raise an error. Args: hook (Callable): The user-defined hook to be registered. prepend (bool): If true, the provided ``hook`` will be fired before all existing ``backward_pre`` hooks on this :class:`torch.nn.modules.Module`. Otherwise, the provided ``hook`` will be fired after all existing ``backward_pre`` hooks on this :class:`torch.nn.modules.Module`. Note that global ``backward_pre`` hooks registered with :func:`register_module_full_backward_pre_hook` will fire before all hooks registered by this method. Returns: :class:`torch.utils.hooks.RemovableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()``",0.9611758
tensorflow.python.data.ops.dataset_ops.take_while,"A transformation that stops dataset iteration based on a `predicate`. >>> dataset = tf.data.Dataset.range(10) >>> dataset = dataset.take_while(lambda x: x < 5) >>> list(dataset.as_numpy_iterator()) [0, 1, 2, 3, 4] Args: predicate: A function that maps a nested structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to a scalar `tf.bool` tensor. name: (Optional.) A name for the tf.data operation. Returns: A new `Dataset` with the transformation applied as described above.",torch.amp.autocast_mode.custom_bwd,"Create a helper decorator for backward methods of custom autograd functions. Autograd functions are subclasses of :class:`torch.autograd.Function`. Ensures that ``backward`` executes with the same autocast state as ``forward``. See the :ref:`example page<amp-custom-examples>` for more detail. Args: device_type(str): Device type to use. 'cuda', 'cpu', 'xpu' and so on. The type is the same as the `type` attribute of a :class:`torch.device`. Thus, you may obtain the device type of a tensor using `Tensor.device.type`.",0.9610908
tensorflow.python.ops.array_ops.size_v2,"Returns the size of a tensor. See also `tf.shape`. Returns a 0-D `Tensor` representing the number of elements in `input` of type `out_type`. Defaults to tf.int32. For example: >>> t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]) >>> tf.size(t) <tf.Tensor: shape=(), dtype=int32, numpy=12> Args: input: A `Tensor` or `SparseTensor`. out_type: (Optional) The specified non-quantized numeric output type of the operation. Defaults to `tf.int32`. (Note: there is an experimental flag, `tf_shape_default_int64` that changes the default to `tf.int64`. This is an unsupported, experimental setting that causes known breakages.) name: A name for the operation (optional). Returns: A `Tensor` of type `out_type`. Defaults to `tf.int32`. @compatibility(numpy) Equivalent to np.size() @end_compatibility",torch._higher_order_ops.associative_scan.associative_scan,"Performs an inclusive scan with an associative pointwise combine function. .. warning:: `torch.associative_scan` is a prototype feature in PyTorch. It currently does not support autograd and you may run into miscompiles. Read more about feature classification at: https://pytorch.org/blog/pytorch-feature-classification-changes/#prototype This operator requires runtime code generation and so requires support for ``torch.compile``. Further, only CUDA device codegen is supported at the moment. Args: combine_fn (Callable): A binary callable with type ``(Tensor, Tensor) -> Tensor``, or if input is a pytree ``(pytree, pytree) -> pytree``. This function must be pure, pointwise, and satisfy the associative property. input (torch.Tensor): The input tensor, or nested pytree of tensors. All inputs are expected to have the same shape. dim (int): the dimension to scan over Example:: def add(x: torch.Tensor, y: torch.Tensor): return x + y cumsum = associative_scan(add, x, dim)",0.96108097
tensorflow.python.ops.ragged.ragged_tensor.merge_dims,"Merges outer_axis...inner_axis into a single dimension. Returns a copy of this RaggedTensor with the specified range of dimensions flattened into a single dimension, with elements in row-major order. #### Examples: >>> rt = tf.ragged.constant([[[1, 2], [3]], [[4, 5, 6]]]) >>> print(rt.merge_dims(0, 1)) <tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]> >>> print(rt.merge_dims(1, 2)) <tf.RaggedTensor [[1, 2, 3], [4, 5, 6]]> >>> print(rt.merge_dims(0, 2)) tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32) To mimic the behavior of `np.flatten` (which flattens all dimensions), use `rt.merge_dims(0, -1). To mimic the behavior of `tf.layers.Flatten` (which flattens all dimensions except the outermost batch dimension), use `rt.merge_dims(1, -1)`. Args: outer_axis: `int`: The first dimension in the range of dimensions to merge. May be negative if `self.shape.rank` is statically known. inner_axis: `int`: The last dimension in the range of dimensions to merge. May be negative if `self.shape.rank` is statically known. Returns: A copy of this tensor, with the specified dimensions merged into a single dimension. The shape of the returned tensor will be `self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:]`, where `N` is the total number of slices in the merged dimensions.",torch.functional.split,"Splits the tensor into chunks. Each chunk is a view of the original tensor. If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension :attr:`dim` is not divisible by :attr:`split_size`. If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according to :attr:`split_size_or_sections`. Args: tensor (Tensor): tensor to split. split_size_or_sections (int) or (list(int)): size of a single chunk or list of sizes for each chunk dim (int): dimension along which to split the tensor. Example:: >>> a = torch.arange(10).reshape(5, 2) >>> a tensor([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) >>> torch.split(a, 2) (tensor([[0, 1], [2, 3]]), tensor([[4, 5], [6, 7]]), tensor([[8, 9]])) >>> torch.split(a, [1, 4]) (tensor([[0, 1]]), tensor([[2, 3], [4, 5], [6, 7], [8, 9]]))",0.96104324
tensorflow.python.ops.array_ops.zeros_like_v2,"Creates a tensor with all elements set to zero. See also `tf.zeros`. Given a single tensor or array-like object (`input`), this operation returns a tensor of the same type and shape as `input` with all elements set to zero. Optionally, you can use `dtype` to specify a new type for the returned tensor. Note that the layout of the input tensor is not preserved if the op is used inside tf.function. To obtain a tensor with the same layout as the input, chain the returned value to a `dtensor.relayout_like`. Examples: >>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]]) >>> tf.zeros_like(tensor) <tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0]], dtype=int32)> >>> tf.zeros_like(tensor, dtype=tf.float32) <tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.]], dtype=float32)> >>> tf.zeros_like([[1, 2, 3], [4, 5, 6]]) <tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0]], dtype=int32)> Args: input: A `Tensor` or array-like object. dtype: A type for the returned `Tensor`. Must be `float16`, `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `complex64`, `complex128`, `bool` or `string` (optional). name: A name for the operation (optional). layout: Optional, `tf.experimental.dtensor.Layout`. If provided, the result is a [DTensor](https://www.tensorflow.org/guide/dtensor_overview) with the provided layout. Returns: A `Tensor` with all elements set to zero.",torch.nn.utils.rnn.pad_packed_sequence,"Pad a packed batch of variable length sequences. It is an inverse operation to :func:`pack_padded_sequence`. The returned Tensor's data will be of size ``T x B x *`` (if :attr:`batch_first` is ``False``) or ``B x T x *`` (if :attr:`batch_first` is ``True``) , where ``T`` is the length of the longest sequence and ``B`` is the batch size. Example: >>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence >>> seq = torch.tensor([[1, 2, 0], [3, 0, 0], [4, 5, 6]]) >>> lens = [2, 1, 3] >>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False) >>> packed PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0])) >>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True) >>> seq_unpacked tensor([[1, 2, 0], [3, 0, 0], [4, 5, 6]]) >>> lens_unpacked tensor([2, 1, 3]) .. note:: :attr:`total_length` is useful to implement the ``pack sequence -> recurrent network -> unpack sequence`` pattern in a :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`. See :ref:`this FAQ section <pack-rnn-unpack-with-data-parallelism>` for details. Args: sequence (PackedSequence): batch to pad batch_first (bool, optional): if ``True``, the output will be in ``B x T x *`` format, ``T x B x *`` otherwise. padding_value (float, optional): values for padded elements. total_length (int, optional): if not ``None``, the output will be padded to have length :attr:`total_length`. This method will throw :class:`ValueError` if :attr:`total_length` is less than the max sequence length in :attr:`sequence`. Returns: Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to ``pack_padded_sequence`` or ``pack_sequence``.",0.96103984
tensorflow.python.autograph.pyct.loader.load_ast,"Loads the given AST as a Python module. Compiling the AST code this way ensures that the source code is readable by e.g. `pdb` or `inspect`. Args: nodes: Union[ast.AST, Iterable[ast.AST]], the code to compile, as an AST object. indentation: Text, the string to use for indentation. include_source_map: bool, whether return a source map. delete_on_exit: bool, whether to delete the temporary file used for compilation on exit. Returns: Tuple[module, Text, Dict[LineLocation, OriginInfo]], containing: the module containing the unparsed nodes, the source code corresponding to nodes, and the source map. Is include_source_map is False, the source map will be None.",torch.nn.modules.module.named_buffers,"Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool, optional): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True. remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True. Yields: (str, torch.Tensor): Tuple containing the name and buffer Example:: >>> # xdoctest: +SKIP(""undefined vars"") >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size())",0.96102965
tensorflow.python.keras.engine.training.test_on_batch,"Test the model on a single batch of samples. Args: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A TensorFlow tensor, or a list of tensors (in case the model has multiple inputs). - A dict mapping input names to the corresponding array/tensors, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or TensorFlow tensor(s). It should be consistent with `x` (you cannot have Numpy inputs and tensor targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. reset_metrics: If `True`, the metrics returned will be only for this batch. If `False`, the metrics will be statefully accumulated across batches. return_dict: If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list. Returns: Scalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs. Raises: RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`. ValueError: In case of invalid user-provided arguments.",torch.testing._internal.common_modules.no_batch_dim_reference_fn,"Reference function for modules supporting no batch dimensions. Unbatched inputs are unsqueezed to form a single batch input before passing them to the module. The output is squeezed to compare with the output of unbatched input to the module. Currently it only supports modules which return a single Tensor as output. You can bind the following kwargs. Kwargs: batch_first[bool] : If True, all the Tensors in `args` while be unsqueezed at dim `0` . and output will be squeezed at dim `0` else dim `1` for both. kwargs_to_batchify[dict] : Dictionary specifying the name of the argument and dimension to unsqueeze. Useful if there are few arguments whose batch dimension are different from the ones selected by `batch_first`. is_criterion[bool] : Specify if the module is a criterion and handle the reduction for output accordingly.",0.96099085
tensorflow.python.ops.array_ops.size,"Returns the size of a tensor. Returns a 0-D `Tensor` representing the number of elements in `input` of type `out_type`. Defaults to tf.int32. For example: python t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]) tf.size(t) # 12 Args: input: A `Tensor` or `SparseTensor`. name: A name for the operation (optional). out_type: (Optional) The specified non-quantized numeric output type of the operation. Defaults to `tf.int32`. (Note: there is an experimental flag, `tf_shape_default_int64` that changes the default to `tf.int64`. This is an unsupported, experimental setting that causes known breakages.) Returns: A `Tensor` of type `out_type`. Defaults to `tf.int32`. @compatibility(numpy) Equivalent to np.size() @end_compatibility",torch.fx.graph.call_function,"Insert a ``call_function`` ``Node`` into the ``Graph``. A ``call_function`` node represents a call to a Python callable, specified by ``the_function``. Args: the_function (Callable[..., Any]): The function to be called. Can be any PyTorch operator, Python function, or member of the ``builtins`` or ``operator`` namespaces. args (Optional[Tuple[Argument, ...]]): The positional arguments to be passed to the called function. kwargs (Optional[Dict[str, Argument]]): The keyword arguments to be passed to the called function type_expr (Optional[Any]): an optional type annotation representing the Python type the output of this node will have. Returns: The newly created and inserted ``call_function`` node. .. note:: The same insertion point and type expression rules apply for this method as :meth:`Graph.create_node`.",0.9609884
tensorflow.python.types.trace.is_subtype_of,"Returns True if `self` is a subtype of `other`. For example, `tf.function` uses subtyping for dispatch: if `a.is_subtype_of(b)` is True, then an argument of `TraceType` `a` can be used as argument to a `ConcreteFunction` traced with an a `TraceType` `b`. Args: other: A TraceType object to be compared against. Example: python class Dimension(TraceType): def __init__(self, value: Optional[int]): self.value = value def is_subtype_of(self, other): # Either the value is the same or other has a generalized value that # can represent any specific ones. return (self.value == other.value) or (other.value is None)",torch.jit.frontend.get_jit_def,"Build a JIT AST (TreeView) from the given function. Args: fn: A function object to compile or a pre-parsed ParsedDef object def_name: The name to give to the resulting AST object. This is not always the same as `fn.__name__`, for example: def _forward(self): ... forward = _forward In this case, the `__name__` attribute of the function object is ""_forward"", but we want the result AST to have the name ""forward"". self_name: If this function is a method, what the type name of `self` is.",0.9609486
tensorflow.python.keras.saving.save.load_model,"Loads a model saved via `model.save()`. Usage: >>> model = tf.keras.Sequential([ ... tf.keras.layers.Dense(5, input_shape=(3,)), ... tf.keras.layers.Softmax()]) >>> model.save('/tmp/model') >>> loaded_model = tf.keras.models.load_model('/tmp/model') >>> x = tf.random.uniform((10, 3)) >>> assert np.allclose(model.predict(x), loaded_model.predict(x)) Note that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as `""dense_1/kernel:0""`. It is recommended that you use the layer properties to access specific variables, e.g. `model.get_layer(""dense_1"").kernel`. Args: filepath: One of the following: - String or `pathlib.Path` object, path to the saved model - `h5py.File` object from which to load the model custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. compile: Boolean, whether to compile the model after loading. options: Optional `tf.saved_model.LoadOptions` object that specifies options for loading from SavedModel. Returns: A Keras model instance. If the original model was compiled, and saved with the optimizer, then the returned model will be compiled. Otherwise, the model will be left uncompiled. In the case that an uncompiled model is returned, a warning is displayed if the `compile` argument is set to `True`. Raises: ImportError: if loading from an hdf5 file and h5py is not available. IOError: In case of an invalid savefile.",torch.hub.load_state_dict_from_url,"Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in `model_dir`, it's deserialized and returned. The default value of ``model_dir`` is ``<hub_dir>/checkpoints`` where ``hub_dir`` is the directory returned by :func:`~torch.hub.get_dir`. Args: url (str): URL of the object to download model_dir (str, optional): directory in which to save the object map_location (optional): a function or a dict specifying how to remap storage locations (see torch.load) progress (bool, optional): whether or not to display a progress bar to stderr. Default: True check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False file_name (str, optional): name for the downloaded file. Filename from ``url`` will be used if not set. weights_only(bool, optional): If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See :func:`~torch.load` for more details. Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB) >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",0.9609444
tensorflow.python.framework.test_util.enable_graph_building_optimization,"Decorator for enabling graph_building_optimization on a test. This function returns a decorator intended to be applied to test methods in a `tf.test.TestCase` class. Doing so will enable graph_building_optimization, execute the test, then reset the feature flag to its default value. Example: class MyTest(test.TestCase): @enable_graph_building_optimization def testFoo(self): ... Args: fn: the function to be wrapped. Returns: The wrapped function.",torch._numpy.testing.utils.assert_no_gc_cycles,"Fail if the given callable produces any reference cycles. If called with all arguments omitted, may be used as a context manager: with assert_no_gc_cycles(): do_something() .. versionadded:: 1.15.0 Parameters ---------- func : callable The callable to test. \*args : Arguments Arguments passed to `func`. \*\*kwargs : Kwargs Keyword arguments passed to `func`. Returns ------- Nothing. The result is deliberately discarded to ensure that all cycles are found.",0.96092224
tensorflow.python.eager.context.get_device_details,Returns details about a physical devices. Args: device: A `tf.config.PhysicalDevice` returned by `tf.config.list_physical_devices` or `tf.config.get_visible_devices`. Returns: A dict with string keys.,torch.ao.quantization.backend_config.backend_config.to_dict,Convert this ``DTypeConfig`` to a dictionary with the items described in :func:`~torch.ao.quantization.backend_config.DTypeConfig.from_dict`.,0.9609015
tensorflow.python.framework.test_util.enable_nested_function_shape_inference,"Decorator for enabling nested_function_shape_inference on a test. This function returns a decorator intended to be applied to test methods in a `tf.test.TestCase` class. Doing so will set nested_function_shape_inference, reset the context, execute the test, then reset the context to the state it was in prior to this test. Example: class MyTest(test.TestCase): @enable_nested_function_shape_inference def testFoo(self): ... Args: fn: the function to be wrapped. Returns: The wrapped function.",torch.amp.grad_scaler.load_state_dict,"Load the scaler state. If this instance is disabled, :meth:`load_state_dict` is a no-op. Args: state_dict(dict): scaler state. Should be an object returned from a call to :meth:`state_dict`.",0.960863
tensorflow.python.training.supervisor.loop,"Start a LooperThread that calls a function periodically. If `timer_interval_secs` is None the thread calls `target(*args, **kwargs)` repeatedly. Otherwise it calls it every `timer_interval_secs` seconds. The thread terminates when a stop is requested. The started thread is added to the list of threads managed by the supervisor so it does not need to be passed to the `stop()` method. Args: timer_interval_secs: Number. Time boundaries at which to call `target`. target: A callable object. args: Optional arguments to pass to `target` when calling it. kwargs: Optional keyword arguments to pass to `target` when calling it. Returns: The started thread.",torch.distributed.elastic.rendezvous.etcd_server.start,"Start the server, and waits for it to be ready. When this function returns the sever is ready to take requests. Args: timeout: time (in seconds) to wait for the server to be ready before giving up. num_retries: number of retries to start the server. Each retry will wait for max ``timeout`` before considering it as failed. stderr: the standard error file handle. Valid values are `subprocess.PIPE`, `subprocess.DEVNULL`, an existing file descriptor (a positive integer), an existing file object, and `None`. Raises: TimeoutError: if the server is not ready within the specified timeout",0.96084374
tensorflow.python.types.trace.placeholder_value,"Creates a placeholder for tracing. tf.funcion traces with the placeholder value rather than the actual value. For example, a placeholder value can represent multiple different actual values. This means that the trace generated with that placeholder value is more general and reusable which saves expensive retracing. Args: placeholder_context: A context reserved for internal/future usage. For the `Fruit` example shared above, implementing: python class FruitTraceType: def placeholder_value(self, placeholder_context): return Fruit() instructs tf.function to trace with the `Fruit()` objects instead of the actual `Apple()` and `Mango()` objects when it receives a call to `get_mixed_flavor(Apple(), Mango())`. For example, Tensor arguments are replaced with Tensors of similar shape and dtype, output from a tf.Placeholder op. More generally, placeholder values are the arguments of a tf.function, as seen from the function's body: python @tf.function def foo(x): # Here `x` is be the placeholder value ... foo(x) # Here `x` is the actual value",torch.fx.graph.create_node,"Create a ``Node`` and add it to the ``Graph`` at the current insert-point. Note that the current insert-point can be set via :meth:`Graph.inserting_before` and :meth:`Graph.inserting_after`. Args: op (str): the opcode for this Node. One of 'call_function', 'call_method', 'get_attr', 'call_module', 'placeholder', or 'output'. The semantics of these opcodes are described in the ``Graph`` docstring. args (Optional[Tuple[Argument, ...]]): is a tuple of arguments to this node. kwargs (Optional[Dict[str, Argument]]): the kwargs of this Node name (Optional[str]): an optional string name for the ``Node``. This will influence the name of the value assigned to in the Python generated code. type_expr (Optional[Any]): an optional type annotation representing the Python type the output of this node will have. Returns: The newly-created and inserted node.",0.9608112
tensorflow.python.data.ops.dataset_ops.map_with_legacy_function,"Maps `map_func` across the elements of this dataset. Note: This is an escape hatch for existing uses of `map` that do not work with V2 functions. New uses are strongly discouraged and existing uses should migrate to `map` as this method will be removed in V2. Args: map_func: A function mapping a (nested) structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to another (nested) structure of tensors. num_parallel_calls: (Optional.) A `tf.int32` scalar `tf.Tensor`, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value `tf.data.AUTOTUNE` is used, then the number of parallel calls is set dynamically based on available CPU. deterministic: (Optional.) When `num_parallel_calls` is specified, this boolean controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the `tf.data.Options.deterministic` option (`True` by default) controls the behavior. Returns: Dataset: A `Dataset`.",torch.amp.autocast_mode.custom_fwd,"Create a helper decorator for ``forward`` methods of custom autograd functions. Autograd functions are subclasses of :class:`torch.autograd.Function`. See the :ref:`example page<amp-custom-examples>` for more detail. Args: device_type(str): Device type to use. 'cuda', 'cpu', 'xpu' and so on. The type is the same as the `type` attribute of a :class:`torch.device`. Thus, you may obtain the device type of a tensor using `Tensor.device.type`. cast_inputs (:class:`torch.dtype` or None, optional, default=None): If not ``None``, when ``forward`` runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes ``forward`` with autocast disabled. If ``None``, ``forward``'s internal ops execute with the current autocast state. .. note:: If the decorated ``forward`` is called outside an autocast-enabled region, :func:`custom_fwd<custom_fwd>` is a no-op and ``cast_inputs`` has no effect.",0.9608044
tensorflow.python.keras.legacy_tf_layers.base.keras_style_scope,"Use Keras-style variable management. All tf.layers and tf RNN cells created in this scope use Keras-style variable management. Creating such layers with a scope= argument is disallowed, and reuse=True is disallowed. The purpose of this scope is to allow users of existing layers to slowly transition to a Keras layers API without breaking existing functionality. One example of this is when using TensorFlow's RNN classes with Keras Models or Networks. Because Keras models do not properly set variable scopes, users of RNNs may either accidentally share scopes between two different models, or get errors about variables that already exist. Example: python class RNNModel(tf.keras.Model): def __init__(self, name): super(RNNModel, self).__init__(name=name) self.rnn = tf.compat.v1.nn.rnn_cell.MultiRNNCell( [tf.compat.v1.nn.rnn_cell.LSTMCell(64) for _ in range(2)]) def call(self, input, state): return self.rnn(input, state) model_1 = RNNModel(""model_1"") model_2 = RNNModel(""model_2"") # OK output_1, next_state_1 = model_1(input, state) # Raises an error about trying to create an already existing variable. output_2, next_state_2 = model_2(input, state) The solution is to wrap the model construction and execution in a keras-style scope: python with keras_style_scope(): model_1 = RNNModel(""model_1"") model_2 = RNNModel(""model_2"") # model_1 and model_2 are guaranteed to create their own variables. output_1, next_state_1 = model_1(input, state) output_2, next_state_2 = model_2(input, state) assert len(model_1.weights) > 0 assert len(model_2.weights) > 0 assert(model_1.weights != model_2.weights) Yields: A keras layer style scope.",torch._functorch.functional_call.stack_module_state,"stack_module_state(models) -> params, buffers Prepares a list of torch.nn.Modules for ensembling with :func:`vmap`. Given a list of ``M`` ``nn.Modules`` of the same class, returns two dictionaries that stack all of their parameters and buffers together, indexed by name. The stacked parameters are optimizable (i.e. they are new leaf nodes in the autograd history that are unrelated to the original parameters and can be passed directly to an optimizer). Here's an example of how to ensemble over a very simple model: .. code-block:: python num_models = 5 batch_size = 64 in_features, out_features = 3, 3 models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)] data = torch.randn(batch_size, 3) def wrapper(params, buffers, data): return torch.func.functional_call(model[0], (params, buffers), data) params, buffers = stack_module_state(models) output = vmap(wrapper, (0, 0, None))(params, buffers, data) assert output.shape == (num_models, batch_size, out_features) When there's submodules, this follows state dict naming conventions .. code-block:: python import torch.nn as nn class Foo(nn.Module): def __init__(self, in_features, out_features): super().__init__() hidden = 4 self.l1 = nn.Linear(in_features, hidden) self.l2 = nn.Linear(hidden, out_features) def forward(self, x): return self.l2(self.l1(x)) num_models = 5 in_features, out_features = 3, 3 models = [Foo(in_features, out_features) for i in range(num_models)] params, buffers = stack_module_state(models) print(list(params.keys())) # ""l1.weight"", ""l1.bias"", ""l2.weight"", ""l2.bias"" .. warning:: All of the modules being stacked together must be the same (except for the values of their parameters/buffers). For example, they should be in the same mode (training vs eval).",0.96075344
tensorflow.python.tpu.tpu.batch_parallel,"Shards `computation` along the batch dimension for parallel execution. Convenience wrapper around shard(). `inputs` must be a list of Tensors or None (equivalent to an empty list). Each input is split into `num_shards` pieces along the 0-th dimension, and computation is applied to each shard in parallel. Tensors are broadcast to all shards if they are lexically captured by `computation`. e.g., x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...) The outputs from all shards are concatenated back together along their 0-th dimension. Inputs and outputs of the computation must be at least rank-1 Tensors. Args: computation: A Python function that builds a computation to apply to each shard of the input. inputs: A list of input tensors or None (equivalent to an empty list). The 0-th dimension of each Tensor must have size divisible by `num_shards`. num_shards: The number of shards. infeed_queue: If not `None`, the `InfeedQueue` from which to append a tuple of arguments as inputs to `computation`. device_assignment: If not `None`, a `DeviceAssignment` describing the mapping between logical cores in the computation with physical cores in the TPU topology. Uses a default device assignment if `None`. The `DeviceAssignment` may be omitted if each shard of the computation uses only one core, and there is either only one shard, or the number of shards is equal to the number of cores in the TPU system. name: (Deprecated) Does nothing. xla_options: An instance of `tpu.XLAOptions` which indicates the options passed to XLA compiler. Use `None` for default options. Returns: A list of output tensors. Raises: ValueError: If `num_shards <= 0`",torch.distributed._tensor.api.from_local,"Create a :class:`DTensor` from a local torch.Tensor on each rank according to the `device_mesh` and `placements` specified. Args: local_tensor (torch.Tensor): local torch.Tensor on each rank. device_mesh (:class:`DeviceMesh`, optional): DeviceMesh to place the tensor, if not specified, must be called under a DeviceMesh context manager, default: None placements (List[:class:`Placement`], optional): the placements that describes how to place the local torch.Tensor on DeviceMesh, must have the same number of elements as `device_mesh.ndim`. If not specified, we will by default replicate the tensor across the `device_mesh` from the first rank of each dimension of the `device_mesh`. Keyword args: run_check (bool, optional): indicate whether to run check across ranks to check meta information and data. if have :class:`Replicate` in `placements`, the data on first rank of the device mesh dimension will be broadcasted to other ranks. shape (torch.Size, optional): A List of int which specifies the size of DTensor which build on top of `local_tensor`. Note this needs to be provided if the shape of `local_tensor` are different across the ranks. If not provided, `shape` will be computed assuming the given distributed tensor is evenly sharded across ranks. stride (tuple, optional): A List of int which specifies the stride of DTensor. If not provided, `stride` will be computed assuming the given distributed tensor is evenly sharded across ranks. Returns: A :class:`DTensor` object .. note:: `from_local` is differentiable, the `requires_grad` of the created `DTensor` object will depend on if `local_tensor` requires_grad or not.",0.96074855
tensorflow.python.keras.utils.generic_utils.get,"Given a shared object ID, returns a previously instantiated object. Args: object_id: shared object ID to use when attempting to find already-loaded object. Returns: The object, if we've seen this ID before. Else, `None`.",torch.distributed.rpc.backend_registry.backend_registered,"Checks if backend_name is registered as an RPC backend. Args: backend_name (str): string to identify the RPC backend. Returns: True if the backend has been registered with ``register_backend``, else False.",0.96073747
tensorflow.python.framework.importer.import_graph_def,"Imports the graph from `graph_def` into the current default `Graph`. This function provides a way to import a serialized TensorFlow [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto) protocol buffer, and extract individual objects in the `GraphDef` as `tf.Tensor` and `tf.Operation` objects. Once extracted, these objects are placed into the current default `Graph`. See `tf.Graph.as_graph_def` for a way to create a `GraphDef` proto. Args: graph_def: A `GraphDef` proto containing operations to be imported into the default graph. input_map: A dictionary mapping input names (as strings) in `graph_def` to `Tensor` objects. The values of the named input tensors in the imported graph will be re-mapped to the respective `Tensor` values. return_elements: A list of strings containing operation names in `graph_def` that will be returned as `Operation` objects; and/or tensor names in `graph_def` that will be returned as `Tensor` objects. name: (Optional.) A prefix that will be prepended to the names in `graph_def`. Note that this does not apply to imported function names. Defaults to `""import""`. op_dict: (Optional.) Deprecated, do not use. producer_op_list: (Optional.) An `OpList` proto with the (possibly stripped) list of `OpDef`s used by the producer of the graph. If provided, unrecognized attrs for ops in `graph_def` that have their default value according to `producer_op_list` will be removed. This will allow some more `GraphDef`s produced by later binaries to be accepted by earlier binaries. Returns: A list of `Operation` and/or `Tensor` objects from the imported graph, corresponding to the names in `return_elements`, and None if `returns_elements` is None. Raises: TypeError: If `graph_def` is not a `GraphDef` proto, `input_map` is not a dictionary mapping strings to `Tensor` objects, or `return_elements` is not a list of strings. ValueError: If `input_map`, or `return_elements` contains names that do not appear in `graph_def`, or `graph_def",torch.autograd.function.save_for_backward,"Save given tensors for a future call to :func:`~Function.backward`. ``save_for_backward`` should be called at most once, in either the :func:`setup_context` or :func:`forward` methods, and only with tensors. All tensors intended to be used in the backward pass should be saved with ``save_for_backward`` (as opposed to directly on ``ctx``) to prevent incorrect gradients and memory leaks, and enable the application of saved tensor hooks. See :class:`torch.autograd.graph.saved_tensors_hooks`. Note that if intermediary tensors, tensors that are neither inputs nor outputs of :func:`forward`, are saved for backward, your custom Function may not support double backward. Custom Functions that do not support double backward should decorate their :func:`backward` method with ``@once_differentiable`` so that performing double backward raises an error. If you'd like to support double backward, you can either recompute intermediaries based on the inputs during backward or return the intermediaries as the outputs of the custom Function. See the `double backward tutorial <https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html>`_ for more details. In :func:`backward`, saved tensors can be accessed through the :attr:`saved_tensors` attribute. Before returning them to the user, a check is made to ensure they weren't used in any in-place operation that modified their content. Arguments can also be ``None``. This is a no-op. See :ref:`extending-autograd` for more details on how to use this method. Example:: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD) >>> class Func(Function): >>> @staticmethod >>> def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int): >>> w = x * z >>> out = x * y + y * z + w * y >>> ctx.save_for_backward(x, y, w, out) >>> ctx.z = z # z is not a tensor >>> return out >>> >>> @staticmethod >>> @once_differentiable >>> def backward(ctx, grad_out): >>> x, y, w, out = ctx.saved_tensors >>> z = ctx.z >>> gx = grad_out * (y + y",0.96072686
tensorflow.python.ops.ctc_ops.ctc_greedy_decoder,"Performs greedy decoding on the logits given in input (best path). Given a tensor as `inputs`, the `blank_index` parameter defines the class index of the blank symbol. For example: If `blank_index` is equal to 1: >>> inf = float(""inf"") >>> logits = tf.constant([[[ 0., -inf, -inf], ... [ -2.3, -inf, -0.1]], ... [[ -inf, -0.5, -inf], ... [ -inf, -inf, -0.1]], ... [[ -inf, -inf, -inf], ... [ -0.1, -inf, -2.3]]]) >>> seq_lens = tf.constant([2, 3]) >>> outputs = tf.nn.ctc_greedy_decoder( ... logits, ... seq_lens, ... blank_index=1) Notes: - Unlike `ctc_beam_search_decoder`, `ctc_greedy_decoder` considers blanks as regular elements when computing the probability of a sequence. - Default `blank_index` is `(num_classes - 1)`, unless overriden. If `merge_repeated` is `True`, merge repeated classes in output. This means that if consecutive logits' maximum indices are the same, only the first of these is emitted. The sequence `A B B * B * B` (where '*' is the blank label) becomes * `A B B B` if `merge_repeated=True`. * `A B B B B` if `merge_repeated=False`. Args: inputs: 3-D `float` `Tensor` sized `[max_time, batch_size, num_classes]`. The logits. sequence_length: 1-D `int32` vector containing sequence lengths, having size `[batch_size]`. merge_repeated: Boolean. Default: True. blank_index: (Optional). Default: `num_classes - 1`. Define the class index to use for the blank label. Negative values will start from num_classes, ie, -1 will reproduce the ctc_greedy_decoder behavior of using num_classes - 1 for the blank symbol, which corresponds to the default. Returns: A tuple `(decoded, neg_sum_logits)` where decoded: A single-element list. `decoded[0]` is an `SparseTensor` containing the decoded outputs s.t.: `decoded.indices`: Indices matrix `(total_decoded_outputs, 2)`. The rows store: `[batch, time]`. `decoded.values`: Values vector, size `(total_decoded_outputs)`. The vector stores the decoded classes. `decoded.dense_shape`: Shape vector, size `(2)`. The shape values are: `[",torch.utils.data._utils.collate.default_collate,"Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a :class:`torch.Tensor`, a `Sequence` of :class:`torch.Tensor`, a Collection of :class:`torch.Tensor`, or left unchanged, depending on the input type. This is used as the default function for collation when `batch_size` or `batch_sampler` is defined in :class:`~torch.utils.data.DataLoader`. Here is the general input type (based on the type of the element within the batch) to output type mapping: * :class:`torch.Tensor` -> :class:`torch.Tensor` (with an added outer dimension batch size) * NumPy Arrays -> :class:`torch.Tensor` * `float` -> :class:`torch.Tensor` * `int` -> :class:`torch.Tensor` * `str` -> `str` (unchanged) * `bytes` -> `bytes` (unchanged) * `Mapping[K, V_i]` -> `Mapping[K, default_collate([V_1, V_2, ...])]` * `NamedTuple[V1_i, V2_i, ...]` -> `NamedTuple[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]` * `Sequence[V1_i, V2_i, ...]` -> `Sequence[default_collate([V1_1, V1_2, ...]), default_collate([V2_1, V2_2, ...]), ...]` Args: batch: a single batch to be collated Examples: >>> # xdoctest: +SKIP >>> # Example with a batch of `int`s: >>> default_collate([0, 1, 2, 3]) tensor([0, 1, 2, 3]) >>> # Example with a batch of `str`s: >>> default_collate(['a', 'b', 'c']) ['a', 'b', 'c'] >>> # Example with `Map` inside the batch: >>> default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}]) {'A': tensor([ 0, 100]), 'B': tensor([ 1, 100])} >>> # Example with `NamedTuple` inside the batch: >>> Point = namedtuple('Point', ['x', 'y']) >>> default_collate([Point(0, 0), Point(1, 1)]) Point(x=tensor([0, 1]), y=tensor([0, 1])) >>> # Example with `Tuple` inside the batch: >>> default_collate([(0, 1), (2, 3)]) [tensor([0, 2]), tensor([1, 3])] >>> # Example with `List` inside the batch: >>> default_collate([[0, 1], [2, 3]]) [tensor([0, 2]), tensor([1, 3])] >>> # Two options to extend `default_coll",0.9606993
tensorflow.python.keras.engine.base_layer_utils.needs_keras_history,"Check if any Tensors need to be wrapped in TensorFlowOpLayers. This will never return True inside a sublayer, because sublayers do not need to create Keras History. Otherwise, this returns True if one or more of `tensors` originates from a `keras.Input` and does not have `_keras_history` set. Args: tensors: An arbitrary nested structure of Tensors. ignore_call_context: Whether to ignore the check of if currently outside of a `call` context. This is `True` when creating KerasHistory inside `Node`, where we always know that Tensors are being used with the Functional API. Returns: Bool, whether at least one Tensor needs to be wrapped.",torch.distributed.checkpoint.storage.write_data,"Write all items from ``plan`` using ``planner`` to resolve the data. A subclass should call ``SavePlanner::resolve_data`` on each item from the plan to get access to the underlying object to write. Subclasses should lazily call `resolve_data` as it can allocate memory. In case of tensors, make following assumptions: - They might be on any device, including not matching the one on ``WriteItem::tensor_data`` - They might be views or not contiguous. Only the projection needs to be saved. Args: plan (SavePlan): The save plan to execute. planner (SavePlanner): Planner object to be used to resolve items to data. Returns: A future that completes to a list of WriteResult",0.96068764
tensorflow.python.framework.ops.finalize,"Finalizes this graph, making it read-only. After calling `g.finalize()`, no new operations can be added to `g`. This method is used to ensure that no operations are added to a graph when it is shared between multiple threads, for example when using a `tf.compat.v1.train.QueueRunner`.",torch.utils.data.datapipes.datapipe.reset,"Reset the `IterDataPipe` to the initial state. By default, no-op. For subclasses of `IterDataPipe`, depending on their functionalities, they may want to override this method with implementations that may clear the buffers and reset pointers of the DataPipe. The `reset` method is always called when `__iter__` is called as part of `hook_iterator`.",0.96067035
tensorflow.python.keras.backend.map_fn,Map the function fn over the elements elems and return the outputs. Args: fn: Callable that will be called upon each element in elems elems: tensor name: A string name for the map node in the graph dtype: Output data type. Returns: Tensor with dtype `dtype`.,torch.onnx.symbolic_opset9.sigmoid,Converts the corresponding PyTorch function into ONNX operators. It is not meant to be called directly by a user. Args: g (jit_utils.GraphContext): Graph context. self (Tensor): the input tensor. Returns: ONNX operator,0.9606303
tensorflow.python.ops.nn_ops.log_softmax_v2,"Computes log softmax activations. For each batch `i` and class `j` we have logsoftmax = logits - log(reduce_sum(exp(logits), axis)) Args: logits: A non-empty `Tensor`. Must be one of the following types: `half`, `float32`, `float64`. axis: The dimension softmax would be performed on. The default is -1 which indicates the last dimension. name: A name for the operation (optional). Returns: A `Tensor`. Has the same type as `logits`. Same shape as `logits`. Raises: InvalidArgumentError: if `logits` is empty or `axis` is beyond the last dimension of `logits`.",torch.nn.functional.dropout2d,"Randomly zero out entire channels (a channel is a 2D feature map). For example, the :math:`j`-th channel of the :math:`i`-th sample in the batched input is a 2D tensor :math:`\text{input}[i, j]` of the input tensor. Each channel will be zeroed out independently on every forward call with probability :attr:`p` using samples from a Bernoulli distribution. See :class:`~torch.nn.Dropout2d` for details. Args: p: probability of a channel to be zeroed. Default: 0.5 training: apply dropout if is ``True``. Default: ``True`` inplace: If set to ``True``, will do this operation in-place. Default: ``False``",0.9605999
tensorflow.python.data.experimental.ops.testing.assert_prev,"Asserts which transformations, with which attributes, happened previously. Each transformation is repesented as a tuple in the input. The first element is the base op name of the transformation, not including version suffix. For example, use ""BatchDataset"" instead of ""BatchDatasetV2"". ""BatchDataset"" will match any of ""BatchDataset"", ""BatchDatasetV1"", ""BatchDatasetV2"", etc. The second element is a dict of attribute name-value pairs. Attributes values must be of type bool, int, or string. Example usage: >>> dataset_ops.Dataset.from_tensors(0) \ ... .map(lambda x: x) \ ... .batch(1, deterministic=True, num_parallel_calls=8) \ ... .assert_prev([(""ParallelBatchDataset"", {""deterministic"": True}), \ ... (""MapDataset"", {})]) Args: transformations: A list of tuples identifying the (required) transformation name, with (optional) attribute name-value pairs, that are expected to have happened previously. Returns: A `Dataset` transformation function, which can be passed to `tf.data.Dataset.apply`.",torch.utils.tensorboard.writer.add_hparams,"Add a set of hyperparameters to be compared in TensorBoard. Args: hparam_dict (dict): Each key-value pair in the dictionary is the name of the hyper parameter and it's corresponding value. The type of the value can be one of `bool`, `string`, `float`, `int`, or `None`. metric_dict (dict): Each key-value pair in the dictionary is the name of the metric and it's corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by ``add_scalar`` will be displayed in hparam plugin. In most cases, this is unwanted. hparam_domain_discrete: (Optional[Dict[str, List[Any]]]) A dictionary that contains names of the hyperparameters and all discrete values they can hold run_name (str): Name of the run, to be included as part of the logdir. If unspecified, will use current timestamp. global_step (int): Global step value to record Examples:: from torch.utils.tensorboard import SummaryWriter with SummaryWriter() as w: for i in range(5): w.add_hparams({'lr': 0.1*i, 'bsize': i}, {'hparam/accuracy': 10*i, 'hparam/loss': 10*i}) Expected result: .. image:: _static/img/tensorboard/add_hparam.png :scale: 50 %",0.9605757
tensorflow.python.ops.structured.structured_tensor.merge_dims,"Merges outer_axis...inner_axis into a single dimension. Returns a copy of this RaggedTensor with the specified range of dimensions flattened into a single dimension, with elements in row-major order. >>> st = tf.experimental.StructuredTensor.from_pyval( ... [[{'foo': 12}, {'foo': 33}], [], [{'foo': 99}]]) >>> st.merge_dims(0, 1) <StructuredTensor( fields={ ""foo"": tf.Tensor([12 33 99], shape=(3,), dtype=int32)}, shape=(3,))> Args: outer_axis: `int`: The first dimension in the range of dimensions to merge. May be negative (to index from the last dimension). inner_axis: `int`: The last dimension in the range of dimensions to merge. May be negative (to index from the last dimension). Returns: A copy of this tensor, with the specified dimensions merged into a single dimension. The shape of the returned tensor will be `self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:]`, where `N` is the total number of slices in the merged dimensions.",torch.nn.utils._expanded_weights.conv_utils.unfold3d,"Extract sliding local blocks from an batched input tensor. :class:`torch.nn.Unfold` only supports 4D inputs (batched image-like tensors). This method implements the same action for 5D inputs Args: tensor: An input tensor of shape ``(B, C, D, H, W)``. kernel_size: the size of the sliding blocks padding: implicit zero padding to be added on both sides of input stride: the stride of the sliding blocks in the input spatial dimensions dilation: the spacing between the kernel points. Returns: A tensor of shape ``(B, C * np.prod(kernel_size), L)``, where L - output spatial dimensions. See :class:`torch.nn.Unfold` for more details Example: >>> # xdoctest: +SKIP >>> B, C, D, H, W = 3, 4, 5, 6, 7 >>> tensor = torch.arange(1, B * C * D * H * W + 1.).view(B, C, D, H, W) >>> unfold3d(tensor, kernel_size=2, padding=0, stride=1).shape torch.Size([3, 32, 120])",0.96057296
tensorflow.python.ops.sparse_ops.sparse_tensor_to_dense,"Converts a `SparseTensor` into a dense tensor. For this sparse tensor with three non-empty values: >>> sp_input = tf.sparse.SparseTensor( ... dense_shape=[3, 5], ... values=[7, 8, 9], ... indices =[[0, 1], ... [0, 3], ... [2, 0]]) The output will be a dense `[3, 5]` tensor with values: >>> tf.sparse.to_dense(sp_input).numpy() array([[0, 7, 0, 8, 0], [0, 0, 0, 0, 0], [9, 0, 0, 0, 0]], dtype=int32) Note: Indices must be without repeats. This is only tested if `validate_indices` is `True`. Args: sp_input: The input `SparseTensor`. default_value: Scalar value to set for indices not specified in `sp_input`. Defaults to zero. validate_indices: A boolean value. If `True`, indices are checked to make sure they are sorted in lexicographic order and that there are no repeats. name: A name prefix for the returned tensors (optional). Returns: A dense tensor with shape `sp_input.dense_shape` and values specified by the non-empty values in `sp_input`. Indices not in `sp_input` are assigned `default_value`. Raises: TypeError: If `sp_input` is not a `SparseTensor`.",torch.nn.utils.rnn.pack_sequence,"Packs a list of variable length Tensors. Consecutive call of the next functions: ``pad_sequence``, ``pack_padded_sequence``. ``sequences`` should be a list of Tensors of size ``L x *``, where `L` is the length of a sequence and `*` is any number of trailing dimensions, including zero. For unsorted sequences, use `enforce_sorted = False`. If ``enforce_sorted`` is ``True``, the sequences should be sorted in the order of decreasing length. ``enforce_sorted = True`` is only necessary for ONNX export. Example: >>> from torch.nn.utils.rnn import pack_sequence >>> a = torch.tensor([1, 2, 3]) >>> b = torch.tensor([4, 5]) >>> c = torch.tensor([6]) >>> pack_sequence([a, b, c]) PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None) Args: sequences (list[Tensor]): A list of sequences of decreasing length. enforce_sorted (bool, optional): if ``True``, checks that the input contains sequences sorted by length in a decreasing order. If ``False``, this condition is not checked. Default: ``True``. Returns: a :class:`PackedSequence` object",0.9605682
tensorflow.python.ops.metrics_impl.recall_at_top_k,"Computes recall@k of top-k predictions with respect to sparse labels. Differs from `recall_at_k` in that predictions must be in the form of top `k` class indices, whereas `recall_at_k` expects logits. Refer to `recall_at_k` for more details. Args: labels: `int64` `Tensor` or `SparseTensor` with shape [D1, ... DN, num_labels] or [D1, ... DN], where the latter implies num_labels=1. N >= 1 and num_labels is the number of target classes for the associated prediction. Commonly, N=1 and `labels` has shape [batch_size, num_labels]. [D1, ... DN] must match `predictions`. Values should be in range [0, num_classes), where num_classes is the last dimension of `predictions`. Values outside this range always count towards `false_negative_at_<k>`. predictions_idx: Integer `Tensor` with shape [D1, ... DN, k] where N >= 1. Commonly, N=1 and predictions has shape [batch size, k]. The final dimension contains the top `k` predicted class indices. [D1, ... DN] must match `labels`. k: Integer, k for @k metric. Only used for the default op name. class_id: Integer class ID for which we want binary metrics. This should be in range [0, num_classes), where num_classes is the last dimension of `predictions`. If class_id is outside this range, the method returns NAN. weights: `Tensor` whose rank is either 0, or n-1, where n is the rank of `labels`. If the latter, it must be broadcastable to `labels` (i.e., all dimensions must be either `1`, or the same as the corresponding `labels` dimension). metrics_collections: An optional list of collections that values should be added to. updates_collections: An optional list of collections that updates should be added to. name: Name of new update operation, and namespace for other dependent ops. Returns: recall: Scalar `float64` `Tensor` with the value of `true_positives` divided by the sum of `true_positives` and `false_negatives`. update_op: `Operation` that increments `true_positives` and `false_negatives` variables appropriately, and whose value matc",torch.nn.functional.nll_loss,"Compute the negative log likelihood loss. See :class:`~torch.nn.NLLLoss` for details. Args: input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)` in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \geq 1` in the case of K-dimensional loss. `input` is expected to be log-probabilities. target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for K-dimensional loss. weight (Tensor, optional): a manual rescaling weight given to each class. If given, has to be a Tensor of size `C` size_average (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field :attr:`size_average` is set to ``False``, the losses are instead summed for each minibatch. Ignored when reduce is ``False``. Default: ``True`` ignore_index (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. When :attr:`size_average` is ``True``, the loss is averaged over non-ignored targets. Default: -100 reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per batch element instead and ignores :attr:`size_average`. Default: ``True`` reduction (str, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied, ``'mean'``: the sum of the output will be divided by the number of elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average` and :attr:`reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override :attr:`reduction`. Default: ``'mean'`` Example:: >>> # input is of ",0.9604489
tensorflow.python.ops.nn_ops.avg_pool1d,"Performs the average pooling on the input. Each entry in `output` is the mean of the corresponding size `ksize` window in `value`. Note internally this op reshapes and uses the underlying 2d operation. Args: input: A 3-D `Tensor` of the format specified by `data_format`. ksize: An int or list of `ints` that has length `1` or `3`. The size of the window for each dimension of the input tensor. strides: An int or list of `ints` that has length `1` or `3`. The stride of the sliding window for each dimension of the input tensor. padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm. See [here](https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2) for more information. data_format: An optional string from: ""NWC"", ""NCW"". Defaults to ""NWC"". name: A name for the operation (optional). Returns: A `Tensor` of format specified by `data_format`. The max pooled output tensor.",torch.autograd.forward_ad.make_dual,"Associate a tensor value with its tangent to create a ""dual tensor"" for forward AD gradient computation. The result is a new tensor aliased to :attr:`tensor` with :attr:`tangent` embedded as an attribute as-is if it has the same storage layout or copied otherwise. The tangent attribute can be recovered with :func:`unpack_dual`. This function is backward differentiable. Given a function `f` whose jacobian is `J`, it allows one to compute the Jacobian-vector product (`jvp`) between `J` and a given vector `v` as follows. Example:: >>> # xdoctest: +SKIP(""Undefined variables"") >>> with dual_level(): ... inp = make_dual(x, v) ... out = f(inp) ... y, jvp = unpack_dual(out) Please see the `forward-mode AD tutorial <https://pytorch.org/tutorials/intermediate/forward_ad_usage.html>`__ for detailed steps on how to use this API.",0.96043205
tensorflow.python.saved_model.registration.registration.register_serializable,"Decorator for registering a serializable class. THIS METHOD IS STILL EXPERIMENTAL AND MAY CHANGE AT ANY TIME. Registered classes will be saved with a name generated by combining the `package` and `name` arguments. When loading a SavedModel, modules saved with this registered name will be created using the `_deserialize_from_proto` method. By default, only direct instances of the registered class will be saved/ restored with the `serialize_from_proto`/`deserialize_from_proto` methods. To extend the registration to subclasses, use the `predicate argument`: python class A(tf.Module): pass register_serializable( package=""Example"", predicate=lambda obj: isinstance(obj, A))(A) Args: package: The package that this class belongs to. name: The name to serialize this class under in this package. If None, the class's name will be used. predicate: An optional function that takes a single Trackable argument, and determines whether that object should be serialized with this `package` and `name`. The default predicate checks whether the object's type exactly matches the registered class. Predicates are executed in the reverse order that they are added (later registrations are checked first). Returns: A decorator that registers the decorated class with the passed names and predicate.",torch.package.package_exporter.extern,"Include ``module`` in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package. Args: include (Union[List[str], str]): A string e.g. ``""my_package.my_subpackage""``, or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`. exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string. allow_empty (bool): An optional flag that specifies whether the extern modules specified by this call to the ``extern`` method must be matched to some module during packaging. If an extern module glob pattern is added with ``allow_empty=False``, and :meth:`close` is called (either explicitly or via ``__exit__``) before any modules match that pattern, an exception is thrown. If ``allow_empty=True``, no such exception is thrown.",0.9604243
tensorflow.python.ops.critical_section_ops.execute,"Execute function `fn()` inside the critical section. `fn` should not accept any arguments. To add extra arguments to when calling `fn` in the critical section, create a lambda: python critical_section.execute(lambda: fn(*my_args, **my_kwargs)) Args: fn: The function to execute. Must return at least one tensor. exclusive_resource_access: Whether the resources required by `fn` should be exclusive to this `CriticalSection`. Default: `True`. You may want to set this to `False` if you will be accessing a resource in read-only mode in two different CriticalSections. name: The name to use when creating the execute operation. Returns: The tensors returned from `fn()`. Raises: ValueError: If `fn` attempts to lock this `CriticalSection` in any nested or lazy way that may cause a deadlock. ValueError: If `exclusive_resource_access == True` and another `CriticalSection` has an execution requesting the same resources as `fn``. Note, even if `exclusive_resource_access` is `True`, if another execution in another `CriticalSection` was created without `exclusive_resource_access=True`, a `ValueError` will be raised.",torch.package.package_exporter.intern,"Specify modules that should be packaged. A module must match some ``intern`` pattern in order to be included in the package and have its dependencies processed recursively. Args: include (Union[List[str], str]): A string e.g. ""my_package.my_subpackage"", or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in :meth:`mock`. exclude (Union[List[str], str]): An optional pattern that excludes some patterns that match the include string. allow_empty (bool): An optional flag that specifies whether the intern modules specified by this call to the ``intern`` method must be matched to some module during packaging. If an ``intern`` module glob pattern is added with ``allow_empty=False``, and :meth:`close` is called (either explicitly or via ``__exit__``) before any modules match that pattern, an exception is thrown. If ``allow_empty=True``, no such exception is thrown.",0.9604174
tensorflow.python.ops.math_ops.sparse_segment_sqrt_n,"Computes the sum along sparse segments of a tensor divided by the sqrt(N). `N` is the size of the segment being reduced. Args: data: A `Tensor` with data that will be assembled in the output. indices: A 1-D `Tensor` with indices into `data`. Has same rank as `segment_ids`. segment_ids: A 1-D `Tensor` with indices into the output `Tensor`. Values should be sorted and can be repeated. name: A name for the operation (optional). num_segments: An optional int32 scalar. Indicates the size of the output `Tensor`. sparse_gradient: An optional `bool`. Defaults to `False`. If `True`, the gradient of this function will be sparse (IndexedSlices) instead of dense (Tensor). Returns: A `tensor` of the shape as data, except for dimension 0 which has size `k`, the number of segments specified via `num_segments` or inferred for the last element in `segments_ids`.",torch.nn.utils.prune.prune,"Compute and returns a pruned version of input tensor ``t``. According to the pruning rule specified in :meth:`compute_mask`. Args: t (torch.Tensor): tensor to prune (of same dimensions as ``default_mask``). importance_scores (torch.Tensor): tensor of importance scores (of same shape as ``t``) used to compute mask for pruning ``t``. The values in this tensor indicate the importance of the corresponding elements in the ``t`` that is being pruned. If unspecified or None, the tensor ``t`` will be used in its place. default_mask (torch.Tensor, optional): mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones. Returns: pruned version of tensor ``t``.",0.9603682
tensorflow.python.checkpoint.restore.value_tensors,"Create value `Tensor`s for this object's attributes. Does not require that the Python object has been created. Used for restore-on-create when executing eagerly. Args: shape_and_slices: A dict mapping from object attribute names to a shape and slice string that will be passed to a RestoreV2 op. If the dict is None or if an object attribute is not in the dict, the full tensor will be restored. Returns: A dictionary mapping from object attribute names to `Tensor`s.",torch.ao.quantization.fx.custom_config.from_dict,"Create a ``ConvertCustomConfig`` from a dictionary with the following items: ""preserved_attributes"": a list of attributes that persist even if they are not used in ``forward`` This function is primarily for backward compatibility and may be removed in the future.",0.9603493
tensorflow.python.distribute.cross_device_utils.all_reduce,"All-reduce a dense tensor. Args: input_tensor: a dense tensor. It must have the same shape on all replicas. control_input: if not None, add control edges between control_input and the all-reduce. options: an optional tf.distribute.experimental.CommunicationOptions. If provided, it overrides the default options. Returns: The reduced tensor.",torch.distributed.nn.functional.all_reduce,"Reduces the tensor data across all machines in such a way that all get the final result. After the call the returned tensor is going to be bitwise identical in all processes. Arguments: tensor (Tensor): Input of the collective. op (optional): One of the values from ``torch.distributed.ReduceOp`` enum. Specifies an operation used for element-wise reductions. group (ProcessGroup, optional): The process group to work on. Returns: Tensor: Output of the collective",0.96034837
tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.shutdown_tpu_system,"Shuts down the TPU devices. This will clear all caches, even those that are maintained through sequential calls to tf.tpu.experimental.initialize_tpu_system, such as the compilation cache. Args: cluster_resolver: A tf.distribute.cluster_resolver.TPUClusterResolver, which provides information about the TPU cluster. Raises: RuntimeError: If no TPU devices found for eager execution or if run in a tf.function.",torch.distributed.elastic.multiprocessing.api.close,"Terminates all processes managed by this context and cleans up any meta resources (e.g. redirect, error_file files). Args: death_sig: Death signal to terminate processes. timeout: Time to wait for processes to finish, if process is still alive after this time, it will be terminated via SIGKILL.",0.96031815
tensorflow.python.keras.utils.generic_utils.is_default,Check if a method is decorated with the `default` wrapper.,torch.nn.modules.lazy.has_uninitialized_params,Check if a module has parameters that are not initialized.,0.9603166
tensorflow.python.ops.variables.variables_initializer,"Returns an Op that initializes a list of variables. After you launch the graph in a session, you can run the returned Op to initialize all the variables in `var_list`. This Op runs all the initializers of the variables in `var_list` in parallel. Calling `initialize_variables()` is equivalent to passing the list of initializers to `Group()`. If `var_list` is empty, however, the function still returns an Op that can be run. That Op just has no effect. @compatibility(TF2) In TF2, variables are initialized immediately when they are created. There is no longer a need to run variable initializers before using them. @end_compatibility Args: var_list: List of `Variable` objects to initialize. name: Optional name for the returned operation. Returns: An Op that run the initializers of all the specified variables.",torch.distributed._composable.fsdp.fully_shard.unshard,"Unshards the module's parameters by allocating memory and all-gathering the parameters. This method is *not* recursive. Args: async_op (bool): If ``True``, then returns a :class:`UnshardHandle` that has a :meth:`wait` method to wait on the unshard op. If ``False``, then returns ``None`` and waits on the handle inside this function. .. warning:: This method is experimental and subject to change. .. note:: If ``async_op=True``, then the user does not have to call :meth:`wait` on the returned handle if waiting on the unshard op in the module's pre-forward is tolerable. FSDP will wait on the pending unshard op in the pre-forward automatically.",0.96007776
tensorflow.python.tpu.tpu_optimizer.compute_gradients,"Compute gradients of ""loss"" for the variables in ""var_list"". This simply wraps `compute_gradients()` from the real optimizer. The gradients will be aggregated in `apply_gradients()` so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas. When the CrossShardOptimizer is constructed with `reduction == losses.Reduction.MEAN` (default), this function scales the loss by `1.0 / num_shards` before computing the gradients. Assuming the optimizer uses the default implementation of `compute_gradients()`, the gradients of the scaled loss are scaled by `1.0 / num_shards` compared to the gradients of the original loss. This scaling factor is important because `apply_gradients()` sums gradients across shards, rather than averaging them. However, the scaling factor must be taken into account when clipping the norm of the gradients or performing other postprocessing. Args: loss: A Tensor containing the value to minimize. var_list: Optional list or tuple of `tf.Variable` to update to minimize `loss`. Defaults to the list of variables collected in the graph under the key `GraphKey.TRAINABLE_VARIABLES`. **kwargs: Keyword arguments for compute_gradients(). Returns: A list of (gradient, variable) pairs. Raises: ValueError: If not within a tpu_shard_context or group_assignment is invalid.",torch.distributed.fsdp.fully_sharded_data_parallel.clip_grad_norm_,"Clip the gradient norm of all parameters. The norm is computed over all parameters' gradients as viewed as a single vector, and the gradients are modified in-place. Args: max_norm (float or int): max norm of the gradients norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for infinity norm. Returns: Total norm of the parameters (viewed as a single vector). If every FSDP instance uses ``NO_SHARD``, meaning that no gradients are sharded across ranks, then you may directly use :func:`torch.nn.utils.clip_grad_norm_`. If at least some FSDP instance uses a sharded strategy (i.e. one other than ``NO_SHARD``), then you should use this method instead of :func:`torch.nn.utils.clip_grad_norm_` since this method handles the fact that gradients are sharded across ranks. The total norm returned will have the ""largest"" dtype across all parameters/gradients as defined by PyTorch's type promotion semantics. For example, if *all* parameters/gradients use a low precision dtype, then the returned norm's dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm's dtype will be FP32. .. warning:: This needs to be called on all ranks since it uses collective communications.",0.95991814
tensorflow.python.ops.variables.moving_average_variables,"Returns all variables that maintain their moving averages. If an `ExponentialMovingAverage` object is created and the `apply()` method is called on a list of variables, these variables will be added to the `GraphKeys.MOVING_AVERAGE_VARIABLES` collection. This convenience function returns the contents of that collection. Args: scope: (Optional.) A string. If supplied, the resulting list is filtered to include only items whose `name` attribute matches `scope` using `re.match`. Items without a `name` attribute are never returned if a scope is supplied. The choice of `re.match` means that a `scope` without special tokens filters by prefix. Returns: A list of Variable objects.",torch.utils.data.graph.traverse,"Traverse the DataPipes and their attributes to extract the DataPipe graph. [Deprecated] When ``only_dataPipe`` is specified as ``True``, it would only look into the attribute from each DataPipe that is either a DataPipe and a Python collection object such as ``list``, ``tuple``, ``set`` and ``dict``. Note: This function is deprecated. Please use `traverse_dps` instead. Args: datapipe: the end DataPipe of the graph only_datapipe: If ``False`` (default), all attributes of each DataPipe are traversed. This argument is deprecating and will be removed after the next release. Returns: A graph represented as a nested dictionary, where keys are ids of DataPipe instances and values are tuples of DataPipe instance and the sub-graph",0.95987993
tensorflow.python.feature_column.serialization.serialize_feature_columns,Serializes a list of FeatureColumns. Returns a list of Keras-style config dicts that represent the input FeatureColumns and can be used with `deserialize_feature_columns` for reconstructing the original columns. Args: feature_columns: A list of FeatureColumns. Returns: Keras serialization for the list of FeatureColumns. Raises: ValueError if called with input that is not a list of FeatureColumns.,torch.ao.quantization.fx._model_report.model_report_visualizer.get_all_unique_feature_names,"The purpose of this method is to provide a user the set of all feature names so that if they wish to use the filtering capabilities of the generate_table_view(), or use either of the generate_plot_view() or generate_histogram_view(), they don't need to manually parse the generated_reports dictionary to get this information. Args: plottable_features_only (bool): True if the user is only looking for plottable features, False otherwise plottable features are those that are tensor values Default: True (only return those feature names that are plottable) Returns all the unique module fqns present in the reports the ModelReportVisualizer instance was initialized with.",0.95981735
tensorflow.python.data.benchmarks.benchmark_base.run_and_report_benchmark,"Benchmarks the dataset and reports the stats. Runs the dataset `iters` times. In each iteration, the benchmark measures the time it takes to go through `num_elements` elements of the dataset. This is followed by logging/printing the benchmark stats. Args: dataset: Dataset to benchmark. num_elements: Number of dataset elements to iterate through each benchmark iteration. name: Name of the benchmark. iters: Number of times to repeat the timing. extras: A dict which maps string keys to additional benchmark info. warmup: If true, warms up the session caches by running an untimed run. apply_default_optimizations: Determines whether default optimizations should be applied. session_config: A ConfigProto protocol buffer with configuration options for the session. Applicable only for benchmarking in graph mode. Returns: A float, representing the per-element wall time of the dataset in seconds. This is the median time (with respect to `iters`) it takes for the dataset to go through `num_elements` elements, divided by `num_elements.`",torch.utils.throughput_benchmark.benchmark,"Run a benchmark on the module. Args: num_warmup_iters (int): Warmup iters are used to make sure we run a module a few times before actually measuring things. This way we avoid cold caches and any other similar problems. This is the number of warmup iterations for each of the thread in separate num_iters (int): Number of iterations the benchmark should run with. This number is separate from the warmup iterations. Also the number is shared across all the threads. Once the num_iters iterations across all the threads is reached, we will stop execution. Though total number of iterations might be slightly larger. Which is reported as stats.num_iters where stats is the result of this function profiler_output_path (str): Location to save Autograd Profiler trace. If not empty, Autograd Profiler will be enabled for the main benchmark execution (but not the warmup phase). The full trace will be saved into the file path provided by this argument This function returns BenchmarkExecutionStats object which is defined via pybind11. It currently has two fields: - num_iters - number of actual iterations the benchmark have made - avg_latency_ms - average time it took to infer on one input example in milliseconds",0.95978725
tensorflow.python.keras.backend.repeat,"Repeats a 2D tensor. if `x` has shape (samples, dim) and `n` is `2`, the output will have shape `(samples, 2, dim)`. Args: x: Tensor or variable. n: Python integer, number of times to repeat. Returns: A tensor. Example: >>> b = tf.constant([[1, 2], [3, 4]]) >>> b <tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)> >>> tf.keras.backend.repeat(b, n=2) <tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[1, 2], [1, 2]], [[3, 4], [3, 4]]], dtype=int32)>",torch.functional.cartesian_prod,"Do cartesian product of the given sequence of tensors. The behavior is similar to python's `itertools.product`. Args: *tensors: any number of 1 dimensional tensors. Returns: Tensor: A tensor equivalent to converting all the input tensors into lists, do `itertools.product` on these lists, and finally convert the resulting list into tensor. Example:: >>> import itertools >>> a = [1, 2, 3] >>> b = [4, 5] >>> list(itertools.product(a, b)) [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)] >>> tensor_a = torch.tensor(a) >>> tensor_b = torch.tensor(b) >>> torch.cartesian_prod(tensor_a, tensor_b) tensor([[1, 4], [1, 5], [2, 4], [2, 5], [3, 4], [3, 5]])",0.9597788
tensorflow.python.ops.sparse_ops.sparse_softmax,"Applies softmax to a batched N-D `SparseTensor`. The inputs represent an N-D SparseTensor with logical shape `[..., B, C]` (where `N >= 2`), and with indices sorted in the canonical lexicographic order. This op is equivalent to applying the normal `tf.nn.softmax()` to each innermost logical submatrix with shape `[B, C]`, but with the catch that *the implicitly zero elements do not participate*. Specifically, the algorithm is equivalent to: (1) Applies `tf.nn.softmax()` to a densified view of each innermost submatrix with shape `[B, C]`, along the size-C dimension; (2) Masks out the original implicitly-zero locations; (3) Renormalizes the remaining elements. Hence, the `SparseTensor` result has exactly the same non-zero indices and shape. Example using a 3-D SparseTensor: >>> st = tf.sparse.from_dense( ... [[[0., np.e], ... [1., 0.]], ... ... [[np.e, 0.], ... [np.e, np.e]]]) >>> res = tf.sparse.softmax(st) >>> res.indices <tf.Tensor: shape=(5, 3), dtype=int64, numpy= array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [1, 1, 1]])> >>> res.values <tf.Tensor: ... numpy=array([1. , 1. , 1. , 0.5, 0.5], dtype=float32)> >>> res.dense_shape <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 2])> >>> tf.sparse.to_dense(res) <tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy= array([[[0. , 1. ], [1. , 0. ]], [[1. , 0. ], [0.5, 0.5]]], dtype=float32)> Args: sp_input: N-D `SparseTensor`, where `N >= 2`. name: optional name of the operation. Returns: output: N-D `SparseTensor` representing the results.",torch.functional.tensordot,"Returns a contraction of a and b over multiple dimensions. :attr:`tensordot` implements a generalized matrix product. Args: a (Tensor): Left tensor to contract b (Tensor): Right tensor to contract dims (int or Tuple[List[int], List[int]] or List[List[int]] containing two lists or Tensor): number of dimensions to contract or explicit lists of dimensions for :attr:`a` and :attr:`b` respectively When called with a non-negative integer argument :attr:`dims` = :math:`d`, and the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`, respectively, :func:`~torch.tensordot` computes .. math:: r_{i_0,...,i_{m-d}, i_d,...,i_n} = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}. When called with :attr:`dims` of the list form, the given dimensions will be contracted in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted dimensions. Examples:: >>> a = torch.arange(60.).reshape(3, 4, 5) >>> b = torch.arange(24.).reshape(4, 3, 2) >>> torch.tensordot(a, b, dims=([1, 0], [0, 1])) tensor([[4400., 4730.], [4532., 4874.], [4664., 5018.], [4796., 5162.], [4928., 5306.]]) >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) >>> a = torch.randn(3, 4, 5, device='cuda') >>> b = torch.randn(4, 5, 6, device='cuda') >>> c = torch.tensordot(a, b, dims=2).cpu() tensor([[ 8.3504, -2.5436, 6.2922, 2.7556, -1.0732, 3.2741], [ 3.3161, 0.0704, 5.0187, -0.4079, -4.3126, 4.8744], [ 0.8223, 3.9445, 3.2168, -0.2400, 3.4117, 1.7780]]) >>> a = torch.randn(3, 5, 4, 6) >>> b = torch.randn(6, 4, 5, 3) >>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0])) tensor([[ 7.7193, -2.4867, -10.3204], [ 1.5513, -14.4737, -6.5113], [ -0.2850, 4.2573, -3.5997]])",0.9597247
tensorflow.python.data.ops.dataset_ops.get_single_element,"Returns the single element of the `dataset`. The function enables you to use a `tf.data.Dataset` in a stateless ""tensor-in tensor-out"" expression, without creating an iterator. This facilitates the ease of data transformation on tensors using the optimized `tf.data.Dataset` abstraction on top of them. For example, lets consider a `preprocessing_fn` which would take as an input the raw features and returns the processed feature along with it's label. python def preprocessing_fn(raw_feature): # ... the raw_feature is preprocessed as per the use-case return feature raw_features = ... # input batch of BATCH_SIZE elements. dataset = (tf.data.Dataset.from_tensor_slices(raw_features) .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE) .batch(BATCH_SIZE)) processed_features = dataset.get_single_element() In the above example, the `raw_features` tensor of length=BATCH_SIZE was converted to a `tf.data.Dataset`. Next, each of the `raw_feature` was mapped using the `preprocessing_fn` and the processed features were grouped into a single batch. The final `dataset` contains only one element which is a batch of all the processed features. NOTE: The `dataset` should contain only one element. Now, instead of creating an iterator for the `dataset` and retrieving the batch of features, the `tf.data.get_single_element()` function is used to skip the iterator creation process and directly output the batch of features. This can be particularly useful when your tensor transformations are expressed as `tf.data.Dataset` operations, and you want to use those transformations while serving your model. #### Keras python model = ... # A pre-built or custom model class PreprocessingModel(tf.keras.Model): def __init__(self, model): super().__init__(self) self.model = model @tf.function(input_signature=[...]) def serving_fn(self, data): ds = tf.data.Dataset.from_tensor_slices(data) ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE) ds = ds.batch(batch_size=BATCH_SIZE) return tf.argmax(s",torch.utils.bundled_inputs.augment_many_model_functions_with_bundled_inputs,"Add bundled sample inputs to a model for an arbitrary list of public functions. Models with bundled inputs can be invoked in a uniform manner by benchmarking and code coverage tools. Augmented models will support the following methods: `get_all_bundled_inputs_for_<function_name>() -> List[Tuple[Any, ...]]` Returns a list of tuples suitable for passing to the model like `for inp in model.get_all_bundled_inputs_for_foo(): model.foo(*inp)` `get_bundled_inputs_functions_and_info() -> Dict[str, Dict[str: List[str]]]` Returns a dictionary mapping function names to a metadata dictionary. This nested dictionary maps preset strings like: 'get_inputs_function_name' -> the name of a function attribute in this model that can be run to get back a list of inputs corresponding to that function. 'info' -> the user provided extra information about the bundled inputs If forward has bundled inputs then these following functions are also defined: `get_all_bundled_inputs() -> List[Tuple[Any, ...]]` Returns a list of tuples suitable for passing to the model like `for inp in model.get_all_bundled_inputs(): model(*inp)` `get_num_bundled_inputs() -> int` Equivalent to `len(model.get_all_bundled_inputs())`, but slightly easier to call from C++. Inputs can be specified in one of two ways: - The model can define `_generate_bundled_inputs_for_<function_name>`. If the user chooses this method inputs[<function>] should map to None - The `inputs` argument to this function can be a dictionary mapping functions to a list of inputs, of the same form that will be returned by get_all_bundled_inputs_for_<function_name>. The type of the inputs is List[Tuple[Any, ...]]. The outer list corresponds with a list of inputs, the inner tuple is the list of args that together make up one input. For inputs of functions that take one arg, this will be a tuple of length one. The Any, ... is the actual data that makes up the args, e.g. a tensor. Info is an optional parameter that maps functions to a list of strings p",0.9597238
tensorflow.python.checkpoint.checkpoint.object_metadata,"Retrieves information about the objects in a checkpoint. Example usage: python object_graph = tf.contrib.checkpoint.object_metadata( tf.train.latest_checkpoint(checkpoint_directory)) ckpt_variable_names = set() for node in object_graph.nodes: for attribute in node.attributes: ckpt_variable_names.add(attribute.full_name) Args: save_path: The path to the checkpoint, as returned by `save` or `tf.train.latest_checkpoint`. Returns: A parsed `tf.contrib.checkpoint.TrackableObjectGraph` protocol buffer. Raises: ValueError: If an object graph was not found in the checkpoint.",torch.nn.modules.utils.consume_prefix_in_state_dict_if_present,"Strip the prefix in state_dict in place, if any. ..note:: Given a `state_dict` from a DP/DDP model, a local model can load it by applying `consume_prefix_in_state_dict_if_present(state_dict, ""module."")` before calling :meth:`torch.nn.Module.load_state_dict`. Args: state_dict (OrderedDict): a state-dict to be loaded to the model. prefix (str): prefix.",0.95971197
tensorflow.python.training.session_run_hook.original_args,"A `SessionRunArgs` object holding the original arguments of `run()`. If user called `MonitoredSession.run(fetches=a, feed_dict=b)`, then this field is equal to SessionRunArgs(a, b). Returns: A `SessionRunArgs` object",torch.distributed.checkpoint.stateful.state_dict,"Objects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in `load_state_dict()`. .. warning:: Because of the inplace nature of restoring a checkpoint, this function is also called during `torch.distributed.checkpoint.load`. Returns: Dict: The objects state dict",0.9597078
tensorflow.python.tpu.tpu_system_metadata.master_job,"Returns the canonical job name to use to place TPU computations on. Args: master: A `string` representing the TensorFlow master to use. cluster_def: A ClusterDef object describing the TPU cluster. Returns: A string containing the job name, or None if no job should be specified. Raises: ValueError: If the user needs to specify a tpu_job_name, because we are unable to infer the job name automatically, or if the user-specified job names are inappropriate.",torch.distributed.distributed_c10d.irecv,"Receives a tensor asynchronously. .. warning:: ``tag`` is not supported with the NCCL backend. Args: tensor (Tensor): Tensor to fill with received data. src (int, optional): Source rank on global process group (regardless of ``group`` argument). Will receive from any process if unspecified. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. tag (int, optional): Tag to match recv with remote send Returns: A distributed request object. None, if not part of the group",0.95969856
tensorflow.python.debug.wrappers.dumping_wrapper.prepare_run_debug_urls,Implementation of abstract method in superclass. See doc of `NonInteractiveDebugWrapperSession.prepare_run_debug_urls()` for details. This implementation creates a run-specific subdirectory under self._session_root and stores information regarding run `fetches` and `feed_dict.keys()` in the subdirectory. Args: fetches: Same as the `fetches` argument to `Session.run()` feed_dict: Same as the `feed_dict` argument to `Session.run()` Returns: debug_urls: (`str` or `list` of `str`) file:// debug URLs to be used in this `Session.run()` call.,torch.optim.optimizer.register_state_dict_pre_hook,"Register a state dict pre-hook which will be called before :meth:`~torch.optim.Optimizer.state_dict` is called. It should have the following signature:: hook(optimizer) -> None The ``optimizer`` argument is the optimizer instance being used. The hook will be called with argument ``self`` before calling ``state_dict`` on ``self``. The registered hook can be used to perform pre-processing before the ``state_dict`` call is made. Args: hook (Callable): The user defined hook to be registered. prepend (bool): If True, the provided pre ``hook`` will be fired before all the already registered pre-hooks on ``state_dict``. Otherwise, the provided ``hook`` will be fired after all the already registered pre-hooks. (default: False) Returns: :class:`torch.utils.hooks.RemoveableHandle`: a handle that can be used to remove the added hook by calling ``handle.remove()``",0.95968276
tensorflow.python.training.session_manager.wait_for_session,"Creates a new `Session` and waits for model to be ready. Creates a new `Session` on 'master'. Waits for the model to be initialized or recovered from a checkpoint. It's expected that another thread or process will make the model ready, and that this is intended to be used by threads/processes that participate in a distributed training configuration where a different thread/process is responsible for initializing or recovering the model being trained. NB: The amount of time this method waits for the session is bounded by max_wait_secs. By default, this function will wait indefinitely. Args: master: `String` representation of the TensorFlow master to use. config: Optional ConfigProto proto used to configure the session. max_wait_secs: Maximum time to wait for the session to become available. Returns: A `Session`. May be None if the operation exceeds the timeout specified by config.operation_timeout_in_ms. Raises: tf.DeadlineExceededError: if the session is not available after max_wait_secs.",torch.distributed.elastic.multiprocessing.api.wait,"Wait for the specified ``timeout`` seconds, polling every ``period`` seconds for the processes to be done. Returns ``None`` if the processes are still running on timeout expiry. Negative timeout values are interpreted as ""wait-forever"". A timeout value of zero simply queries the status of the processes (e.g. equivalent to a poll). ..note: Multiprocessing library registers SIGTERM and SIGINT signal handlers that raise ``SignalException`` when the signals received. It is up to the consumer of the code to properly handle the exception. It is important not to swallow the exception otherwise the process would not terminate. Example of the typical workflow can be: .. code-block:: python pc = start_processes(...) try: pc.wait(1) .. do some other work except SignalException as e: pc.shutdown(e.sigval, timeout=30) If SIGTERM or SIGINT occurs, the code above will try to shutdown child processes by propagating received signal. If child processes will not terminate in the timeout time, the process will send the SIGKILL.",0.9596201
tensorflow.python.tpu.tpu.initialize_system,"Initializes a distributed TPU system for use with TensorFlow. Args: embedding_config: If not None, a `TPUEmbeddingConfiguration` proto describing the desired configuration of the hardware embedding lookup tables. If embedding_config is None, no hardware embeddings can be used. job: The job (the XXX in TensorFlow device specification /job:XXX) that contains the TPU devices that will be initialized. If job=None it is assumed there is only one job in the TensorFlow flock, and an error will be returned if this assumption does not hold. compilation_failure_closes_chips: Set the configuration whether we want to close TPU chips when there is a compilation failure. tpu_cancellation_closes_chips: Set the configuration whether we want to close TPU chips when a TPU execution is cancelled. If the value is None, the behavior will be determined by the command line flag `tpu_cancellation_closes_chips` for the TPU worker. WARNING: this argument only applies to TFRT TPU runtime. Returns: A serialized `TopologyProto` that describes the TPU system. Note: the topology must be evaluated using `Session.run` before it can be used.",torch._lazy.closure.add_step_closure,"Adds a closure to the list of the ones to be run at the end of the step. Many times during model training there is the need to print/report (print to console, post to tensorboard, etc...) information which require the content of intermediary tensors to be inspected. Inspecting different tensors content in different points of the model code requires many executions and typically causes performance issues. Adding a step closure will ensure that it will be run after the barrier, when all the live tensors will be already materialized to device data. Live tensors which will include the ones captured by the closure arguments. So using `add_step_closure()` will ensure a single execution will be performed, even when multiple closures are queued, requiring multiple tensors to be inspected. Step closures will be run sequentially in the order they have been queued. Note that even though using this API the execution will be optimized, it is advised to throttle the printing/reporting events once every N steps. Args: closure (callable): The function to be called. args (tuple): The arguments to be passed to the closure. run_async: If True, run the closure asynchronously.",0.9595926
tensorflow.python.tpu.tpu_embedding_v3.embedding_tables,"Returns a dict of embedding tables, keyed by `TableConfig`.",torch.ao.quantization.backend_config.backend_config.configs,Return a copy of the list of configs set in this `BackendConfig`.,0.9595747
tensorflow.python.keras.utils.tf_utils.register_symbolic_tensor_type,"Allows users to specify types regarded as symbolic `Tensor`s. Used in conjunction with `tf.register_tensor_conversion_function`, calling `tf.keras.__internal__.utils.register_symbolic_tensor_type(cls)` allows non-`Tensor` objects to be plumbed through Keras layers. Example: python # One-time setup. class Foo(object): def __init__(self, input_): self._input = input_ def value(self): return tf.constant(42.) tf.register_tensor_conversion_function( Foo, lambda x, *args, **kwargs: x.value()) tf.keras.__internal__.utils.register_symbolic_tensor_type(Foo) # User-land. layer = tf.keras.layers.Lambda(lambda input_: Foo(input_)) Args: cls: A `class` type which shall be regarded as a symbolic `Tensor`.",torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper,"Cast input tensor to ``torch.float16``, cast result of hook back to input dtype. This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (``torch.float16``), and casts the resulting tensor of the given hook back to the input data type, such as ``float32``. Therefore, ``fp16_compress_hook`` is equivalent to ``fp16_compress_wrapper(allreduce_hook)``. Example:: >>> # xdoctest: +SKIP >>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10) >>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))",0.9595227
tensorflow.python.keras.saving.model_config.model_from_json,"Parses a JSON model configuration string and returns a model instance. Usage: >>> model = tf.keras.Sequential([ ... tf.keras.layers.Dense(5, input_shape=(3,)), ... tf.keras.layers.Softmax()]) >>> config = model.to_json() >>> loaded_model = tf.keras.models.model_from_json(config) Args: json_string: JSON string encoding a model configuration. custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization. Returns: A Keras model instance (uncompiled).",torch.ao.pruning._experimental.data_sparsifier.benchmarks.evaluate_disk_savings.save_model_states,"Dumps the state_dict() of the model. Args: state_dict (Dict) The state_dict() as dumped by dlrm_s_pytorch.py. Only the model state will be extracted from this dictionary. This corresponds to the 'state_dict' key in the state_dict dictionary. >>> model_state = state_dict['state_dict'] save_file_name (str) The filename (not path) when saving the model state dictionary sparse_block_shape (Tuple) The block shape corresponding to the data norm sparsifier. **Used for creating save directory** norm (str) type of norm (L1, L2) for the datanorm sparsifier. **Used for creating save directory** zip (bool) if True, the file is zip-compressed.",0.959522
tensorflow.python.eager.context.get_c_function,Get a C API TF_Function from the context. Args: name: Name of the function to get. Returns: A ScopedTFFunction wrapping the C API TF_Function.,torch.testing._internal.distributed.rpc.dist_optimizer_test.rpc_async_method,"Call rpc.rpc_async on a method in a remote object. Args: method: the method (for example, Class.method) obj_rref (RRef): remote reference to the object args: positional arguments to pass to the method kwargs: keyword arguments to pass to the method Returns a Future to the method call result.",0.9594792
tensorflow.python.checkpoint.checkpoint_management.save,"Creates a new checkpoint and manages it. Args: checkpoint_number: An optional integer, or an integer-dtype `Variable` or `Tensor`, used to number the checkpoint. If `None` (default), checkpoints are numbered using `checkpoint.save_counter`. Even if `checkpoint_number` is provided, `save_counter` is still incremented. A user-provided `checkpoint_number` is not incremented even if it is a `Variable`. check_interval: An optional boolean. The argument is only effective when `checkpoint_interval` is passed into the manager. If `True`, the manager will only save the checkpoint if the interval between checkpoints is larger than `checkpoint_interval`. Otherwise it will always save the checkpoint unless a checkpoint has already been saved for the current step. options: Optional `tf.train.CheckpointOptions` object. This argument only works with TF2 checkpoint objects. For example, options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost') Returns: The path to the new checkpoint. It is also recorded in the `checkpoints` and `latest_checkpoint` properties. `None` if no checkpoint is saved.",torch.distributed.algorithms._checkpoint.checkpoint_wrapper.checkpoint_wrapper,"Wrap a module for activation checkpointing. If the module is wrapped with this function, all subsequent calls to the module will, automatically perform checkpointing without the user having to explicitly call ``checkpoint`` function. Usage:: checkpointed_module = checkpoint_wrapper(module) outputs = checkpointed_module(inputs) Args: module (nn.Module): The module to be wrapped checkpoint_impl (Optional[CheckpointImpl]): The checkpointing implementation to use. Note that this will only be passed into the ``torch.utils.checkpoint.checkpoint`` implementation, and is ignored if a custom ``checkpoint_fn`` is specified. Note that for implementations using reentrant checkpoint from ``torch.utils.checkpoint``, keyword arguments will only be supported if ``checkpoint_impl`` is passed as ``CheckpointImpl.REENTRANT`. checkpoint_fn (Optional[Callable]): Functional checkpoint implementation to use. If this is specified, it will be used over the default ``torch.utils.checkpoint.checkpoint`` implementation and the `checkpoint_impl` argument will be ignored. **checkpoint_fn_kwargs: (Dict[str, Any]): Keyword arguments to pass into `checkpoint_fn`. Returns: (nn.Module): Wrapped module",0.95947635
tensorflow.python.keras.backend.separable_conv1d,"1D convolution with separable filters. Args: x: input tensor depthwise_kernel: convolution kernel for the depthwise convolution. pointwise_kernel: kernel for the 1x1 convolution. strides: stride integer. padding: string, `""same""` or `""valid""`. data_format: string, `""channels_last""` or `""channels_first""`. dilation_rate: integer dilation rate. Returns: Output tensor. Raises: ValueError: if `data_format` is neither `channels_last` or `channels_first`.",torch.nn.utils.clip_grad.clip_grad_value_,"Clip the gradients of an iterable of parameters at specified value. Gradients are modified in-place. Args: parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (float): maximum allowed value of the gradients. The gradients are clipped in the range :math:`\left[\text{-clip\_value}, \text{clip\_value}\right]` foreach (bool): use the faster foreach-based implementation If ``None``, use the foreach implementation for CUDA and CPU native tensors and silently fall back to the slow implementation for other device types. Default: ``None``",0.95945203
tensorflow.python.data.ops.iterator_ops.make_initializer,Returns a `tf.Operation` that initializes this iterator on `dataset`. Args: dataset: A `Dataset` whose `element_spec` if compatible with this iterator. name: (Optional.) A name for the created operation. Returns: A `tf.Operation` that can be run to initialize this iterator on the given `dataset`. Raises: TypeError: If `dataset` and this iterator do not have a compatible `element_spec`.,torch.distributed.fsdp.fully_sharded_data_parallel.get_state_dict_type,Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at ``module``. The target module does not have to be an FSDP module. Returns: A ``StateDictSettings`` containing the state_dict_type and state_dict / optim_state_dict configs that are currently set. Raises: ``AssertionError`` if the ``StateDictSettings`` for different FSDP submodules differ.,0.9594146
tensorflow.python.ops.random_ops_util.get_key_counter_alg,"Calculates the key, counter and algorithm to pass to raw RNG ops. This function calculates the key and counter, and determines the algorithm that will be passed to the raw RNG ops like `StatelessRandomUniformV2`. Depending on the input `alg`, the key and counter may be scrambled or copied from `seed`. If `alg` is `""auto_select""`, the key and counter will be determined at runtime based on device type. Args: seed: An integer tensor of shape [2]. The seed to calculate the key and counter from. alg: The RNG algorithm. See `tf.random.stateless_uniform` for an explanation. Returns: A pair (key, counter, algorithm) suitable for V2 stateless RNG ops like `StatelessRandomUniformV2`.",torch._functorch.compilers.memory_efficient_fusion,"Wrapper function over :func:`aot_function` and :func:`aot_module` to perform memory efficient fusion. It uses the :func:`min_cut_rematerialization_partition` partitioner to perform efficient recomputation. It uses NVFuser to compile the generated forward and backward graphs. .. warning:: This API is experimental and likely to change. Args: fn (Union[Callable, nn.Module]): A Python function or a ``nn.Module`` that takes one ore more arguments. Must return one or more Tensors. **kwargs: Any other overrides you want to make to the settings Returns: Returns a ``Callable`` or ``nn.Module`` that retains the eager behavior of the original :attr:`fn`, but whose forward and backward graphs have gone through recomputation optimizations, and the graphs have been compiled with nvfuser.",0.95940167
tensorflow.python.eager.remote.connect_to_cluster,"Connects to the given cluster. Will make devices on the cluster available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices. If the given local job name is not present in the cluster specification, it will be automatically added, using an unused port on the localhost. Device filters can be specified to isolate groups of remote tasks to avoid undesired accesses between workers. Workers accessing resources or launching ops / functions on filtered remote devices will result in errors (unknown devices). For any remote task, if no device filter is present, all cluster devices will be visible; if any device filter is specified, it can only see devices matching at least one filter. Devices on the task itself are always visible. Device filters can be particially specified. For example, for a cluster set up for parameter server training, the following device filters might be specified: python cdf = tf.config.experimental.ClusterDeviceFilters() # For any worker, only the devices on PS nodes and itself are visible for i in range(num_workers): cdf.set_device_filters('worker', i, ['/job:ps']) # Similarly for any ps, only the devices on workers and itself are visible for i in range(num_ps): cdf.set_device_filters('ps', i, ['/job:worker']) tf.config.experimental_connect_to_cluster(cluster_def, cluster_device_filters=cdf) Args: cluster_spec_or_resolver: A `ClusterSpec` or `ClusterResolver` describing the cluster. job_name: The name of the local job. task_index: The local task index. protocol: The communication protocol, such as `""grpc""`. If unspecified, will use the default from `python/platform/remote_utils.py`. make_master_device_default: If True and a cluster resolver is passed, will automatically enter the master task device scope, which indicates the master becomes the default device to run ops. It won't do anything if a cluster spec is passed. Will throw an error if the caller is currently already in some",torch.distributed.distributed_c10d.all_gather_object,"Gathers picklable objects from the whole group into a list. Similar to :func:`all_gather`, but Python objects can be passed in. Note that the object must be picklable in order to be gathered. Args: object_list (list[Any]): Output list. It should be correctly sized as the size of the group for this collective and will contain the output. obj (Any): Pickable Python object to be broadcast from current process. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is ``None``. Returns: None. If the calling rank is part of this group, the output of the collective will be populated into the input ``object_list``. If the calling rank is not part of the group, the passed in ``object_list`` will be unmodified. .. note:: Note that this API differs slightly from the :func:`all_gather` collective since it does not provide an ``async_op`` handle and thus will be a blocking call. .. note:: For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by ``torch.cuda.current_device()`` and it is the user's responsiblity to ensure that this is set so that each rank has an individual GPU, via ``torch.cuda.set_device()``. .. warning:: :func:`all_gather_object` uses ``pickle`` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. .. warning:: Calling :func:`all_gather_object` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using :func:`all_gather` instead. Example:: >>> # xdoctest: +SKIP(""need process group init"") >>> # Note: Process group initialization omitted on each rank. >>> import torch.distributed as dist >>> # Assumes world_size of 3. >>> gather_objects = [""foo"", 12, {",0.95939887
tensorflow.python.ops.distributions.distribution.param_static_shapes,param_shapes with static (i.e. `TensorShape`) shapes. This is a class method that describes what key/value arguments are required to instantiate the given `Distribution` so that a particular shape is returned for that instance's call to `sample()`. Assumes that the sample's shape is known statically. Subclasses should override class method `_param_shapes` to return constant-valued tensors when constant values are fed. Args: sample_shape: `TensorShape` or python list/tuple. Desired shape of a call to `sample()`. Returns: `dict` of parameter name to `TensorShape`. Raises: ValueError: if `sample_shape` is a `TensorShape` and is not fully defined.,torch.nn.utils._expanded_weights.expanded_weights_utils.forward_helper,Compute the forward pass for a function that has expanded weight(s) passed to it. It will run the forward pass where all ExpandedWeights are their original weight. It runs checks on the given arguments and detaches the outputs. .. note:: First argument in :attr:`expanded_args` must be the input with the batch dimension as the first element of the shape .. note:: :attr:`func` must return a Tensor or tuple of Tensors Args: func: The function to be called expanded_args: Arguments to be passed to :attr:`func`. Will include arguments that need to be unpacked because they are ExpandedWeights expanded_kwargs: Keyword arguments to be passed to :attr:`func`. Similar to :attr:`expanded_args`.,0.95939195
tensorflow.python.ops.ragged.ragged_array_ops.ragged_shape,Returns the shape of a RaggedTensor. Args: input: A `RaggedTensor` name: A name for the operation (optional). out_type: dtype used to encode the shape. Returns: A `tf.experimental.DynamicRaggedShape`,torch.ao.quantization.qconfig.get_default_qat_qconfig,"Returns the default QAT qconfig for the specified backend. Args: * `backend` (str): a string representing the target backend. Currently supports `x86` (default), `fbgemm`, `qnnpack` and `onednn`. * `version`: version, for backwards compatibility. Can be `None` or `1`. Return: qconfig",0.95938617
tensorflow.python.feature_column.feature_column_v2.get_sparse_tensors,"Returns an IdWeightPair. `IdWeightPair` is a pair of `SparseTensor`s which represents ids and weights. `IdWeightPair.id_tensor` is typically a `batch_size` x `num_buckets` `SparseTensor` of `int64`. `IdWeightPair.weight_tensor` is either a `SparseTensor` of `float` or `None` to indicate all weights should be taken to be 1. If specified, `weight_tensor` must have exactly the same shape and indices as `sp_ids`. Expected `SparseTensor` is same as parsing output of a `VarLenFeature` which is a ragged matrix. Args: transformation_cache: A `FeatureTransformationCache` object to access features. state_manager: A `StateManager` to create / access resources such as lookup tables.",torch.distributed.checkpoint.planner_helpers.create_read_items_for_chunk_list,Create a list of ``ReadItem`` based on the checkpoint and local chunks. This applies the resharding algorithm and computes the reads needed to satisfy ``local_chunks`` with a checkpoint described by ``checkpoint_md``. Args: fqn (str) : The state_dict FQN to pass to ``ReadItem``. checkpoint_md (TensorStorageMetadata): metadata for a given tensor from a checkpoint. local_chunks (List[ChunkStorageMetadata]): Local chunks that needs to be loaded. Returns: A list of ``ReadItem`` that will satisfy all input chunks.,0.95934635
tensorflow.python.eager.polymorphic_function.function_type_utils.to_input_signature,Extracts an input_signature from function_type instance.,torch._inductor.kernel.flex_attention.create_placeholder,Creates a placeholder input buffers for producing subgraph_output.,0.9592665
tensorflow.python.ops.math_ops.reduce_any_v1,"Computes `tf.math.logical_or` of elements across dimensions of a tensor. This is the reduction operation for the elementwise `tf.math.logical_or` op. Reduces `input_tensor` along the dimensions given in `axis`. Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each of the entries in `axis`, which must be unique. If `keepdims` is true, the reduced dimensions are retained with length 1. If `axis` is None, all dimensions are reduced, and a tensor with a single element is returned. For example: >>> x = tf.constant([[True, True], [False, False]]) >>> tf.reduce_any(x) <tf.Tensor: shape=(), dtype=bool, numpy=True> >>> tf.reduce_any(x, 0) <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, True])> >>> tf.reduce_any(x, 1) <tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])> Args: input_tensor: The boolean tensor to reduce. axis: The dimensions to reduce. If `None` (the default), reduces all dimensions. Must be in the range `[-rank(input_tensor), rank(input_tensor))`. keepdims: If true, retains reduced dimensions with length 1. name: A name for the operation (optional). reduction_indices: The old (deprecated) name for axis. keep_dims: Deprecated alias for `keepdims`. Returns: The reduced tensor. @compatibility(numpy) Equivalent to np.any @end_compatibility",torch.nn.modules.sparse.from_pretrained,"Create Embedding instance from given 2-dimensional FloatTensor. Args: embeddings (Tensor): FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``. freeze (bool, optional): If ``True``, the tensor does not get updated in the learning process. Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True`` padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient; therefore, the embedding vector at :attr:`padding_idx` is not updated during training, i.e. it remains as a fixed ""pad"". max_norm (float, optional): See module initialization documentation. norm_type (float, optional): See module initialization documentation. Default ``2``. scale_grad_by_freq (bool, optional): See module initialization documentation. Default ``False``. sparse (bool, optional): See module initialization documentation. Examples:: >>> # FloatTensor containing pretrained weights >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) >>> embedding = nn.Embedding.from_pretrained(weight) >>> # Get embeddings for index 1 >>> input = torch.LongTensor([1]) >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> embedding(input) tensor([[ 4.0000, 5.1000, 6.3000]])",0.9592625
tensorflow.python.data.ops.dataset_ops.ignore_errors,"Drops elements that cause errors. >>> dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.]) >>> dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, """")) >>> list(dataset.as_numpy_iterator()) Traceback (most recent call last): ... InvalidArgumentError: ... Tensor had Inf values >>> dataset = dataset.ignore_errors() >>> list(dataset.as_numpy_iterator()) [1.0, 0.5, 0.25] Args: log_warning: (Optional.) A bool indicating whether or not ignored errors should be logged to stderr. Defaults to `False`. name: (Optional.) A string indicating a name for the `tf.data` operation. Returns: A new `Dataset` with the transformation applied as described above.",torch.nn.utils.prune.is_pruned,"Check if a module is pruned by looking for pruning pre-hooks. Check whether ``module`` is pruned by looking for ``forward_pre_hooks`` in its modules that inherit from the :class:`BasePruningMethod`. Args: module (nn.Module): object that is either pruned or unpruned Returns: binary answer to whether ``module`` is pruned. Examples: >>> from torch.nn.utils import prune >>> m = nn.Linear(5, 7) >>> print(prune.is_pruned(m)) False >>> prune.random_unstructured(m, name='weight', amount=0.2) >>> print(prune.is_pruned(m)) True",0.95926
tensorflow.python.keras.utils.conv_utils.conv_connected_inputs,"Return locations of the input connected to an output position. Assume a convolution with given parameters is applied to an input having N spatial dimensions with `input_shape = (d_in1, ..., d_inN)`. This method returns N ranges specifying the input region that was convolved with the kernel to produce the output at position `output_position = (p_out1, ..., p_outN)`. Example: >>> input_shape = (4, 4) >>> kernel_shape = (2, 1) >>> output_position = (1, 1) >>> strides = (1, 1) >>> padding = ""valid"" >>> conv_connected_inputs(input_shape, kernel_shape, output_position, ... strides, padding) [range(1, 3), range(1, 2)] Args: input_shape: tuple of size N: `(d_in1, ..., d_inN)`, spatial shape of the input. kernel_shape: tuple of size N, spatial shape of the convolutional kernel / receptive field. output_position: tuple of size N: `(p_out1, ..., p_outN)`, a single position in the output of the convolution. strides: tuple of size N, strides along each spatial dimension. padding: type of padding, string `""same""` or `""valid""`. `""valid""` means no padding. `""same""` results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. Returns: N ranges `[[p_in_left1, ..., p_in_right1], ..., [p_in_leftN, ..., p_in_rightN]]` specifying the region in the input connected to output_position.",torch.nn.functional.fractional_max_pool2d_with_indices,"fractional_max_pool2d(input, kernel_size, output_size=None, output_ratio=None, return_indices=False, _random_samples=None) Applies 2D fractional max pooling over an input signal composed of several input planes. Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes. Args: kernel_size: the size of the window to take a max over. Can be a single number :math:`k` (for a square kernel of :math:`k \times k`) or a tuple `(kH, kW)` output_size: the target output size of the image of the form :math:`oH \times oW`. Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH` output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1) return_indices: if ``True``, will return the indices along with the outputs. Useful to pass to :func:`~torch.nn.functional.max_unpool2d`. Examples:: >>> input = torch.randn(20, 16, 50, 32) >>> # pool of square window of size=3, and target output size 13x12 >>> F.fractional_max_pool2d(input, 3, output_size=(13, 12)) >>> # pool of square window and target output size being half of input image size >>> F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5)) .. _Fractional MaxPooling: http://arxiv.org/abs/1412.6071",0.9592476
tensorflow.python.ops.stateful_random_ops.set_global_generator,"Replaces the global generator with another `Generator` object. This function replaces the global generator with the provided `generator` object. A random number generator utilizes a `tf.Variable` object to store its state. The user shall be aware of caveats how `set_global_generator` interacts with `tf.function`: - tf.function puts restrictions on Variable creation thus one cannot freely create a new random generator instance inside `tf.function`. To call `set_global_generator` inside `tf.function`, the generator instance must have already been created eagerly. - tf.function captures the Variable during trace-compilation, thus a compiled f.function will not be affected `set_global_generator` as demonstrated by random_test.py/RandomTest.testResetGlobalGeneratorBadWithDefun . For most use cases, avoid calling `set_global_generator` after program initialization, and prefer to reset the state of the existing global generator instead, such as, >>> rng = tf.random.get_global_generator() >>> rng.reset_from_seed(30) Args: generator: the new `Generator` object.",torch.distributed.optim.named_optimizer.load_state_dict,"Define the default behavior to load a state_dict for ``_NamedOptimizer``. Sample Code my_model = MyModule() optimizer = _NamedOptimizer(my_model.named_parameters(), Adagrad) ... optim_state_dict = optimizer.state_dict() ... ... optimizer.load_state_dict(optim_state_dict) ... Args: state_dict (Dict[str, Any]) : A ``state_dict`` to load into the optimizer. Note that this state dict update is performed in place. .. note:: PyTorch is using lazy init to initialize the optim states. So it is possible that there is no optim state when user call ``load_state_dict`` and for ``_NamedOptimizer`` we make it stricter that users can only call ``load_state_dict`` after the state is initialized. By doing this, we can validate the optim ``state_dict`` to be loaded.",0.95924735
tensorflow.python.keras.optimizer_v2.adagrad.from_config,"Creates an optimizer from its config. This method is the reverse of `get_config`, capable of instantiating the same optimizer from the config dictionary. Args: config: A Python dictionary, typically the output of get_config. custom_objects: A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter. Returns: An optimizer instance.",torch.fx.graph.python_code,"Turn this ``Graph`` into valid Python code. Args: root_module (str): The name of the root module on which to look-up qualified name targets. This is usually 'self'. Returns: A PythonCode object, consisting of two fields: src: the Python source code representing the object globals: a dictionary of global names in `src` -> the objects that they reference.",0.9592437
tensorflow.python.debug.lib.debug_data.node_traceback,Try to retrieve the Python traceback of node's construction. Args: element_name: (`str`) Name of a graph element (node or tensor). Returns: (list) The traceback list object as returned by the `extract_trace` method of Python's traceback module. Raises: LookupError: If Python graph is not available for traceback lookup. KeyError: If the node cannot be found in the Python graph loaded.,torch.fx.passes.annotate_getitem_nodes.annotate_getitem_nodes,"Annotate the type of getitem nodes, inferred from the type of sequence node. If sequence node is not annotated with a type, do nothing. Currently support getitem nodes from Tuple, List, and NamedTuple sequence node. This is helpful since annotations on local names within function are lost during FX transforms. Adding back known type annotation for getitem nodes to improve jit scriptability. Args: graph (Graph): The graph to be annotated",0.95924276
tensorflow.python.tpu.tpu_feed.generate_dequeue_op,"Generate TPU dequeue ops. Args: tpu_device: The TPU device ordinal where the infeed instruction should be placed. Returns: A list of Outputs corresponding to a partition of infeed dequeued into XLA, suitable for use within a replicated block. Raises: ValueError: if the types or shapes of the tuple elements have not been set; or if a dequeue op has already been generated.",torch.fx.graph.find_nodes,"Allows for fast query of nodes Args: op (str): the name of the operation target (Optional[Target]): the target of the node. For call_function, the target is required. For other ops, the target is optional. sort (bool): whether to return nodes in the order they appear on on the graph. Returns: Iteratable of nodes with the requested op and target.",0.95918304
tensorflow.python.framework.ops.experimental_set_type,"Sets the corresponding node's `experimental_type` field. See the description of `NodeDef.experimental_type` for more info. Args: type_proto: A FullTypeDef proto message. The root type_if of this object must be `TFT_PRODUCT`, even for ops which only have a singlre return value.",torch.onnx.utils.unregister_custom_op_symbolic,"Unregisters ``symbolic_name``. See ""Custom Operators"" in the module documentation for an example usage. Args: symbolic_name (str): The name of the custom operator in ""<domain>::<op>"" format. opset_version (int): The ONNX opset version in which to unregister.",0.95917463
tensorflow.python.data.experimental.ops.writers.write,"Writes a dataset to a TFRecord file. An operation that writes the content of the specified dataset to the file specified in the constructor. If the file exists, it will be overwritten. Args: dataset: a `tf.data.Dataset` whose elements are to be written to a file Returns: In graph mode, this returns an operation which when executed performs the write. In eager mode, the write is performed by the method itself and there is no return value. Raises TypeError: if `dataset` is not a `tf.data.Dataset`. TypeError: if the elements produced by the dataset are not scalar strings.",torch.nn.modules.module.get_buffer,"Return the buffer given by ``target`` if it exists, otherwise throw an error. See the docstring for ``get_submodule`` for a more detailed explanation of this method's functionality as well as how to correctly specify ``target``. Args: target: The fully-qualified string name of the buffer to look for. (See ``get_submodule`` for how to specify a fully-qualified string.) Returns: torch.Tensor: The buffer referenced by ``target`` Raises: AttributeError: If the target string references an invalid path or resolves to something that is not a buffer",0.95913935
tensorflow.python.checkpoint.graph_view.attached_dependencies,"Returns list of dependencies that should be saved in the checkpoint. These dependencies are not tracked by root, but are in the checkpoint. This is defined when the user creates a Checkpoint with both root and kwargs set. Returns: A list of TrackableReferences.",torch.onnx._internal.fx.onnxfunction_dispatcher.get_function_overloads,"Get the function overloads from the registry. Args: node: The node to get the function overloads for. diagnostic_context: The diagnostic context to use for reporting errors. Returns: The list contains ONNXFunctions, starting with the default ones and followed by any custom ones.",0.9590753
tensorflow.python.debug.lib.debug_events_reader.device_name_map,Get a map mapping device IDs to device names.,torch.fx.experimental.accelerator_partitioner.get_logical_id_to_device,Get a mapping from device logical ID to Device object.,0.95907104
tensorflow.python.tpu.ops.tpu_ops.send_tpu_embedding_gradients,"A placeholder op for feeding per-sample gradients to the embedding layer. Args: inputs: A TensorList of gradients with which to update embedding tables. This argument has the same length and shapes as the return value of RecvTPUEmbeddingActivations, but contains gradients of the model's loss with respect to the embedding activations. The embedding tables are updated from these gradients via the optimizers specified in the TPU embedding configuration given to tpu.initialize_system. config: Serialized TPUEmbeddingConfiguration proto. learning_rates: A TensorList of float32 scalars, one for each dynamic learning rate tag: see the comments in //third_party/tensorflow/core/protobuf/tpu/ optimization_parameters.proto. Multiple tables can share the same dynamic learning rate tag as specified in the configuration. If the learning rates for all tables are constant, this list should be empty. name: A name for the operation (optional). Returns: A SendTPUEmbeddingGradients operation.",torch.nn.utils.parametrize.cached,"Context manager that enables the caching system within parametrizations registered with :func:`register_parametrization`. The value of the parametrized objects is computed and cached the first time they are required when this context manager is active. The cached values are discarded when leaving the context manager. This is useful when using a parametrized parameter more than once in the forward pass. An example of this is when parametrizing the recurrent kernel of an RNN or when sharing weights. The simplest way to activate the cache is by wrapping the forward pass of the neural network .. code-block:: python import torch.nn.utils.parametrize as P ... with P.cached(): output = model(inputs) in training and evaluation. One may also wrap the parts of the modules that use several times the parametrized tensors. For example, the loop of an RNN with a parametrized recurrent kernel: .. code-block:: python with P.cached(): for x in xs: out_rnn = self.rnn_cell(x, out_rnn)",0.9590409
tensorflow.python.ops.losses.losses_impl.huber_loss,"Adds a [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) term to the training procedure. For each value x in `error=labels-predictions`, the following is calculated: 0.5 * x^2 if |x| <= d 0.5 * d^2 + d * (|x| - d) if |x| > d where d is `delta`. `weights` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `weights` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `weights` vector. If the shape of `weights` matches the shape of `predictions`, then the loss of each measurable element of `predictions` is scaled by the corresponding value of `weights`. Args: labels: The ground truth output tensor, same dimensions as 'predictions'. predictions: The predicted outputs. weights: Optional `Tensor` whose rank is either 0, or the same rank as `labels`, and must be broadcastable to `labels` (i.e., all dimensions must be either `1`, or the same as the corresponding `losses` dimension). delta: `float`, the point where the huber loss function changes from a quadratic to linear. scope: The scope for the operations performed in computing the loss. loss_collection: collection to which the loss will be added. reduction: Type of reduction to apply to loss. Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same shape as `labels`; otherwise, it is scalar. Raises: ValueError: If the shape of `predictions` doesn't match that of `labels` or if the shape of `weights` is invalid. Also if `labels` or `predictions` is None. @compatibility(eager) The `loss_collection` argument is ignored when executing eagerly. Consider holding on to the return value or collecting losses via a `tf.keras.Model`. @end_compatibility",torch.nn.functional.binary_cross_entropy_with_logits,"Calculate Binary Cross Entropy between target and input logits. See :class:`~torch.nn.BCEWithLogitsLoss` for details. Args: input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits). target: Tensor of the same shape as input with values between 0 and 1 weight (Tensor, optional): a manual rescaling weight if provided it's repeated to match input tensor shape size_average (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field :attr:`size_average` is set to ``False``, the losses are instead summed for each minibatch. Ignored when reduce is ``False``. Default: ``True`` reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per batch element instead and ignores :attr:`size_average`. Default: ``True`` reduction (str, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied, ``'mean'``: the sum of the output will be divided by the number of elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average` and :attr:`reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override :attr:`reduction`. Default: ``'mean'`` pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target. Must be a tensor with equal size along the class dimension to the number of classes. Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of size [B, C, H, W] will apply different pos_weights to each element of the batch or [C, H, W] the same pos_weights acro",0.95899314
tensorflow.python.ops.collective_ops.all_gather,"Accumulates tensors collectively, across devices, along first dimension. Args: t: the tensor to participate in the accumulation. group_size: the total number of tensors to be collectively accumulated. Each must reside on a different device. Should be a positive integer. group_key: an integer identifying the group of devices. instance_key: an integer identifying the participating group of Ops. communication_hint: preferred collective communication. The implementation may fall back to another mechanism. Options include `auto`, `ring`, and `nccl`. timeout: a float. If set to a non zero, set a completion timeout to detect staleness. If the timer goes off, a DeadlineExceededError is raised. The timeout value in seconds. This feature is experimental. Returns: An Op implementing the distributed operation. Raises: ValueError: if any of the input parameter constraints are not met.",torch.distributed.distributed_c10d.reduce,"Reduces the tensor data across all machines. Only the process with rank ``dst`` is going to receive the final result. Args: tensor (Tensor): Input and output of the collective. The function operates in-place. dst (int): Destination rank on global process group (regardless of ``group`` argument) op (optional): One of the values from ``torch.distributed.ReduceOp`` enum. Specifies an operation used for element-wise reductions. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group",0.95895207
tensorflow.python.debug.lib.debug_data.get_rel_timestamps,"Get the relative timestamp from for a debug-dumped tensor. Relative timestamp means (absolute timestamp - `t0`), where `t0` is the absolute timestamp of the first dumped tensor in the dump root. The tensor may be dumped multiple times in the dump root directory, so a list of relative timestamps (`numpy.ndarray`) is returned. Args: node_name: (`str`) name of the node that the tensor is produced by. output_slot: (`int`) output slot index of tensor. debug_op: (`str`) name of the debug op. device_name: (`str`) name of the device. If there is only one device or if the specified debug_watch_key exists on only one device, this argument is optional. Returns: (`list` of `int`) list of relative timestamps. Raises: WatchKeyDoesNotExistInDebugDumpDirError: If the tensor watch key does not exist in the debug dump data.",torch.distributed.tensor.parallel.input_reshard.input_reshard,"Register hooks to an nn.Module for input resharding, enabling sharding and restoration during backward computation. Register hooks to an nn.Module with input resharding so that we can shard per the given `tp_device_mesh` and `input_reshard_dim` and restore the input back when recomputing the activations in the backward. The reason why we can do this is that for Tensor Parallel(TP), the input are same across all TP ranks. Args: module (:class:`nn.Module`): Module to be registered with input resharding. tp_device_mesh (:class:`DeviceMesh`): Object which describes the mesh topology of devices for Tensor Parallel. input_reshard_dim (Optional[int]): The dimension of where we perform the sharding of input. If set None, there is no sharding of input. Default: None Return: A :class:`nn.Module` object registered with TP input resharding.",0.9588992
tensorflow.python.framework.op_callbacks.add_op_callback,"Add a thread-local callback that intercepts op execution and op creation. The `callback_fn` will be invoked immediately after any of the three types of events: - The execution of an TensorFlow operation (""op"" for short hereafter) under eager mode, - The execution of a FuncGraph under eager mode, - The creation of an op during graph construction (e.g., in @tf.function-decorated Python functions). Known limitations: 1. Under graph mode, overriding the output tensors of control-flow ops, including ""If"", ""StatelessIf"" and ""While"", may cause errors (b/139668453). Overriding other tensors in a graph consisting of such control-flow ops is okay. 2. Under eager mode, calling eager ops from the callback function itself may lead to recursion stack overflow. This can be prevented by returning from the callback function immediately on encountering the op type involved (b/140334369). Args: callback_fn: A callback_fn that has the following signature: def callback_fn(op_type, inputs, attrs, outputs, op_name=None, graph=None): # op_type: The type of the op, as a string. E.g., ""MatMul"". # For the special case of FuncGraph execution, op_type # takes the name of the graph name, e.g., # ""__inference_my_func_24"". # inputs: (`tuple` of `Tensor`s) Input tensors to the op or the # FuncGraph. # - In eager execution, these are `EagerTensor`s. # - In graph construction, these are non-eager `Tensor`s # that form the inputs to the just-created op. # attrs: The attributes of the op or FuncGraph of which the execution # or creation caused the current invocation of the callback. # This is applicable to both eager- and graph-based execution, # as well as graph construction. # This is a tuple of alternating attribute keys and attribute # values. E.g., `('adjoint_a', False, 'adjoint_b', False)`. # outputs: (`tuple of `Tensor`s) Output tensors from the op or # FuncGraph. # In eager execution, these are `EagerTensor`s. # In graph construction, these are non-eager `Tensor`s that # are the outputs of the ",torch.cuda.graphs.make_graphed_callables,"Accept callables (functions or :class:`nn.Module<torch.nn.Module>`\ s) and returns graphed versions. Each graphed callable's forward pass runs its source callable's forward CUDA work as a CUDA graph inside a single autograd node. The graphed callable's forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable's backward work as a CUDA graph. Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop. See :ref:`Partial-network capture<partial-network-capture>` for detailed use and constraints. If you pass a tuple of several callables, their captures will use the same memory pool. See :ref:`Graph memory management<graph-memory-management>` for when this is appropriate. Arguments: callables (torch.nn.Module or Python function, or tuple of these): Callable or callables to graph. See :ref:`Graph memory management<graph-memory-management>` for when passing a tuple of callables is appropriate. If you pass a tuple of callables, their order in the tuple must be the same order they'll run in the live workload. sample_args (tuple of Tensors, or tuple of tuples of Tensors): Samples args for each callable. If a single callable was passed, ``sample_args`` must be a single tuple of argument Tensors. If a tuple of callables was passed, ``sample_args`` must be tuple of tuples of argument Tensors. num_warmup_iters (int): The number of warmup iterations. Currently, ``DataDistributedParallel`` needs 11 iterations for warm up. Default: ``3``. allow_unused_input (bool): If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False. pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or :meth:`other_Graph_instance.pool()<torch.cuda.CUDAGraph.pool>`) that hints this graph may share memory with the indicated pool. See :ref:`Graph memory management<graph-memory-management>`",0.9588106
tensorflow.python.framework.ops.add_exit_callback_to_default_func_graph,"Add a callback to run when the default function graph goes out of scope. Usage: python @tf.function def fn(x, v): expensive = expensive_object(v) add_exit_callback_to_default_func_graph(lambda: expensive.release()) return g(x, expensive) fn(x=tf.constant(...), v=...) # `expensive` has been released. Args: fn: A callable that takes no arguments and whose output is ignored. To be executed when exiting func graph scope. Raises: RuntimeError: If executed when the current default graph is not a FuncGraph, or not currently executing in function creation mode (e.g., if inside an init_scope).",torch._dynamo.bytecode_transformation.create_call_function,"Creates a sequence of instructions that makes a function call. `push_null` is used in Python 3.11+ only. It is used in codegen when a function call is intended to be made with the NULL + fn convention, and we know that the NULL has not been pushed yet. We will push a NULL and rotate it to the correct position immediately before making the function call. push_null should default to True unless you know you are calling a function that you codegen'd with a null already pushed, for example (assume `math` is available in the global scope), create_load_global(""math"", True) # pushes a null create_load_attr(""sqrt"") create_instruction(""LOAD_CONST"", argval=25) create_call_function(1, False)",0.95880806
tensorflow.python.autograph.impl.api.convert,"Decorator that compiles a function to use TensorFlow ops. The decorator is dynamic - it recompiles the target whenever the decorated function is called. This means the parameter values are known at conversion. It also means that repeated calls with different types of parameters will be correctly processed. Args: recursive: bool, whether to recursively convert any functions or classes that the converted function may use. optional_features: converted.Feature, allows toggling optional or experimental features. When set to None, only the core features are enabled. user_requested: bool, whether this is a function that the user explicitly asked to be converted. See ConversionOptions.user_requested. conversion_ctx: Optional ag_ctx.ControlStatusCtx, the Autograph context in which `f` is used. Returns: Callable, a decorator that converts the given function into an equivalent function that uses TensorFlow ops.",torch._inductor.codegen.cuda.gemm_template.render,"The primary entry point for the code rendering process used in this template. Renders the Cutlass based CUDA C++ code for the GEMM Kernel that this template is designed to implement, including potentially fused epilogues. Args: kernel (CUDATemplateKernel): The kernel to be rendered. op (cutlass_gemm_op.GemmOperation, optional): A GEMM operation that is required to be compatible with the input and output definitions as well as a possible epilogue. Defaults to None. **kwargs: Additional keyword arguments. Currently unused. Returns: str: Cutlass based CUDA C++ code fragment as a string, to be used by the current CUDATemplateKernel or autotuning code. Note: All inputs and their corresponding buffer addresses and names take precedence over previously passed inputs to the template at construction time. However, they should be layout compatible.",0.9587094
tensorflow.python.distribute.cluster_resolver.cluster_resolver.task_type,"Returns the task type this `ClusterResolver` indicates. In TensorFlow distributed environment, each job may have an applicable task type. Valid task types in TensorFlow include 'chief': a worker that is designated with more responsibility, 'worker': a regular worker for training/evaluation, 'ps': a parameter server, or 'evaluator': an evaluator that evaluates the checkpoints for metrics. See [Multi-worker configuration]( https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration) for more information about 'chief' and 'worker' task type, which are most commonly used. Having access to such information is useful when user needs to run specific code according to task types. For example, python cluster_spec = tf.train.ClusterSpec({ ""ps"": [""localhost:2222"", ""localhost:2223""], ""worker"": [""localhost:2224"", ""localhost:2225"", ""localhost:2226""] }) # SimpleClusterResolver is used here for illustration; other cluster # resolvers may be used for other source of task type/id. simple_resolver = SimpleClusterResolver(cluster_spec, task_type=""worker"", task_id=1) ... if cluster_resolver.task_type == 'worker': # Perform something that's only applicable on workers. This block # will run on this particular instance since we've specified this task to # be a worker in above cluster resolver. elif cluster_resolver.task_type == 'ps': # Perform something that's only applicable on parameter servers. This # block will not run on this particular instance. Returns `None` if such information is not available or is not applicable in the current distributed environment, such as training with `tf.distribute.experimental.TPUStrategy`. For more information, please see `tf.distribute.cluster_resolver.ClusterResolver`'s class doc.",torch.distributed.distributed_c10d.new_subgroups_by_enumeration,"Create subgroups by dividing the global world. The division is specified by a nested list of ranks. The subgroups cannot have overlap, and some ranks may not have to be in any subgroup. This is a convenience API that calls ``new_group`` to generate multiple subgroups. It requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. .. warning:: Using multiple process groups with the ``NCCL`` backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See `Using multiple NCCL communicators concurrently <https://docs.nvid ia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using -multiple-nccl-communicators-concurrently>`_ for more details. Args: ranks_per_subgroup_list (list[list[int]]): A nested list of ranks of group members. timeout (timedelta, optional): see `init_process_group` for details and default value. backend (str or Backend, optional): The backend to use. Depending on build-time configurations, valid values are ``gloo`` and ``nccl``. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., ``""gloo""``), which can also be accessed via :class:`Backend` attributes (e.g., ``Backend.GLOO``). If ``None`` is passed in, the backend corresponding to the default process group will be used. Default is ``None``. pg_options (ProcessGroupOptions, optional): process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the ``nccl`` backend, ``is_high_priority_stream`` can be specified so that process group can pick up",0.9586869
tensorflow.python.data.ops.dataset_ops.unbatch,"Splits elements of a dataset into multiple elements. For example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where `B` may vary for each input element, then for each element in the dataset, the unbatched dataset will contain `B` consecutive elements of shape `[a0, a1, ...]`. >>> elements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ] >>> dataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64) >>> dataset = dataset.unbatch() >>> list(dataset.as_numpy_iterator()) [1, 2, 3, 1, 2, 1, 2, 3, 4] Note: `unbatch` requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of `unbatch`. Args: name: (Optional.) A name for the tf.data operation. Returns: A new `Dataset` with the transformation applied as described above.",torch.functional.broadcast_tensors,"broadcast_tensors(*tensors) -> List of Tensors Broadcasts the given tensors according to :ref:`broadcasting-semantics`. Args: *tensors: any number of tensors of the same type .. warning:: More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first. Example:: >>> x = torch.arange(3).view(1, 3) >>> y = torch.arange(2).view(2, 1) >>> a, b = torch.broadcast_tensors(x, y) >>> a.size() torch.Size([2, 3]) >>> a tensor([[0, 1, 2], [0, 1, 2]])",0.9586477
tensorflow.python.ops.conv2d_benchmark.build_graph,"builds a graph containing a sequence of conv2d operations. Args: device: String, the device to run on. dtype: Data type for the convolution. data_format: A string from: ""NHWC"" or ""NCHW"". Data format for input and output data. input_shape: Shape of the input tensor. filter_shape: Shape of the filter tensor. strides: A list of ints. 1-D of length 4. The stride of sliding window for each dimension of input. padding: A string from: ""SAME"", ""VALID"". The type of padding algorithm to use. num_iters: number of iterations to run conv2d. warmup_iters: number of iterations for warmup runs. Returns: An array of tensors to run()",torch.nn.parallel.comm.reduce_add_coalesced,"Sum tensors from multiple GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations. Args: inputs (Iterable[Iterable[Tensor]]): iterable of iterables that contain tensors from a single device. destination (int, optional): a device on which the output will be placed (default: current device). buffer_size (int): maximum size of the buffer used for coalescing Returns: A tuple of tensors containing an elementwise sum of each group of inputs, placed on the ``destination`` device.",0.9586421
tensorflow.python.util.dispatch.register_dispatchable_type,"Class decorator that registers a type for use with type-based dispatch. Should *not* be used with subclasses of `CompositeTensor` or `ExtensionType` (which are automatically registered). Note: this function is intended to support internal legacy use cases (such as RaggedTensorValue), and will probably not be exposed as a public API. Args: cls: The class to register. Returns: `cls`.",torch.package.package_importer.python_version,Returns the version of python that was used to create this package. Note: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on. Returns: :class:`Optional[str]` a python version e.g. 3.8.9 or None if no version was stored with this package,0.9585308
tensorflow.python.ops.parallel_for.control_flow_ops.for_loop,"Runs `loop_fn` `iters` times and stacks the outputs. Runs `loop_fn` `iters` times, with input values from 0 to `iters - 1`, and stacks corresponding outputs of the different runs. Args: loop_fn: A function that takes an int32 scalar tf.Tensor object representing the iteration number, and returns a possibly nested structure of tensor objects. The shape of these outputs should not depend on the input. loop_fn_dtypes: dtypes for the outputs of `loop_fn`. iters: Number of iterations for which to run `loop_fn`. parallel_iterations: The number of iterations that can be dispatched in parallel. This knob can be used to control the total memory usage. Returns: Returns a nested structure of stacked output tensor objects with the same nested structure as the output of `loop_fn`.",torch._functorch.partitioners.default_partition,"Partitions the :attr:`joint_module` in a manner that closely resembles the behavior observed in the original ``.forward()`` and ``.backward()`` of the callable, i.e., the resulting forward graph contains those operators that are executed in the original ``.forward()`` callable passed to :func:`aot_function`. The default partitioner collects the operators that are between the forward inputs and the forward outputs. This helps in finding the tensors which have to be stashed for the backward pass. These stashed tensors become the output of the generated forward graph. The remaining operators are then placed in the backward graph. .. warning:: This API is experimental and likely to change. Args: joint_module(fx.GraphModule): The joint forward and backward graph. This is the result of AOT Autograd tracing. Returns: Returns the generated forward and backward Fx graph modules.",0.95850635
tensorflow.python.ops.signal.dct_ops.dct,"Computes the 1D [Discrete Cosine Transform (DCT)][dct] of `input`. Types I, II, III and IV are supported. Type I is implemented using a length `2N` padded `tf.signal.rfft`. Type II is implemented using a length `2N` padded `tf.signal.rfft`, as described here: [Type 2 DCT using 2N FFT padded (Makhoul)] (https://dsp.stackexchange.com/a/10606). Type III is a fairly straightforward inverse of Type II (i.e. using a length `2N` padded `tf.signal.irfft`). Type IV is calculated through 2N length DCT2 of padded signal and picking the odd indices. @compatibility(scipy) Equivalent to [scipy.fftpack.dct] (https://docs.scipy.org/doc/scipy-1.4.0/reference/generated/scipy.fftpack.dct.html) for Type-I, Type-II, Type-III and Type-IV DCT. @end_compatibility Args: input: A `[..., samples]` `float32`/`float64` `Tensor` containing the signals to take the DCT of. type: The DCT type to perform. Must be 1, 2, 3 or 4. n: The length of the transform. If length is less than sequence length, only the first n elements of the sequence are considered for the DCT. If n is greater than the sequence length, zeros are padded and then the DCT is computed as usual. axis: For future expansion. The axis to compute the DCT along. Must be `-1`. norm: The normalization to apply. `None` for no normalization or `'ortho'` for orthonormal normalization. name: An optional name for the operation. Returns: A `[..., samples]` `float32`/`float64` `Tensor` containing the DCT of `input`. Raises: ValueError: If `type` is not `1`, `2`, `3` or `4`, `axis` is not `-1`, `n` is not `None` or greater than 0, or `norm` is not `None` or `'ortho'`. ValueError: If `type` is `1` and `norm` is `ortho`. [dct]: https://en.wikipedia.org/wiki/Discrete_cosine_transform",torch.nn.utils.prune.ln_structured,"Prune tensor by removing channels with the lowest L\ ``n``-norm along the specified dimension. Prunes tensor corresponding to parameter called ``name`` in ``module`` by removing the specified ``amount`` of (currently unpruned) channels along the specified ``dim`` with the lowest L\ ``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called ``name+'_mask'`` corresponding to the binary mask applied to the parameter ``name`` by the pruning method. 2) replacing the parameter ``name`` by its pruned version, while the original (unpruned) parameter is stored in a new parameter named ``name+'_orig'``. Args: module (nn.Module): module containing the tensor to prune name (str): parameter name within ``module`` on which pruning will act. amount (int or float): quantity of parameters to prune. If ``float``, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If ``int``, it represents the absolute number of parameters to prune. n (int, float, inf, -inf, 'fro', 'nuc'): See documentation of valid entries for argument ``p`` in :func:`torch.norm`. dim (int): index of the dim along which we define channels to prune. importance_scores (torch.Tensor): tensor of importance scores (of same shape as module parameter) used to compute mask for pruning. The values in this tensor indicate the importance of the corresponding elements in the parameter being pruned. If unspecified or None, the module parameter will be used in its place. Returns: module (nn.Module): modified (i.e. pruned) version of the input module Examples: >>> from torch.nn.utils import prune >>> m = prune.ln_structured( ... nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf') ... )",0.9584984
tensorflow.python.ops.collective_ops.all_gather_v2,"Accumulates tensors collectively, across devices, along first dimension. Args: t: the tensor to participate in the accumulation. group_size: an int32 tensor, the total number of tensors to be collectively accumulated. Each must reside on a different device. Should be a positive integer. group_key: an int32 tensor identifying the group of devices. instance_key: an int32 tensor identifying the participating group of Ops. communication_hint: preferred collective communication. The implementation may fall back to another mechanism. Options include `auto`, `ring`, and `nccl`. timeout: a float. If set to a non zero, set a completion timeout to detect staleness. If the timer goes off, a DeadlineExceededError is raised. The timeout value in seconds. This feature is experimental. ordering_token: a resource tensor on the same device as the op to order the collectives in a per-device manner by auto control dependency. This argument can be omited when there is one collective Op per `tf.function`, or when explicit control dependency is used instead of auto control dependency. name: name of the Op. Returns: An Op implementing the distributed operation.",torch.distributed.distributed_c10d.barrier,"Synchronize all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait(). Args: group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op device_ids ([int], optional): List of device/GPU ids. Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group .. note:: `ProcessGroupNCCL` now relies on stream synchronization instead of device synchronization to block the CPU. Thus, please do not assume that `barrier()` would perform a device synchronization.",0.9584613
tensorflow.python.keras.utils.io_utils.ask_to_proceed_with_overwrite,"Produces a prompt asking about overwriting a file. Args: filepath: the path to the file to be overwritten. Returns: True if we can proceed with overwrite, False otherwise.",torch.utils.file_baton.try_acquire,"Try to atomically create a file under exclusive access. Returns: True if the file could be created, else False.",0.95834935
tensorflow.python.training.saving.saveable_object_util.recreate_saveable_objects,Returns a dict of SaveableObject factories generated from loaded fns.,torch.export.exported_program.module,Returns a self contained GraphModule with all the parameters/buffers inlined.,0.9583236
tensorflow.python.framework.tensor_util.constant_value_as_shape,"A version of `constant_value()` that returns a `TensorShape`. This version should be used when a constant tensor value is interpreted as a (possibly partial) shape, e.g. in the shape function for `tf.reshape()`. By explicitly requesting a `TensorShape` as the return value, it is possible to represent unknown dimensions; by contrast, `constant_value()` is all-or-nothing. Args: tensor: The rank-0 or rank-1 Tensor to be evaluated. Returns: A `TensorShape` based on the constant value of the given `tensor`. Raises: ValueError: If the shape is rank-0 and is not statically known to be -1.",torch.nn.utils.prune.apply,"Add pruning on the fly and reparametrization of a tensor. Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask. Args: module (nn.Module): module containing the tensor to prune name (str): parameter name within ``module`` on which pruning will act. amount (int or float): quantity of parameters to prune. If ``float``, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If ``int``, it represents the absolute number of parameters to prune.",0.9582963
tensorflow.python.compiler.xla.experimental.xla_sharding.proto,Return the sharding protobuf of type xla_data_pb2.OpSharding.,torch.ao.quantization.backend_config.native.get_native_backend_config,Return the `BackendConfig` for PyTorch Native backend (fbgemm/qnnpack).,0.9582741
tensorflow.python.autograph.operators.py_builtins.super_in_original_context,"Executes the super function in the context of a specified function. See https://docs.python.org/3/library/functions.html#super for the exact details Args: f: Callable, typically the super builtin args: List[Any], the original call arguments caller_fn_scope: Optional[function_wrappers.FunctionScope], the function scope of the converted function in which this call was originally made Returns: The result of calling `f` as if it was called in the frame indicated by `caller_fn_scope`.",torch.fx.interpreter.call_function,Execute a ``call_function`` node and return the result. Args: target (Target): The call target for this node. See `Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for details on semantics args (Tuple): Tuple of positional args for this invocation kwargs (Dict): Dict of keyword arguments for this invocation Return Any: The value returned by the function invocation,0.9581975
tensorflow.python.ops.math_ops.range,"Creates a sequence of numbers. Creates a sequence of numbers that begins at `start` and extends by increments of `delta` up to but not including `limit`. The dtype of the resulting tensor is inferred from the inputs unless it is provided explicitly. Like the Python builtin `range`, `start` defaults to 0, so that `range(n) = range(0, n)`. For example: >>> start = 3 >>> limit = 18 >>> delta = 3 >>> tf.range(start, limit, delta) <tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 3, 6, 9, 12, 15], dtype=int32)> >>> start = 3 >>> limit = 1 >>> delta = -0.5 >>> tf.range(start, limit, delta) <tf.Tensor: shape=(4,), dtype=float32, numpy=array([3. , 2.5, 2. , 1.5], dtype=float32)> >>> limit = 5 >>> tf.range(limit) <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)> Args: start: A 0-D `Tensor` (scalar). Acts as first entry in the range if `limit` is not None; otherwise, acts as range limit and first entry defaults to 0. limit: A 0-D `Tensor` (scalar). Upper limit of sequence, exclusive. If None, defaults to the value of `start` while the first entry of the range defaults to 0. delta: A 0-D `Tensor` (scalar). Number that increments `start`. Defaults to 1. dtype: The type of the elements of the resulting tensor. name: A name for the operation. Defaults to ""range"". Returns: An 1-D `Tensor` of type `dtype`. @compatibility(numpy) Equivalent to np.arange @end_compatibility",torch.distributed.distributed_c10d.all_reduce,"Reduces the tensor data across all machines in a way that all get the final result. After the call ``tensor`` is going to be bitwise identical in all processes. Complex tensors are supported. Args: tensor (Tensor): Input and output of the collective. The function operates in-place. op (optional): One of the values from ``torch.distributed.ReduceOp`` enum. Specifies an operation used for element-wise reductions. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group Examples: >>> # xdoctest: +SKIP(""no rank"") >>> # All tensors below are of torch.int64 type. >>> # We have 2 process groups, 2 ranks. >>> device = torch.device(f'cuda:{rank}') >>> tensor = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank >>> tensor tensor([1, 2], device='cuda:0') # Rank 0 tensor([3, 4], device='cuda:1') # Rank 1 >>> dist.all_reduce(tensor, op=ReduceOp.SUM) >>> tensor tensor([4, 6], device='cuda:0') # Rank 0 tensor([4, 6], device='cuda:1') # Rank 1 >>> # All tensors below are of torch.cfloat type. >>> # We have 2 process groups, 2 ranks. >>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat, device=device) + 2 * rank * (1+1j) >>> tensor tensor([1.+1.j, 2.+2.j], device='cuda:0') # Rank 0 tensor([3.+3.j, 4.+4.j], device='cuda:1') # Rank 1 >>> dist.all_reduce(tensor, op=ReduceOp.SUM) >>> tensor tensor([4.+4.j, 6.+6.j], device='cuda:0') # Rank 0 tensor([4.+4.j, 6.+6.j], device='cuda:1') # Rank 1",0.95816916
tensorflow.python.ops.array_ops.extract_image_patches,"Extract patches from images and put them in the ""depth"" output dimension. Args: `images`: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`. 4-D Tensor with shape `[batch, in_rows, in_cols, depth]`. `ksizes`: A list of `ints` that has length `>= 4`. The size of the sliding window for each dimension of `images`. `strides`: A list of `ints` that has length `>= 4`. 1-D of length 4. How far the centers of two consecutive patches are in the images. Must be: `[1, stride_rows, stride_cols, 1]`. `rates`: A list of `ints` that has length `>= 4`. 1-D of length 4. Must be: `[1, rate_rows, rate_cols, 1]`. This is the input stride, specifying how far two consecutive patch samples are in the input. Equivalent to extracting patches with `patch_sizes_eff = patch_sizes + (patch_sizes - 1) * (rates - 1)`, followed by subsampling them spatially by a factor of `rates`. This is equivalent to `rate` in dilated (a.k.a. Atrous) convolutions. `padding`: A `string` from: ""SAME"", ""VALID"". The type of padding algorithm to use. We specify the size-related attributes as: ksizes = [1, ksize_rows, ksize_cols, 1] strides = [1, strides_rows, strides_cols, 1] rates = [1, rates_rows, rates_cols, 1] name: A name for the operation (optional). Returns: A Tensor. Has the same type as images.",torch.nn.modules.sparse.forward,"Forward pass of EmbeddingBag. Args: input (Tensor): Tensor containing bags of indices into the embedding matrix. offsets (Tensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines the starting index position of each bag (sequence) in :attr:`input`. per_sample_weights (Tensor, optional): a tensor of float / double weights, or None to indicate all weights should be taken to be ``1``. If specified, :attr:`per_sample_weights` must have exactly the same shape as input and is treated as having the same :attr:`offsets`, if those are not ``None``. Only supported for ``mode='sum'``. Returns: Tensor output shape of `(B, embedding_dim)`. .. note:: A few notes about ``input`` and ``offsets``: - :attr:`input` and :attr:`offsets` have to be of the same type, either int or long - If :attr:`input` is 2D of shape `(B, N)`, it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and this will return ``B`` values aggregated in a way depending on the :attr:`mode`. :attr:`offsets` is ignored and required to be ``None`` in this case. - If :attr:`input` is 1D of shape `(N)`, it will be treated as a concatenation of multiple bags (sequences). :attr:`offsets` is required to be a 1D tensor containing the starting index positions of each bag in :attr:`input`. Therefore, for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as having ``B`` bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.",0.958129
tensorflow.python.distribute.distribute_lib.experimental_local_results,"Returns the list of all local per-replica values contained in `value`. Note: This only returns values on the worker initiated by this client. When using a `tf.distribute.Strategy` like `tf.distribute.experimental.MultiWorkerMirroredStrategy`, each worker will be its own client, and this function will only return values computed on that worker. Args: value: A value returned by `experimental_run()`, `run(), or a variable created in `scope`. Returns: A tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`",torch.distributed._spmd.api.replacement,Implement this method to return a new :class:`nn.Module` instance to replace the ``orig_submodule`` argument in the model. This helps if ``orig_submodule`` is not traceable or should not be traced. Args: fqn (str): fully quantified name of the submodule. orig_submodule (class:`nn.Module`): original submodule instance to replace. Returns: A new :class:`nn.Module` instance to replace the original one.,0.9581158
tensorflow.python.ops.sparse_ops.sparse_maximum,"Returns the element-wise max of two SparseTensors. Assumes the two SparseTensors have the same shape, i.e., no broadcasting. Example: >>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7]) >>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7]) >>> res = tf.sparse.maximum(sp_zero, sp_one) >>> res.indices <tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[0], [1]])> >>> res.values <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)> >>> res.dense_shape <tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])> The reduction version of this elementwise operation is `tf.sparse.reduce_max` Args: sp_a: a `SparseTensor` operand whose dtype is real, and indices lexicographically ordered. sp_b: the other `SparseTensor` operand with the same requirements (and the same shape). name: optional name of the operation. Returns: output: the output SparseTensor.",torch.nn.modules.module.apply,"Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self. Typical use includes initializing the parameters of a model (see also :ref:`nn-init-doc`). Args: fn (:class:`Module` -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[1., 1.], [1., 1.]], requires_grad=True) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )",0.9580946
tensorflow.python.saved_model.method_name_updater.save,"Saves the updated `SavedModel`. Args: new_export_dir: Path where the updated `SavedModel` will be saved. If None, the input `SavedModel` will be overriden with the updates. Raises: errors.OpError: If there are errors during the file save operation.",torch.onnx._internal.exporter.save_diagnostics,Saves the export diagnostics as a SARIF log to the specified destination path. Args: destination: The destination to save the diagnostics SARIF log. It must have a `.sarif` extension. Raises: ValueError: If the destination path does not end with `.sarif` extension.,0.95803887
tensorflow.python.debug.lib.debug_data.run_feed_keys_info,"Get a str representation of the feed_dict used in the Session.run() call. Returns: If the information is available from one `Session.run` call, a `str` obtained from `repr(feed_dict)`. If the information is available from multiple `Session.run` calls, a `list` of `str` obtained from `repr(feed_dict)`. If the information is not available, `None`.",torch.fx.interpreter.fetch_args_kwargs_from_env,"Fetch the concrete values of ``args`` and ``kwargs`` of node ``n`` from the current execution environment. Args: n (Node): The node for which ``args`` and ``kwargs`` should be fetched. Return: Tuple[Tuple, Dict]: ``args`` and ``kwargs`` with concrete values for ``n``.",0.95803374
tensorflow.python.kernel_tests.signal.test_util.evaluate_tflite_model,Evaluates the provided tf.lite model with the given input ndarrays. Args: tflite_model: bytes. The serialized tf.lite model. input_ndarrays: A list of NumPy arrays to feed as input to the model. Returns: A list of ndarrays produced by the model. Raises: ValueError: If the number of input arrays does not match the number of inputs the model expects.,torch.fx.passes.splitter_base.generate_inputs_for_submodules,"Generate inputs for targeting submdoules in the given model. Note that if two submodules refer to the same obj, this function doesn't work. Args: model: root model. inputs: inputs to the root model. target_submodules: submodules that we want to generate inputs for. Returns: A dict that maps from submodule name to its inputs.",0.958018
tensorflow.python.framework.traceable_stack.peek_top_obj,Return the most recent stored object.,torch.storage.clone,Return a copy of this storage.,0.9579643
tensorflow.python.util.dispatch.unregister_dispatch_for,"Unregisters a function that was registered with `@dispatch_for_*`. This is primarily intended for testing purposes. Example: >>> # Define a type and register a dispatcher to override `tf.abs`: >>> class MyTensor(tf.experimental.ExtensionType): ... value: tf.Tensor >>> @tf.experimental.dispatch_for_api(tf.abs) ... def my_abs(x: MyTensor): ... return MyTensor(tf.abs(x.value)) >>> tf.abs(MyTensor(5)) MyTensor(value=<tf.Tensor: shape=(), dtype=int32, numpy=5>) >>> # Unregister the dispatcher, so `tf.abs` no longer calls `my_abs`. >>> unregister_dispatch_for(my_abs) >>> tf.abs(MyTensor(5)) Traceback (most recent call last): ... ValueError: Attempt to convert a value ... to a Tensor. Args: dispatch_target: The function to unregister. Raises: ValueError: If `dispatch_target` was not registered using `@dispatch_for`, `@dispatch_for_unary_elementwise_apis`, or `@dispatch_for_binary_elementwise_apis`.",torch.serialization.add_safe_globals,"Marks the given globals as safe for ``weights_only`` load. For example, functions added to this list can be called during unpickling, classes could be instantiated and have state set. Args: safe_globals (List[Any]): list of globals to mark as safe Example: >>> # xdoctest: +SKIP(""Can't torch.save(t, ...) as doctest thinks MyTensor is defined on torch.serialization"") >>> import tempfile >>> class MyTensor(torch.Tensor): ... pass >>> t = MyTensor(torch.randn(2, 3)) >>> with tempfile.NamedTemporaryFile() as f: ... torch.save(t, f.name) # Running `torch.load(f.name, weights_only=True)` will fail with # Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default. # Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint. ... torch.serialization.add_safe_globals([MyTensor]) ... torch.load(f.name, weights_only=True) # MyTensor([[-0.5024, -1.8152, -0.5455], # [-0.8234, 2.0500, -0.3657]])",0.9579143
tensorflow.python.ops.metrics_impl.metric_variable,"Create variable in `GraphKeys.(LOCAL|METRIC_VARIABLES)` collections. If running in a `DistributionStrategy` context, the variable will be ""sync on read"". This means: * The returned object will be a container with separate variables per replica of the model. * When writing to the variable, e.g. using `assign_add` in a metric update, the update will be applied to the variable local to the replica. * To get a metric's result value, we need to sum the variable values across the replicas before computing the final answer. Furthermore, the final answer should be computed once instead of in every replica. Both of these are accomplished by running the computation of the final result value inside `distribute_lib.get_replica_context().merge_call(fn)`. Inside the `merge_call()`, ops are only added to the graph once and access to a sync on read variable in a computation returns the sum across all replicas. Args: shape: Shape of the created variable. dtype: Type of the created variable. validate_shape: (Optional) Whether shape validation is enabled for the created variable. name: (Optional) String name of the created variable. Returns: A (non-trainable) variable initialized to zero, or if inside a `DistributionStrategy` scope a sync on read variable container.",torch.distributed._tensor.api.full_tensor,"Return the full tensor of this DTensor. It will perform necessary collectives to gather the local tensors from other ranks in its DeviceMesh and concatenate them together. It's a syntatic sugar of the following code: `dtensor.redistribute(placements=[Replicate()] * mesh.ndim).to_local()` Keyword args: grad_placements (List[:class:`Placement`], optional): the placements describes the future layout of any gradient layout of the full Tensor returned from this function. `full_tensor` converts DTensor to a full torch.Tensor and the returned torch.tensor might not be used as the original replicated DTensor layout later in the code. This argument is the hint that user can give to autograd in case the gradient layout of the returned tensor does not match the original replicated DTensor layout. If not specified, we will assume the gradient layout of the full tensor be replicated. Returns: A :class:`torch.Tensor` object that represents the full tensor of this DTensor. .. note:: `full_tensor` is differentiable.",0.95791394
tensorflow.python.keras.metrics.get_config,Returns the serializable config of the metric.,torch.testing._internal.common_utils.get_tensors_from,Returns a set of all Tensor objects in the given args and kwargs.,0.95786023
tensorflow.python.ops.stateful_random_ops.from_seed,"Creates a generator from a seed. A seed is a 1024-bit unsigned integer represented either as a Python integer or a vector of integers. Seeds shorter than 1024-bit will be padded. The padding, the internal structure of a seed and the way a seed is converted to a state are all opaque (unspecified). The only semantics specification of seeds is that two different seeds are likely to produce two independent generators (but no guarantee). Args: seed: the seed for the RNG. alg: (optional) the RNG algorithm. If None, it will be auto-selected. See `__init__` for its possible values. Returns: The new generator.",torch.cuda.random.manual_seed,"Set the seed for generating random numbers for the current GPU. It's safe to call this function if CUDA is not available; in that case, it is silently ignored. Args: seed (int): The desired seed. .. warning:: If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use :func:`manual_seed_all`.",0.9577798
tensorflow.python.ops.check_ops.assert_near,"Assert the condition `x` and `y` are close element-wise. Example of adding a dependency to an operation: python with tf.control_dependencies([tf.compat.v1.assert_near(x, y)]): output = tf.reduce_sum(x) This condition holds if for every pair of (possibly broadcast) elements `x[i]`, `y[i]`, we have tf.abs(x[i] - y[i]) <= atol + rtol * tf.abs(y[i]). If both `x` and `y` are empty, this is trivially satisfied. The default `atol` and `rtol` is `10 * eps`, where `eps` is the smallest representable positive number such that `1 + eps != 1`. This is about `1.2e-6` in `32bit`, `2.22e-15` in `64bit`, and `0.00977` in `16bit`. See `numpy.finfo`. Args: x: Float or complex `Tensor`. y: Float or complex `Tensor`, same `dtype` as, and broadcastable to, `x`. rtol: `Tensor`. Same `dtype` as, and broadcastable to, `x`. The relative tolerance. Default is `10 * eps`. atol: `Tensor`. Same `dtype` as, and broadcastable to, `x`. The absolute tolerance. Default is `10 * eps`. data: The tensors to print out if the condition is False. Defaults to error message and first few entries of `x`, `y`. summarize: Print this many entries of each tensor. message: A string to prefix to the default message. name: A name for this operation (optional). Defaults to ""assert_near"". Returns: Op that raises `InvalidArgumentError` if `x` and `y` are not close enough. @compatibility(numpy) Similar to `numpy.testing.assert_allclose`, except tolerance depends on data type. This is due to the fact that `TensorFlow` is often used with `32bit`, `64bit`, and even `16bit` data. @end_compatibility",torch.distributed.fsdp.fully_sharded_data_parallel.full_optim_state_dict,"Return the full optimizer state-dict. Consolidates the full optimizer state on rank 0 and returns it as a :class:`dict` following the convention of :meth:`torch.optim.Optimizer.state_dict`, i.e. with keys ``""state""`` and ``""param_groups""``. The flattened parameters in ``FSDP`` modules contained in ``model`` are mapped back to their unflattened parameters. This needs to be called on all ranks since it uses collective communications. However, if ``rank0_only=True``, then the state dict is only populated on rank 0, and all other ranks return an empty :class:`dict`. Unlike ``torch.optim.Optimizer.state_dict()``, this method uses full parameter names as keys instead of parameter IDs. Like in :meth:`torch.optim.Optimizer.state_dict`, the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using ``torch.save()``. Args: model (torch.nn.Module): Root module (which may or may not be a :class:`FullyShardedDataParallel` instance) whose parameters were passed into the optimizer ``optim``. optim (torch.optim.Optimizer): Optimizer for ``model`` 's parameters. optim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]): Input passed into the optimizer ``optim`` representing either a :class:`list` of parameter groups or an iterable of parameters; if ``None``, then this method assumes the input was ``model.parameters()``. This argument is deprecated, and there is no need to pass it in anymore. (Default: ``None``) rank0_only (bool): If ``True``, saves the populated :class:`dict` only on rank 0; if ``False``, saves it on all ranks. (Default: ``True``) group (dist.ProcessGroup): Model's process group or ``None`` if using the default process group. (Default: ``None``) Returns: Dict[str, Any]: A :class:`dict` containing the optimizer state for ``model`` 's original unflattened parameters and including keys ""state"" and ""param_groups"" fo",0.9577497
tensorflow.python.module.module.trainable_variables,Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first).,torch.distributed.optim.zero_redundancy_optimizer.step,Perform a single optimizer step and syncs parameters across all ranks. Arguments: closure (Callable): a closure that re-evaluates the model and returns the loss; optional for most optimizers. Returns: Optional loss depending on the underlying local optimizer. .. note: Any extra parameters are passed to the base optimizer as-is.,0.9577184
tensorflow.python.tpu.tpu_embedding_for_serving.cpu_embedding_lookup,"Apply standard lookup ops with `tf.tpu.experimental.embedding` configs. This function is a utility which allows using the `tf.tpu.experimental.embedding` config objects with standard lookup functions. This can be used when exporting a model which uses `tf.tpu.experimental.embedding.TPUEmbedding` for serving on CPU. In particular `tf.tpu.experimental.embedding.TPUEmbedding` only supports lookups on TPUs and should not be part of your serving graph. Note that TPU specific options (such as `max_sequence_length`) in the configuration objects will be ignored. In the following example we take a trained model (see the documentation for `tf.tpu.experimental.embedding.TPUEmbedding` for the context) and create a saved model with a serving function that will perform the embedding lookup and pass the results to your model: python model = model_fn(...) embedding = tf.tpu.experimental.embedding.TPUEmbedding( feature_config=feature_config, batch_size=1024, optimizer=tf.tpu.experimental.embedding.SGD(0.1)) checkpoint = tf.train.Checkpoint(model=model, embedding=embedding) checkpoint.restore(...) @tf.function(input_signature=[{'feature_one': tf.TensorSpec(...), 'feature_two': tf.TensorSpec(...), 'feature_three': tf.TensorSpec(...)}]) def serve_tensors(embedding_features): embedded_features = tf.tpu.experimental.embedding.serving_embedding_lookup( embedding_features, None, embedding.embedding_tables, feature_config) return model(embedded_features) model.embedding_api = embedding tf.saved_model.save(model, export_dir=..., signatures={'serving_default': serve_tensors}) NOTE: It's important to assign the embedding API object to a member of your model as `tf.saved_model.save` only supports saving variables as one `Trackable` object. Since the model's weights are in `model` and the embedding table are managed by `embedding`, we assign `embedding` to an attribute of `model` so that tf.saved_model.save can find the embedding variables. NOTE: The same `serve_tensors` function and `tf.saved_m",torch.ao.quantization.quantize_fx.convert_fx,"Convert a calibrated or trained model to a quantized model Args: * `graph_module` (torch.fx.GraphModule): A prepared and calibrated/trained model (GraphModule) * `convert_custom_config` (ConvertCustomConfig): custom configurations for convert function. See :class:`~torch.ao.quantization.fx.custom_config.ConvertCustomConfig` for more details * `_remove_qconfig` (bool): Option to remove the qconfig attributes in the model after convert. * `qconfig_mapping` (QConfigMapping): config for specifying how to convert a model for quantization. The keys must include the ones in the qconfig_mapping passed to `prepare_fx` or `prepare_qat_fx`, with the same values or `None`. Additional keys can be specified with values set to `None`. For each entry whose value is set to None, we skip quantizing that entry in the model:: qconfig_mapping = QConfigMapping .set_global(qconfig_from_prepare) .set_object_type(torch.nn.functional.add, None) # skip quantizing torch.nn.functional.add .set_object_type(torch.nn.functional.linear, qconfig_from_prepare) .set_module_name(""foo.bar"", None) # skip quantizing module ""foo.bar"" * `backend_config` (BackendConfig): A configuration for the backend which describes how operators should be quantized in the backend, this includes quantization mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.), observer placement for each operators and fused operators. See :class:`~torch.ao.quantization.backend_config.BackendConfig` for more details Return: A quantized model (torch.nn.Module) Example:: # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training # convert_fx converts a calibrated/trained model to a quantized model for the # target hardware, this includes converting the model first to a reference # quantized model, and then lower the reference quantized model to a backend # Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and # they share the same set of quantized operators, so we are u",0.9576681
tensorflow.python.ops.variables.model_variables,"Returns all variables in the MODEL_VARIABLES collection. Args: scope: (Optional.) A string. If supplied, the resulting list is filtered to include only items whose `name` attribute matches `scope` using `re.match`. Items without a `name` attribute are never returned if a scope is supplied. The choice of `re.match` means that a `scope` without special tokens filters by prefix. Returns: A list of local Variable objects.",torch.utils.data.graph.traverse_dps,"Traverse the DataPipes and their attributes to extract the DataPipe graph. This only looks into the attribute from each DataPipe that is either a DataPipe and a Python collection object such as ``list``, ``tuple``, ``set`` and ``dict``. Args: datapipe: the end DataPipe of the graph Returns: A graph represented as a nested dictionary, where keys are ids of DataPipe instances and values are tuples of DataPipe instance and the sub-graph",0.9576361
tensorflow.python.ops.sort_ops.argsort,"Returns the indices of a tensor that give its sorted order along an axis. >>> values = [1, 10, 26.9, 2.8, 166.32, 62.3] >>> sort_order = tf.argsort(values) >>> sort_order.numpy() array([0, 3, 1, 2, 5, 4], dtype=int32) For a 1D tensor: >>> sorted = tf.gather(values, sort_order) >>> assert tf.reduce_all(sorted == tf.sort(values)) For higher dimensions, the output has the same shape as `values`, but along the given axis, values represent the index of the sorted element in that slice of the tensor at the given position. >>> mat = [[30,20,10], ... [20,10,30], ... [10,30,20]] >>> indices = tf.argsort(mat) >>> indices.numpy() array([[2, 1, 0], [1, 0, 2], [0, 2, 1]], dtype=int32) If `axis=-1` these indices can be used to apply a sort using `tf.gather`: >>> tf.gather(mat, indices, batch_dims=-1).numpy() array([[10, 20, 30], [10, 20, 30], [10, 20, 30]], dtype=int32) See also: * `tf.sort`: Sort along an axis. * `tf.math.top_k`: A partial sort that returns a fixed number of top values and corresponding indices. Args: values: 1-D or higher **numeric** `Tensor`. axis: The axis along which to sort. The default is -1, which sorts the last axis. direction: The direction in which to sort the values (`'ASCENDING'` or `'DESCENDING'`). stable: If True, equal elements in the original tensor will not be re-ordered in the returned order. Unstable sort is not yet implemented, but will eventually be the default for performance reasons. If you require a stable order, pass `stable=True` for forwards compatibility. name: Optional name for the operation. Returns: An int32 `Tensor` with the same shape as `values`. The indices that would sort each slice of the given `values` along the given `axis`. Raises: ValueError: If axis is not a constant scalar, or the direction is invalid. tf.errors.InvalidArgumentError: If the `values.dtype` is not a `float` or `int` type.",torch.distributed.distributed_c10d.all_to_all_single,"Split input tensor and then scatter the split list to all processes in a group. Later the received tensors are concatenated from all the processes in the group and returned as a single output tensor. Complex tensors are supported. Args: output (Tensor): Gathered concatenated output tensor. input (Tensor): Input tensor to scatter. output_split_sizes: (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of ``output`` tensor must divide equally by ``world_size``. input_split_sizes: (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of ``input`` tensor must divide equally by ``world_size``. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op. Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group. .. warning:: `all_to_all_single` is experimental and subject to change. Examples: >>> # xdoctest: +SKIP(""Undefined rank"") >>> input = torch.arange(4) + rank * 4 >>> input tensor([0, 1, 2, 3]) # Rank 0 tensor([4, 5, 6, 7]) # Rank 1 tensor([8, 9, 10, 11]) # Rank 2 tensor([12, 13, 14, 15]) # Rank 3 >>> output = torch.empty([4], dtype=torch.int64) >>> dist.all_to_all_single(output, input) >>> output tensor([0, 4, 8, 12]) # Rank 0 tensor([1, 5, 9, 13]) # Rank 1 tensor([2, 6, 10, 14]) # Rank 2 tensor([3, 7, 11, 15]) # Rank 3 >>> # Essentially, it is similar to following operation: >>> scatter_list = list(input.chunk(world_size)) >>> gather_list = list(output.chunk(world_size)) >>> for i in range(world_size): >>> dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i) >>> # Another example with uneven split >>> input tensor([0, 1, 2, 3, 4, 5]) # Rank 0 tensor([10, 11, 12, 13, 14, 15, 16, 17, 18]) # Rank 1 tensor([20, 21, 22, 23, 24]) # Rank 2 tensor([30, 31, 32, 33, 34, 35, 36]) # Rank 3 >>> input_splits [2, 2, 1, 1] # Rank 0 [3,",0.9575859
tensorflow.python.autograph.pyct.transpiler.create,Initializes a function.,torch.testing._internal.common_fsdp.init,Initializes an instance of this model.,0.9575527
tensorflow.python.keras.engine.base_layer.get_output_shape_at,"Retrieves the output shape(s) of a layer at a given node. Args: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode.",torch._inductor.codegen.cuda.cuda_kernel.size,"Hook called from template code to get the size of an arg. Generates code which represents size of a given node in [start_index, end_index). If node is None, returns default_value. TODO: Will add needed args to pass it in if it is dynamic.",0.9575235
tensorflow.python.ops.random_crop_ops.stateless_random_crop,"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Usage Example: >>> image = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]] >>> seed = (1, 2) >>> tf.image.stateless_random_crop(value=image, size=(1, 2, 3), seed=seed) <tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy= array([[[1, 2, 3], [4, 5, 6]]], dtype=int32)> Args: value: Input tensor to crop. size: 1-D tensor with size the rank of `value`. seed: A shape [2] Tensor, the seed to the random number generator. Must have dtype `int32` or `int64`. (When using XLA, only `int32` is allowed.) name: A name for this operation (optional). Returns: A cropped tensor of the same rank as `value` and shape `size`.",torch.nn.functional.max_pool3d_with_indices,"max_pool3d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False) Applies a 3D max pooling over an input signal composed of several input planes. .. note:: The order of :attr:`ceil_mode` and :attr:`return_indices` is different from what seen in :class:`~torch.nn.MaxPool3d`, and will change in a future release. See :class:`~torch.nn.MaxPool3d` for details. Args: input: input tensor :math:`(\text{minibatch} , \text{in\_channels} , iD, iH , iW)`, minibatch dim optional. kernel_size: size of the pooling region. Can be a single number or a tuple `(kT, kH, kW)` stride: stride of the pooling operation. Can be a single number or a tuple `(sT, sH, sW)`. Default: :attr:`kernel_size` padding: Implicit negative infinity padding to be added on both sides, must be >= 0 and <= kernel_size / 2. dilation: The stride between elements within a sliding window, must be > 0. ceil_mode: If ``True``, will use `ceil` instead of `floor` to compute the output shape. This ensures that every element in the input tensor is covered by a sliding window. return_indices: If ``True``, will return the argmax along with the max values. Useful for :class:`torch.nn.functional.max_unpool3d` later",0.95749325
tensorflow.python.distribute.multi_worker_test_base.run_multiple_tasks_in_processes,Run `cmd_args` in a process for each task in `cluster_spec`.,torch.testing._internal.common_distributed.run_test_with_threaded_pg,Run the current test associated with `test_name` using the threaded process group.,0.95748234
tensorflow.python.compiler.xla.jit.experimental_jit_scope,"Enable or disable JIT compilation of operators within the scope. NOTE: This is an experimental feature. The compilation is a hint and only supported on a best-effort basis. Example usage: python with tf.xla.experimental.jit_scope(): c = tf.matmul(a, b) # compiled with tf.xla.experimental.jit_scope(compile_ops=False): d = tf.matmul(a, c) # not compiled with tf.xla.experimental.jit_scope( compile_ops=lambda node_def: 'matmul' in node_def.op.lower()): e = tf.matmul(a, b) + d # matmul is compiled, the addition is not. Example of `separate_compiled_gradients`: python # In the example below, the computations for f, g and h will all be compiled # in separate scopes. with tf.xla.experimental.jit_scope( separate_compiled_gradients=True): f = tf.matmul(a, b) g = tf.gradients([f], [a, b], name='mygrads1') h = tf.gradients([f], [a, b], name='mygrads2') Ops that are not in the scope may be clustered and compiled with ops in the scope with `compile_ops=True`, while the ops in the scope with `compile_ops=False` will never be compiled. For example: python # In the example below, x and loss may be clustered and compiled together, # while y will not be compiled. with tf.xla.experimental.jit_scope(): x = tf.matmul(a, b) with tf.xla.experimental.jit_scope(compile_ops=False): y = tf.matmul(c, d) loss = x + y If you want to only compile the ops in the scope with `compile_ops=True`, consider adding an outer `jit_scope(compile_ops=False)`: python # In the example below, only x will be compiled. with tf.xla.experimental.jit_scope(compile_ops=False): with tf.xla.experimental.jit_scope(): x = tf.matmul(a, b) y = tf.matmul(c, d) loss = x + y Args: compile_ops: Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter `node_def` and returns a python bool. separate_compiled_gradients: If true put each gradient subgraph into a separate compilation scope. This gives fine-grained control over which portions of the graph will be compiled as",torch._functorch.make_functional.make_functional_with_buffers,"make_functional_with_buffers(model, disable_autograd_tracking=False) -> func, params, buffers Given a ``torch.nn.Module``, make_functional_with_buffers extracts the state (params and buffers) and returns a functional version of the model ``func`` that can be invoked like a function. ``func`` can be invoked as follows: .. code-block:: python import torch import torch.nn as nn from functorch import make_functional_with_buffers x = torch.randn(4, 3) model = nn.Linear(3, 3) func, params, buffers = make_functional_with_buffers(model) func(params, buffers, x) And here is an example of applying the grad transform over the parameters of a model: .. code-block:: python import torch import torch.nn as nn from functorch import make_functional_with_buffers, grad x = torch.randn(4, 3) t = torch.randn(4, 3) model = nn.Linear(3, 3) func, params, buffers = make_functional_with_buffers(model) def compute_loss(params, buffers, x, t): y = func(params, buffers, x) return nn.functional.mse_loss(y, t) grad_weights = grad(compute_loss)(params, buffers, x, t) Args: model (torch.nn.Module): Input model. disable_autograd_tracking (bool): Flag to disable gradients tracking for output parameters. The returned params are unrelated to the set of params from the original model. If False (default), the params will have ``requires_grad=True`` on them (aka they will be trackable with regular PyTorch autograd), matching the requires_grad-ness of the params from the original model. Otherwise, the returned params will have ``requires_grad=False``. Default, False. If you plan on using regular PyTorch autograd (e.g., if you want to call ``.backward()`` or ``torch.autograd.grad()``, then set ``disable_autograd_tracking=False``. Otherwise, if you're only planning on using functorch's gradient transforms, then please set ``disable_autograd_tracking=True`` to avoid unnecessarily tracking history with PyTorch autograd.",0.9574275
tensorflow.python.ops.linalg.linear_operator_util.split_arg_into_blocks,"Split `x` into blocks matching `operators`'s `domain_dimension`. Specifically, if we have a blockwise lower-triangular matrix, with block sizes along the diagonal `[M_j, M_j] j = 0,1,2..J`, this method splits `arg` on `axis` into `J` tensors, whose shape at `axis` is `M_j`. Args: block_dims: Iterable of `TensorShapes`. block_dims_fn: Callable returning an iterable of `Tensor`s. arg: `Tensor`. `arg` is split into `J` tensors. axis: Python `Integer` representing the axis to split `arg` on. Returns: A list of `Tensor`s.",torch.distributed._spmd.api.transform,"Given a DTensor-expanded graph and sharding schema for every node, conduct additional transformation for the sub-graph from the :class:`nn.Module` returned by :meth:`torch.distributed._spmd.Override.replacement` if necessary. Args: gm (:class:`fx.Graph`): a DTensor-expanded graph. flat_state (List[str, :class:`Tensor`]): a reference to the list of flattened state. The elements in ``flat_state`` map to the first ``len(flat_state)`` placeholders in the graph. The transformation can add state to or remove state from ``flat_state`` as long as it keeps ``flat_state`` and the placeholders consistent. Returns: The :class:`fx.Graph` after transformation.",0.95741916
tensorflow.python.keras.utils.generic_utils.update,"Updates the progress bar. Args: current: Index of current step. values: List of tuples: `(name, value_for_last_step)`. If `name` is in `stateful_metrics`, `value_for_last_step` will be displayed as-is. Else, an average of the metric over time will be displayed. finalize: Whether this is the last update for the progress bar. If `None`, defaults to `current >= self.target`.",torch.nn.functional.dropout,"During training, randomly zeroes some elements of the input tensor with probability :attr:`p`. Uses samples from a Bernoulli distribution. See :class:`~torch.nn.Dropout` for details. Args: p: probability of an element to be zeroed. Default: 0.5 training: apply dropout if is ``True``. Default: ``True`` inplace: If set to ``True``, will do this operation in-place. Default: ``False``",0.95739555
tensorflow.python.ops.control_flow_v2_toggles.control_flow_v2_enabled,Returns `True` if v2 control flow is enabled. Note: v2 control flow is always enabled inside of tf.function.,torch.nn.modules.module.bfloat16,Casts all floating point parameters and buffers to ``bfloat16`` datatype. .. note:: This method modifies the module in-place. Returns: Module: self,0.9573279
tensorflow.python.tools.selective_registration_header_lib.get_header_from_ops_and_kernels,"Returns a header for use with tensorflow SELECTIVE_REGISTRATION. Args: ops_and_kernels: a set of (op_name, kernel_class_name) pairs to include. include_all_ops_and_kernels: if True, ops_and_kernels is ignored and all op kernels are included. Returns: the string of the header that should be written as ops_to_register.h.",torch.testing._internal.dist_utils.get_function_event,Returns the first event that matches partial_event_name in the provided function_events. These function_events should be the output of torch.autograd.profiler.function_events(). Args: function_events: function_events returned by the profiler. event_name (str): partial key that the event was profiled with.,0.9572972
tensorflow.python.ops.parallel_for.gradients.jacobian,"Computes jacobian of `output` w.r.t. `inputs`. Args: output: A tensor. inputs: A tensor or a nested structure of tensor objects. use_pfor: If true, uses pfor for computing the jacobian. Else uses tf.while_loop. parallel_iterations: A knob to control how many iterations and dispatched in parallel. This knob can be used to control the total memory usage. Returns: A tensor or a nested structure of tensors with the same structure as `inputs`. Each entry is the jacobian of `output` w.r.t. to the corresponding value in `inputs`. If output has shape [y_1, ..., y_n] and inputs_i has shape [x_1, ..., x_m], the corresponding jacobian has shape [y_1, ..., y_n, x_1, ..., x_m]. Note that in cases where the gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future.",torch.autograd.function.backward,"Define a formula for differentiating the operation with backward mode automatic differentiation. This function is to be overridden by all subclasses. (Defining this function is equivalent to defining the ``vjp`` function.) It must accept a context :attr:`ctx` as the first argument, followed by as many outputs as the :func:`forward` returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to :func:`forward`. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple of booleans representing whether each input needs gradient. E.g., :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the first input to :func:`forward` needs gradient computed w.r.t. the output.",0.95726633
tensorflow.python.util.protobuf.compare.assertProtoEqual,"Fails with a useful error if a and b aren't equal. Comparison of repeated fields matches the semantics of unittest.TestCase.assertEqual(), ie order and extra duplicates fields matter. Args: self: googletest.TestCase a: proto2 PB instance, or text string representing one. b: proto2 PB instance -- message.Message or subclass thereof. check_initialized: boolean, whether to fail if either a or b isn't initialized. normalize_numbers: boolean, whether to normalize types and precision of numbers before comparison. msg: if specified, is used as the error message on failure. relative_tolerance: float, relative tolerance. If this is not provided, then all floats are compared using string comparison otherwise, floating point comparisons are done using the relative tolerance provided.",torch.onnx._internal.fx.onnxfunction_dispatcher.perfect_match_inputs,"Check if the inputs perfectly match the OpSchema requirements. The definition of perfect match is that the input types are all in the type constraints and the number of inputs matches the number of inputs in the OpSchema. Checking steps: 1. The function signature matches the inputs number, and attribute names. 2. The input/attribute types are all in the type constraints. A function should at least pass the first step to be eligible for the nearest matching. Args: diagnostic: The diagnostic to use for logging detailed info. args: The input arguments organized in PyTorch inputs way. kwargs: The input keyword arguments organized in PyTorch inputs way. Returns: True if the inputs match the requirements, False otherwise.",0.9572406
tensorflow.python.framework.test_util.assertRaisesWithPredicateMatch,"Returns a context manager to enclose code expected to raise an exception. If the exception is an OpError, the op stack is also included in the message predicate search. Args: exception_type: The expected type of exception that should be raised. expected_err_re_or_predicate: If this is callable, it should be a function of one argument that inspects the passed-in exception and returns True (success) or False (please fail the test). Otherwise, the error message is expected to match this regular expression partially. Returns: A context manager to surround code that is expected to raise an exception.",torch.onnx._internal.exporter.is_registered_op,"Returns whether the given op is registered: torch.ops.<namespace>.<op_name>.<overload>. Args: namespace: The namespace of the operator to check. op_name: The name of the operator to check. overload: The overload of the operator to check. If it's default overload, leave it to None. Returns: True if the given op is registered, otherwise False.",0.9572344
tensorflow.python.tpu.tensor_tracer_flags.use_test_undeclared_outputs_dir,Decides the output directory of the report and trace files. Args: None. Returns: True if the output files should be written to the test-undeclared-outputs-directory defined via an env variable.,torch.onnx.verification.validate,"Run the ONNX test case with options.backend, and compare with the expected outputs. Args: options: Options for validation. Raise: AssertionError: if outputs from options.backend and expected outputs are not equal up to specified precision.",0.95719206
tensorflow.python.types.distribute.get_next_as_optional,"Returns a `tf.experimental.Optional` that contains the next value for all replicas. If the `tf.distribute.DistributedIterator` has reached the end of the sequence, the returned `tf.experimental.Optional` will have no value. Example usage: >>> strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""]) >>> global_batch_size = 2 >>> steps_per_loop = 2 >>> dataset = tf.data.Dataset.range(10).batch(global_batch_size) >>> distributed_iterator = iter( ... strategy.experimental_distribute_dataset(dataset)) >>> def step_fn(x): ... # train the model with inputs ... return x >>> @tf.function ... def train_fn(distributed_iterator): ... for _ in tf.range(steps_per_loop): ... optional_data = distributed_iterator.get_next_as_optional() ... if not optional_data.has_value(): ... break ... per_replica_results = strategy.run(step_fn, args=(optional_data.get_value(),)) ... tf.print(strategy.experimental_local_results(per_replica_results)) >>> train_fn(distributed_iterator) ... # ([0 1], [2 3]) ... # ([4], []) Returns: An `tf.experimental.Optional` object representing the next value from the `tf.distribute.DistributedIterator` (if it has one) or no value.",torch.onnx._internal.exporter.serialize,"Protocol method that must be implemented for serialization. Args: onnx_program: Represents the in-memory exported ONNX model destination: A binary IO stream or pre-allocated buffer into which the serialized model should be written. Example: A simple serializer that writes the exported :py:obj:`onnx.ModelProto` in Protobuf format to ``destination``: :: # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX) >>> import io >>> import torch >>> import torch.onnx >>> class MyModel(torch.nn.Module): # Dummy model ... def __init__(self) -> None: ... super().__init__() ... self.linear = torch.nn.Linear(2, 2) ... def forward(self, x): ... out = self.linear(x) ... return out >>> class ProtobufONNXProgramSerializer: ... def serialize( ... self, onnx_program: torch.onnx.ONNXProgram, destination: io.BufferedIOBase ... ) -> None: ... destination.write(onnx_program.model_proto.SerializeToString()) >>> model = MyModel() >>> arg1 = torch.randn(2, 2, 2) # positional input 1 >>> torch.onnx.dynamo_export(model, arg1).save( ... destination=""exported_model.onnx"", ... serializer=ProtobufONNXProgramSerializer(), ... )",0.9571844
tensorflow.python.data.ops.dataset_ops.element_to_bucket_id,Return int64 id of the length bucket for this element.,torch.amp.grad_scaler.get_growth_interval,Return a Python int containing the growth interval.,0.9571795
tensorflow.python.tpu.tensor_tracer.is_enabled,Returns True if TensorTracer is enabled.,torch.distributed._shard.sharded_tensor.api.is_pinned,Returns True if the sharded tensor (each local shard) resides in pinned memory.,0.9571504
tensorflow.python.ops.nn_impl.sufficient_statistics,"Calculate the sufficient statistics for the mean and variance of `x`. These sufficient statistics are computed using the one pass algorithm on an input that's optionally shifted. See: https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data For example: >>> t = [[1, 2, 3], [4, 5, 6]] >>> sufficient_statistics(t, [1]) (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 6, 15], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([14, 77], dtype=int32)>, None) >>> sufficient_statistics(t, [-1]) (<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 6, 15], dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([14, 77], dtype=int32)>, None) Args: x: A `Tensor`. axes: Array of ints. Axes along which to compute mean and variance. As in Python, the axes can also be negative numbers. A negative axis is interpreted as counting from the end of the rank, i.e., axis + rank(values)-th dimension. shift: A `Tensor` containing the value by which to shift the data for numerical stability, or `None` if no shift is to be performed. A shift close to the true mean provides the most numerically stable results. keep_dims: produce statistics with the same dimensionality as the input. name: Name used to scope the operations that compute the sufficient stats. keepdims: Alias for keep_dims. Returns: Four `Tensor` objects of the same type as `x`: * the count (number of elements to average over). * the (possibly shifted) sum of the elements in the array. * the (possibly shifted) sum of squares of the elements in the array. * the shift by which the mean must be corrected or None if `shift` is None.",torch.distributed.distributed_c10d.reduce_scatter_tensor,"Reduces, then scatters a tensor to all ranks in a group. Args: output (Tensor): Output tensor. It should have the same size across all ranks. input (Tensor): Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of ""concatenation"", see ``torch.cat()``. For definition of ""stack"", see ``torch.stack()``. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op. Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group. Examples: >>> # xdoctest: +SKIP(""need process group init"") >>> # All tensors below are of torch.int64 dtype and on CUDA devices. >>> # We have two ranks. >>> device = torch.device(f'cuda:{rank}') >>> tensor_out = torch.zeros(2, dtype=torch.int64, device=device) >>> # Input in concatenation form >>> tensor_in = torch.arange(world_size * 2, dtype=torch.int64, device=device) >>> tensor_in tensor([0, 1, 2, 3], device='cuda:0') # Rank 0 tensor([0, 1, 2, 3], device='cuda:1') # Rank 1 >>> dist.reduce_scatter_tensor(tensor_out, tensor_in) >>> tensor_out tensor([0, 2], device='cuda:0') # Rank 0 tensor([4, 6], device='cuda:1') # Rank 1 >>> # Input in stack form >>> tensor_in = torch.reshape(tensor_in, (world_size, 2)) >>> tensor_in tensor([[0, 1], [2, 3]], device='cuda:0') # Rank 0 tensor([[0, 1], [2, 3]], device='cuda:1') # Rank 1 >>> dist.reduce_scatter_tensor(tensor_out, tensor_in) >>> tensor_out tensor([0, 2], device='cuda:0') # Rank 0 tensor([4, 6], device='cuda:1') # Rank 1 .. warning:: The Gloo backend does not support this API.",0.9570793
tensorflow.python.ops.op_selector.get_unique_graph,"Return the unique graph used by the all the elements in tops. Args: tops: iterable of elements to check (usually a list of tf.Operation and/or tf.Tensor). Or a tf.Graph. check_types: check that the element in tops are of given type(s). If None, the types (tf.Operation, tf.Tensor) are used. none_if_empty: don't raise an error if tops is an empty list, just return None. Returns: The unique graph used by all the tops. Raises: TypeError: if tops is not a iterable of tf.Operation. ValueError: if the graph is not unique.",torch.distributed.distributed_c10d.reduce_scatter,"Reduces, then scatters a list of tensors to all processes in a group. Args: output (Tensor): Output tensor. input_list (list[Tensor]): List of tensors to reduce and scatter. op (optional): One of the values from ``torch.distributed.ReduceOp`` enum. Specifies an operation used for element-wise reductions. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. async_op (bool, optional): Whether this op should be an async op. Returns: Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.",0.9570668
tensorflow.python.feature_column.serialization.deserialize_feature_column,"Deserializes a `config` generated with `serialize_feature_column`. This method should only be used to deserialize parent FeatureColumns when implementing FeatureColumn.from_config(), else deserialize_feature_columns() is preferable. Returns a FeatureColumn for this config. Args: config: A Dict with the serialization of feature columns acquired by `serialize_feature_column`, or a string representing a raw column. custom_objects: A Dict from custom_object name to the associated keras serializable objects (FeatureColumns, classes or functions). columns_by_name: A Dict[String, FeatureColumn] of existing columns in order to avoid duplication. Raises: ValueError if `config` has invalid format (e.g: expected keys missing, or refers to unknown classes). Returns: A FeatureColumn corresponding to the input `config`.",torch.package.importer.get_name,"Given an object, return a name that can be used to retrieve the object from this environment. Args: obj: An object to get the module-environment-relative name for. name: If set, use this name instead of looking up __name__ or __qualname__ on `obj`. This is only here to match how Pickler handles __reduce__ functions that return a string, don't use otherwise. Returns: A tuple (parent_module_name, attr_name) that can be used to retrieve `obj` from this environment. Use it like: mod = importer.import_module(parent_module_name) obj = getattr(mod, attr_name) Raises: ObjNotFoundError: we couldn't retrieve `obj by name. ObjMisMatchError: we found a different object with the same name as `obj`.",0.9570621
tensorflow.python.ops.summary_ops_v2.trace_on,"Starts a trace to record computation graphs and profiling information. Must be invoked in eager mode. When enabled, TensorFlow runtime will collect information that can later be exported and consumed by TensorBoard. The trace is activated across the entire TensorFlow runtime and affects all threads of execution. To stop the trace and export the collected information, use `tf.summary.trace_export`. To stop the trace without exporting, use `tf.summary.trace_off`. Args: graph: If True, enables collection of executed graphs. It includes ones from tf.function invocation and ones from the legacy graph mode. The default is True. profiler: If True, enables the advanced profiler. Enabling profiler implicitly enables the graph collection. The profiler may incur a high memory overhead. The default is False. profiler_outdir: Output directory for profiler. It is required when profiler is enabled when trace was started. Otherwise, it is ignored.",torch.profiler.profiler.supported_activities,"Returns a set of supported profiler tracing activities. Note: profiler uses CUPTI library to trace on-device CUDA kernels. In case when CUDA is enabled but CUPTI is not available, passing ``ProfilerActivity.CUDA`` to profiler results in using the legacy CUDA profiling code (same as in the legacy ``torch.autograd.profiler``). This, in turn, results in including CUDA time in the profiler table output, but not in the JSON trace.",0.957026
tensorflow.python.distribute.tpu_util.outside_or_skip_tpu_context,Returns a context manager that skips current enclosing context if there is any.,torch.export.exported_program.buffers,Returns an iterator over original module buffers.,0.9570044
tensorflow.python.ops.check_ops.assert_near_v2,"Assert the condition `x` and `y` are close element-wise. This Op checks that `x[i] - y[i] < atol + rtol * tf.abs(y[i])` holds for every pair of (possibly broadcast) elements of `x` and `y`. If both `x` and `y` are empty, this is trivially satisfied. If any elements of `x` and `y` are not close, `message`, as well as the first `summarize` entries of `x` and `y` are printed, and `InvalidArgumentError` is raised. The default `atol` and `rtol` is `10 * eps`, where `eps` is the smallest representable positive number such that `1 + eps != 1`. This is about `1.2e-6` in `32bit`, `2.22e-15` in `64bit`, and `0.00977` in `16bit`. See `numpy.finfo`. Args: x: Float or complex `Tensor`. y: Float or complex `Tensor`, same dtype as and broadcastable to `x`. rtol: `Tensor`. Same `dtype` as, and broadcastable to, `x`. The relative tolerance. Default is `10 * eps`. atol: `Tensor`. Same `dtype` as, and broadcastable to, `x`. The absolute tolerance. Default is `10 * eps`. message: A string to prefix to the default message. summarize: Print this many entries of each tensor. name: A name for this operation (optional). Defaults to ""assert_near"". Returns: Op that raises `InvalidArgumentError` if `x` and `y` are not close enough. This can be used with `tf.control_dependencies` inside of `tf.function`s to block followup computation until the check has executed. @compatibility(eager) returns None @end_compatibility Raises: InvalidArgumentError: if the check can be performed immediately and `x != y` is False for any pair of elements in `x` and `y`. The check can be performed immediately during eager execution or if `x` and `y` are statically known. @compatibility(numpy) Similar to `numpy.testing.assert_allclose`, except tolerance depends on data type. This is due to the fact that `TensorFlow` is often used with `32bit`, `64bit`, and even `16bit` data. @end_compatibility",torch.nn.utils.rnn.pack_padded_sequence,"Packs a Tensor containing padded sequences of variable length. :attr:`input` can be of size ``T x B x *`` where ``T`` is the length of the longest sequence, ``B`` is the batch size, and ``*`` is any number of dimensions (including 0). If :attr:`batch_first` is ``False``, ``T x B x *`` :attr:`input` is expected, ``B x T x *`` otherwise. For unsorted sequences, use `enforce_sorted = False`. If :attr:`enforce_sorted` is ``True``, the sequences should be sorted by length in a decreasing order, i.e. ``input[:,0]`` should be the longest sequence, and ``input[:,B-1]`` the shortest one. `enforce_sorted = True` is only necessary for ONNX export. Note: This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a :class:`PackedSequence` object by accessing its ``.data`` attribute. Args: input (Tensor): padded batch of variable length sequences. lengths (Tensor or list(int)): list of sequence lengths of each batch element (must be on the CPU if provided as a tensor). batch_first (bool, optional): if ``True``, the input is expected in ``B x T x *`` format, ``T x B x *`` otherwise. enforce_sorted (bool, optional): if ``True``, the input is expected to contain sequences sorted by length in a decreasing order. If ``False``, the input will get sorted unconditionally. Default: ``True``. Returns: a :class:`PackedSequence` object",0.9569855
tensorflow.python.keras.saving.saved_model.load.get_common_shape,Find a `TensorShape` that is compatible with both `x` and `y`.,torch._export.serde.serialize.serialize_tensor_meta,Extract a TensorMeta describing `t`.,0.9569722
tensorflow.python.distribute.multi_worker_util.is_chief,"Returns whether the given task is chief in the cluster. Since there is at most one evaluator and the evaluator itself should be independent of the training cluster, the evaluator job is also a chief job on its own. If this is currently running under a `_WorkerContext` of distribute coordinator, the arguments can be omitted as the result is already available. Args: cluster_spec: a dict, `ClusterDef` or `ClusterSpec` object specifying the cluster configurations. task_type: the task type in the cluster. task_id: the task id in the cluster. Returns: a boolean indicating whether the given task is chief. Raises: ValueError: if `task_type` is not in the `cluster_spec` or `task_id` exceeds the maximum id of the `task_type`.",torch.distributed.algorithms.join.notify_join_context,"Notifies the join context manager that the calling process has not yet joined. Then, if ``throw_on_early_termination=True``, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so. This method should be called from a :class:`Joinable` object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in :class:`DistributedDataParallel`. Only the first :class:`Joinable` object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous. Arguments: joinable (Joinable): the :class:`Joinable` object calling this method. Returns: An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if ``joinable`` is the first one passed into the context manager; ``None`` otherwise.",0.95696867
tensorflow.python.framework.tensor.placeholder_value,Generates a graph placholder with the given TensorSpec information.,torch.distributed._tensor.examples.checkpoint_example.gen_tensor_parallel_model,generates a nn.Module where parameters are sharded in the tensor-parallel fashion.,0.9569685
tensorflow.python.tpu.tensor_tracer_flags.get_signature_to_agg_fn_map,Returns a map that contains the aggregate function for each signature.,torch.onnx._internal.fx.passes.modularization.stack_trace,Returns the stack trace associated with this node.,0.956944
tensorflow.python.platform.sysconfig.get_link_flags,"Returns the linker flags for linking with TensorFlow. The returned list of arguments can be passed to the linker for linking against TensorFlow. The result is platform dependent. For example, on a typical Linux system with Python 3.7 the following command prints `['-L/usr/local/lib/python3.7/dist-packages/tensorflow', '-l:libtensorflow_framework.so.2']` >>> print(tf.sysconfig.get_link_flags()) Returns: A list of strings for the linker flags.",torch.fx.interpreter.placeholder,"Execute a ``placeholder`` node. In ``Transformer``, this is overridden to insert a new ``placeholder`` into the output graph. Args: target (Target): The call target for this node. See `Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for details on semantics args (Tuple): Tuple of positional args for this invocation kwargs (Dict): Dict of keyword arguments for this invocation",0.95690894
tensorflow.python.ops.gradient_checker.compute_gradient_error,"Computes the gradient error. Computes the maximum error for dy/dx between the computed Jacobian and the numerically estimated Jacobian. This function will modify the tensors passed in as it adds more operations and hence changing the consumers of the operations of the input tensors. This function adds operations to the current session. To compute the error using a particular device, such as a GPU, use the standard methods for setting a device (e.g. using with sess.graph.device() or setting a device function in the session constructor). Args: x: a tensor or list of tensors x_shape: the dimensions of x as a tuple or an array of ints. If x is a list, then this is the list of shapes. y: a tensor y_shape: the dimensions of y as a tuple or an array of ints. x_init_value: (optional) a numpy array of the same shape as ""x"" representing the initial value of x. If x is a list, this should be a list of numpy arrays. If this is none, the function will pick a random tensor as the initial value. delta: (optional) the amount of perturbation. init_targets: list of targets to run to initialize model params. extra_feed_dict: dict that allows fixing specified tensor values during the Jacobian calculation. Returns: The maximum error in between the two Jacobians.",torch._inductor.codegen.cuda.gemm_template.render_gemm_arguments,"Render the Cutlass CUDA C++ code required for passing arguments to the GEMM operation. Args: argument_template (str): Template for the GEMM operation arguments. epilogue_template (str): Template for the epilogue arguments. should_swap_xw (bool): Determines whether X, W operands should be swapped. If True, applies an explicit transpose operation to X and W. X (IRNode): The X input tensor. W (IRNode): The W input tensor. Bias (IRNode): The bias tensor. Y (IRNode): The output tensor. alpha (float): Scaling factor for the product of the inputs. beta (float): Scaling factor for the output tensor. kernel (CUDATemplateKernel): CUDA Template kernel for the operation. epilogue_args (any): Additional arguments for the epilogue state. Returns: str: A block of CUDA C++ code as a string, ready to be used as arguments for the GEMM operation. Note: If `should_swap_xw` is True, a transpose operation will be applied to the X, W, Bias, and Y tensors. This operation also implies the M and N dimensions of Bias and GEMM output to be swapped before the function call.",0.9569065
tensorflow.python.training.sync_replicas_optimizer.variables,Fetches a list of optimizer variables in the default graph. This wraps `variables()` from the actual optimizer. It does not include the `SyncReplicasOptimizer`'s local step. Returns: A list of variables.,torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook.zero_step,Perform partial :class:`ZeroRedundancyOptimizer` :meth:`step` using gradients in the :class:`DistributedDataParallel`. Returns: A :class:`torch.Tensor` representing the contents of the gradient bucket.,0.95687306
tensorflow.python.keras.utils.generic_utils.validate_kwargs,Checks that all keyword arguments are in the set of allowed keys.,torch.onnx._internal.registration.in_base,Checks if a key is in the base dictionary.,0.9568679
tensorflow.python.framework.tensor_conversion.convert_to_tensor_v2_with_dispatch,"Converts the given `value` to a `Tensor`. This function converts Python objects of various types to `Tensor` objects. It accepts `Tensor` objects, numpy arrays, Python lists, and Python scalars. For example: >>> import numpy as np >>> def my_func(arg): ... arg = tf.convert_to_tensor(arg, dtype=tf.float32) ... return arg >>> # The following calls are equivalent. ... >>> value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]])) >>> print(value_1) tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) >>> value_2 = my_func([[1.0, 2.0], [3.0, 4.0]]) >>> print(value_2) tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) >>> value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)) >>> print(value_3) tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) This function can be useful when composing a new operation in Python (such as `my_func` in the example above). All standard Python op constructors apply this function to each of their Tensor-valued inputs, which allows those ops to accept numpy arrays, Python lists, and scalars in addition to `Tensor` objects. Note: This function diverges from default Numpy behavior for `float` and `string` types when `None` is present in a Python list or scalar. Rather than silently converting `None` values, an error will be thrown. Args: value: An object whose type has a registered `Tensor` conversion function. dtype: Optional element type for the returned tensor. If missing, the type is inferred from the type of `value`. dtype_hint: Optional element type for the returned tensor, used when dtype is None. In some cases, a caller may not have a dtype in mind when converting to a tensor, so dtype_hint can be used as a soft preference. If the conversion to `dtype_hint` is not possible, this argument has no effect. name: Optional name to use if a new `Tensor` is created. Returns: A `Tensor` based on `value`. Raises: TypeError: If no conversion function is registered for `value` to `dtype`. RuntimeError: If a regis",torch.nn.utils.init.skip_init,"Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. This can be useful if initialization is slow or if custom initialization will be performed, making the default initialization unnecessary. There are some caveats to this, due to the way this function is implemented: 1. The module must accept a `device` arg in its constructor that is passed to any parameters or buffers created during construction. 2. The module must not perform any computation on parameters in its constructor except initialization (i.e. functions from :mod:`torch.nn.init`). If these conditions are satisfied, the module can be instantiated with parameter / buffer values uninitialized, as if having been created using :func:`torch.empty`. Args: module_cls: Class object; should be a subclass of :class:`torch.nn.Module` args: args to pass to the module's constructor kwargs: kwargs to pass to the module's constructor Returns: Instantiated module with uninitialized parameters / buffers Example:: >>> # xdoctest: +IGNORE_WANT(""non-deterministic"") >>> import torch >>> m = torch.nn.utils.skip_init(torch.nn.Linear, 5, 1) >>> m.weight Parameter containing: tensor([[0.0000e+00, 1.5846e+29, 7.8307e+00, 2.5250e-29, 1.1210e-44]], requires_grad=True) >>> m2 = torch.nn.utils.skip_init(torch.nn.Linear, in_features=6, out_features=1) >>> m2.weight Parameter containing: tensor([[-1.4677e+24, 4.5915e-41, 1.4013e-45, 0.0000e+00, -1.4677e+24, 4.5915e-41]], requires_grad=True)",0.9568213
tensorflow.python.ops.linalg.linear_operator_util.base_dtype,Returns a non-reference `dtype` based on this `dtype`.,torch.distributed.elastic.rendezvous.api.get_as_bool,Return the value for ``key`` as a ``bool``.,0.956752
tensorflow.python.checkpoint.checkpoint_management.checkpoint_exists_internal,"Checks whether a V1 or V2 checkpoint exists with the specified prefix. This is an internal function to check if a checkpoint exists, since it takes into account the naming difference between V1 and V2 formats. Args: checkpoint_prefix: the prefix of a V1 or V2 checkpoint, with V2 taking priority. Typically the result of `Saver.save()` or that of `tf.train.latest_checkpoint()`, regardless of sharded/non-sharded or V1/V2. Returns: A bool, true if a checkpoint referred to by `checkpoint_prefix` exists.",torch.distributed.checkpoint.storage.reset,"Calls to indicates a brand new checkpoint write is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint write. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage. Args: checkpoint_id (Union[str, os.PathLike, None]): The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: ``None``)",0.9567348
tensorflow.python.eager.execute.execute_with_cancellation,"Execute a TensorFlow operation. Args: op_name: Name of the TensorFlow operation (see REGISTER_OP in C++ code) to execute. num_outputs: The number of outputs of the operation to fetch. (Explicitly provided instead of being inferred for performance reasons). inputs: A list of inputs to the operation. Each entry should be a Tensor, or a value which can be passed to the Tensor constructor to create one. attrs: A tuple with alternating string attr names and attr values for this operation. ctx: The value of context.context(). cancellation_manager: a `CancellationManager` object that can be used to cancel the operation. name: Customized name for the operation. Returns: List of output Tensor objects. The list is empty if there are no outputs Raises: An exception on error.",torch._inductor.select_algorithm.store_output,"Stores the final output and appends any epilogue fusions if the buffer hasn't been optimized away. Args: indices (Union[List, Tuple]): The index for each dimension of the output. The dot product of these indices and output strides must match `val`. val (str): The value to store. mask (Optional[str]): An optional mask to use for the store operation. If provided, this mask will be applied to the store. indent_width (int): The number of spaces to use for indentation. This is used when the call to store_output is indented in the kernel definition.",0.95671415
tensorflow.python.ops.distributions.bijector_impl.graph_parents,Returns this `Bijector`'s graph_parents as a Python list.,torch.fx.graph_module.code,Return the Python code generated from the ``Graph`` underlying this ``GraphModule``.,0.95664114
tensorflow.python.ops.ragged.ragged_factory_ops.placeholder,"Creates a placeholder for a `tf.RaggedTensor` that will always be fed. **Important**: This ragged tensor will produce an error if evaluated. Its value must be fed using the `feed_dict` optional argument to `Session.run()`, `Tensor.eval()`, or `Operation.run()`. Args: dtype: The data type for the `RaggedTensor`. ragged_rank: The ragged rank for the `RaggedTensor` value_shape: The shape for individual flat values in the `RaggedTensor`. name: A name for the operation (optional). Returns: A `RaggedTensor` that may be used as a handle for feeding a value, but not evaluated directly. Raises: RuntimeError: if eager execution is enabled @compatibility(TF2) This API is not compatible with eager execution and `tf.function`. To migrate to TF2, rewrite the code to be compatible with eager execution. Check the [migration guide](https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls) on replacing `Session.run` calls. In TF2, you can just pass tensors directly into ops and layers. If you want to explicitly set up your inputs, also see [Keras functional API](https://www.tensorflow.org/guide/keras/functional) on how to use `tf.keras.Input` to replace `tf.compat.v1.ragged.placeholder`. `tf.function` arguments also do the job of `tf.compat.v1.ragged.placeholder`. For more details please read [Better performance with tf.function](https://www.tensorflow.org/guide/function). @end_compatibility",torch._custom_op.impl.impl_backward,"Registers a backward formula. WARNING: if you're a user, please do not use this directly (instead use the torch._custom_ops APIs). Also please see the following for a detailed guide on custom ops. https://docs.google.com/document/d/1aGWtgxV3HppuxQAdddyPrs74_aEntpkYt9MalnCKnhk In order for the CustomOp to work with autograd, you need to register a backward formula. There are two pieces to this: 1. You must give us a function to specify what to save for backward. Call this the ""save for backward"" function. 2. You must give us a function that computes gradients. Call this the ""backward"" function. Use `impl_save_for_backward` to define a ""save for backward"" function that specifies what gets saved for backward. The function should accept two arguments ``(inputs, output)`` and return the quantities to be saved for backward. During runtime, when you call the CustomOp, PyTorch will invoke the ""save for backward"" function with the inputs and output of the CustomOp. Use `impl_backward` to define the ""backward"" function. The backward function must accept ``(ctx, saved, *grads)``: - ``ctx`` is a context object where we may provide information - ``saved`` is exactly what gets returned from the ""save for backward"" function - ``grads`` is one or more gradients. The number of gradients matches the number of outputs of the CustomOp. The backward function must return a dict that maps the name of an input to the CustomOp to its corresponding gradient. All inputs that were declared to be Tensors in the CustomOp definition must be accounted for in the dict. The gradient may be a Tensor or None.",0.95663816
tensorflow.python.eager.polymorphic_function.concrete_function.record,"Record the function call operation. For backprop, indicates the backward function to use and which new Tensors must be watched. For forwardprop from eager, the function call itself will have produced tangents which need to be recorded. Args: flat_outputs: The result of running `forward`. inference_args: A flat list of Tensors with inference inputs to the operation. input_tangents: A flat list of Tensors with input tangents consumed by the operation.",torch.distributed.pipelining.schedules.step,"Run one iteration of the pipeline schedule with *whole-batch* input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation. args: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch.",0.95660526
tensorflow.python.saved_model.load.load_partial,"Partially load a SavedModel (saved from V2). Similar to `tf.saved_model.load`, but with an additional argument that lets you specify which nodes to load. `tf.saved_model.load_partial(export_dir, [""root""])` and `tf.saved_model.load(export_dir)` are equivalent. Note: This only works for SavedModels saved with TensorFlow V2 from `tf.saved_model.save` or Keras. In Tensorflow V2, SavedModel stores the **object graph** of the saved object. The graph contains nodes (`tf.Module`, `tf.Variable`, `tf.function`, Keras layers, etc.) and edges that are the name of the attributes connecting the objects. *Example 1* model = tf.Module() model.child_layer = tf.Module() model.child_layer.v = tf.Variable(5.) tf.saved_model.save(model, '/tmp/model') loaded = tf.__internal__.saved_model.load_partial( ... '/tmp/model', ... ['root.child_layer', 'root.child_layer.v']) loaded['root.child_layer'].v.numpy() 5. loaded['root.child_layer'].v is loaded['root.child_layer.v'] True *Example 2* model = tf.Module() model.child_layer = tf.Module() model.child_layer.v = tf.Variable(5.) >>> tf.saved_model.save(model, '/tmp/model') # Create a variable new_variable = tf.Variable(0.) loaded = tf.__internal__.saved_model.load_partial( ... '/tmp/model', ... {'root.child_layer': None, 'root.child_layer.v': new_variable}) loaded['root.child_layer'].v.numpy() 5. new_variable.numpy() 5. **Loading under different distribution strategies** You can load different parts of the model under different distribution strategies. Note that this is very experimental so use with care. model = tf.Module() model.layer_1 = tf.Module() model.layer_1.v = tf.Variable(5.) model.layer_2 = tf.Module() model.layer_2.v = tf.Variable(7.) tf.saved_model.save(model, '/tmp/model') # Load with no strategy loaded = tf.__internal__.saved_model.load_partial( ... '/tmp/model', ... ['root.layer_1']) loaded['root.layer_1'].v <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0> strategy = tf.distribute.MirroredStrategy() with strategy.scope",torch.onnx.verification.find_mismatch,"Find all mismatches between the original model and the exported model. Experimental. The API is subject to change. This tool helps debug the mismatch between the original PyTorch model and exported ONNX model. It binary searches the model graph to find the minimal subgraph that exhibits the mismatch. Args: model: The model to be exported. input_args: The input arguments to the model. do_constant_folding: Same as `do_constant_folding` in :func:`torch.onnx.export`. training: Same as `training` in :func:`torch.onnx.export`. opset_version: Same as `opset_version` in :func:`torch.onnx.export`. keep_initializers_as_inputs: Same as `keep_initializers_as_inputs` in :func:`torch.onnx.export`. verbose: Same as `verbose` in :func:`torch.onnx.export`. options: The options for the mismatch verification. Returns: A GraphInfo object that contains the mismatch information. Example:: >>> import torch >>> import torch.onnx.verification >>> torch.manual_seed(0) >>> opset_version = 15 >>> # Define a custom symbolic function for aten::relu. >>> # The custom symbolic function is incorrect, which will result in mismatches. >>> def incorrect_relu_symbolic_function(g, self): ... return self >>> torch.onnx.register_custom_op_symbolic( ... ""aten::relu"", ... incorrect_relu_symbolic_function, ... opset_version=opset_version, ... ) >>> class Model(torch.nn.Module): ... def __init__(self): ... super().__init__() ... self.layers = torch.nn.Sequential( ... torch.nn.Linear(3, 4), ... torch.nn.ReLU(), ... torch.nn.Linear(4, 5), ... torch.nn.ReLU(), ... torch.nn.Linear(5, 6), ... ) ... def forward(self, x): ... return self.layers(x) >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_ONNX) >>> graph_info = torch.onnx.verification.find_mismatch( ... Model(), ... (torch.randn(2, 3),), ... opset_version=opset_version, ... ) ===================== Mismatch info for graph partition : ====================== ================================ Mismatch error ================================ Tensor-likes are not close! M",0.956481
tensorflow.python.autograph.pyct.testing.codegen.generate_statement,"Generate a statement node, dispatching to the correct class method.",torch._dynamo.codegen.create_load_python_module,Generate a LOAD_GLOBAL instruction to fetch a given python module.,0.95646584
tensorflow.python.keras.backend_config.image_data_format,"Returns the default image data format convention. Returns: A string, either `'channels_first'` or `'channels_last'` Example: >>> tf.keras.backend.image_data_format() 'channels_last'",torch.distributed.run.run_script_path,"Run the provided `training_script` from within this interpreter. Usage: `script_as_function(""/abs/path/to/script.py"", ""--arg1"", ""val1"")`",0.95645154
tensorflow.python.distribute.failure_handling.failure_handling_util.on_gcp,Detect whether the current running environment is on GCP.,torch.distributed.distributed_c10d.is_mpi_available,Check if the MPI backend is available.,0.9564512
tensorflow.python.kernel_tests.array_ops.gather_op_test.testBatchDimsMatchesPythonBatching,Checks that batch_dims matches multiple calls to tf.gather().,torch.profiler.profiler.cleanup,Calls unregister_callback() to make sure to finalize outputs.,0.9564429
tensorflow.python.ops.ragged.dynamic_ragged_shape.from_row_partitions,"Create a shape from row_partitions. Args: row_partitions: a nonempty list of RowPartition objects. dtype: the dtype to use, or None to use the row_partitions dtype. Returns: a DynamicRaggedShape with inner_rank==1.",torch.autograd.profiler_util.key_averages,"Averages all function events over their keys. Args: group_by_input_shapes: group entries by (event name, input shapes) rather than just event name. This is useful to see which input shapes contribute to the runtime the most and may help with size-specific optimizations or choosing the best candidates for quantization (aka fitting a roof line) group_by_stack_n: group by top n stack trace entries Returns: An EventList containing FunctionEventAvg objects.",0.9564313
tensorflow.python.distribute.multi_process_lib.is_oss,Returns whether the test is run under OSS.,torch.onnx._internal.registration.is_registered_op,Returns whether the given op is registered for the given opset version.,0.9563788
tensorflow.python.eager.monitoring.set,Atomically set the value. Args: value: bool value.,torch.nn.modules.container.append,Append a given value at the end of the list. Args: value (Any): value to append,0.9563782
tensorflow.python.saved_model.builder_impl.add_meta_graph_and_variables,"Adds the current meta graph to the SavedModel and saves variables. Creates a Saver to save the variables from the provided session. Exports the corresponding meta graph def. This function assumes that the variables to be saved have been initialized. For a given `SavedModelBuilder`, this API must be called exactly once and for the first meta graph to save. For subsequent meta graph defs to be added, the `add_meta_graph()` API must be used. Args: sess: The TensorFlow session from which to save the meta graph and variables. tags: The set of tags with which to save the meta graph. signature_def_map: The map of signature def map to add to the meta graph def. assets_list: Assets to be saved with SavedModel. clear_devices: Set to true if the device info on the default graph should be cleared. init_op: Op or group of ops to execute when the graph is loaded. Note that when the init_op is specified it is run after the restore op at load-time. train_op: Op or group of ops that trains the model when run. This will not be run automatically when the graph is loaded, instead saved in a SignatureDef accessible through the exported MetaGraph. strip_default_attrs: Boolean. If `True`, default-valued attributes will be removed from the NodeDefs. For a detailed guide, see [Stripping Default-Valued Attributes](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md#stripping-default-valued-attributes). saver: An instance of tf.compat.v1.train.Saver that will be used to export the metagraph and save variables. If None, a sharded Saver that restores all variables will be used.",torch.nn.parallel.distributed.register_comm_hook,"Register communication hook for user-defined DDP aggregation of gradients across multiple workers. This hook would be very useful for researchers to try out new ideas. For example, this hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while running Distributed DataParallel training. Args: state (object): Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker. hook (Callable): Callable with the following signature: ``hook(state: object, bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]``: This function is called once the bucket is ready. The hook can perform whatever processing is needed and return a Future indicating completion of any async work (ex: allreduce). If the hook doesn't perform any communication, it still must return a completed Future. The Future should hold the new value of grad bucket's tensors. Once a bucket is ready, c10d reducer would call this hook and use the tensors returned by the Future and copy grads to individual parameters. Note that the future's return type must be a single tensor. We also provide an API called ``get_future`` to retrieve a Future associated with the completion of ``c10d.ProcessGroup.Work``. ``get_future`` is currently supported for NCCL and also supported for most operations on GLOO and MPI, except for peer to peer operations (send/recv). .. warning :: Grad bucket's tensors will not be predivided by world_size. User is responsible to divide by the world_size in case of operations like allreduce. .. warning :: DDP communication hook can only be registered once and should be registered before calling backward. .. warning :: The Future object that hook returns should contain a single t",0.95636165
tensorflow.python.ops.init_ops_v2.from_config,"Instantiates an initializer from a configuration dictionary. Example: python initializer = RandomUniform(-1, 1) config = initializer.get_config() initializer = RandomUniform.from_config(config) Args: config: A Python dictionary. It will typically be the output of `get_config`. Returns: An Initializer instance.",torch.onnx._internal.fx.registration.from_builtin_function,"From a builtin function, e.g. operator.add, math.ceil, etc, get the OpName. FX graph uses built-in functions to caculate sympy expression. This function is used to get the OpName from a builtin function. Args: builtin_function (types.BuiltinFunctionType): operator.add, math.ceil, etc. Returns: OpName: _description_",0.95635915
tensorflow.python.ops.signal.spectral_ops.inverse_stft_window_fn_inner,"Computes a window that can be used in `inverse_stft`. Args: frame_length: An integer scalar `Tensor`. The window length in samples. dtype: Data type of waveform passed to `stft`. Returns: A window suitable for reconstructing original waveform in `inverse_stft`. Raises: ValueError: If `frame_length` is not scalar, `forward_window_fn` is not a callable that takes a window length and a `dtype` keyword argument and returns a `[window_length]` `Tensor` of samples in the provided datatype `frame_step` is not scalar, or `frame_step` is not scalar.",torch.autograd.gradcheck.get_numerical_jacobian,"Compute the numerical Jacobian for a given fn and its inputs. This is a Deprecated API. Args: fn: the function to compute the Jacobian for (must take inputs as a tuple) input: input to `fn` target: the Tensors wrt whom Jacobians are calculated (default=`input`) eps: the magnitude of the perturbation during finite differencing (default=`1e-3`) Returns: A list of Jacobians of `fn` (restricted to its first output) with respect to each input or target, if provided. Note that `target` may not even be part of `input` to `fn`, so please be **very careful** in this to not clone `target`.",0.9563195
tensorflow.python.checkpoint.async_checkpoint_helper.read,"Restore the checkpointed variables. This method has exactly the same logic as restore(). This method is implemented only to fulfill the duty of subclassing tf.train.Checkpoint. Args: save_path: The full name of the checkpoint file to be restored. options: CheckpointOption instance. Returns: A load status object, which can be used to make assertions about the status of a checkpoint restoration. See tf.train.Checkpoint.restore() for more details.",torch.distributed.checkpoint.storage.storage_meta,Return the storage-specific metadata. This is used to store additional information in a checkpoint that can be useful for providing request-level observability. StorageMeta is passed to the ``SavePlanner`` during save calls. Returns None by default. TODO: provide an example,0.9563165
tensorflow.python.compiler.tensorrt.utils.get_trtengineop_io_nodes_count,Returns the number of input/output nodes of a TRTEngineOp.,torch._export.utils.nodes_count,Returns the number of nodes that match the node_call_back.,0.9562144
tensorflow.python.eager.polymorphic_function.atomic_function.attributes,Returns FunctionDef attributes in the Runtime.,torch.onnx._internal.diagnostics.infra.context.sarif,Returns the SARIF Run object.,0.95619935
tensorflow.python.ops.lookup_ops.index_to_string_table_from_tensor,"Returns a lookup table that maps a `Tensor` of indices into strings. This operation constructs a lookup table to map int64 indices into string values. The mapping is initialized from a string `vocabulary_list` 1-D `Tensor` where each element is a value and the corresponding index within the tensor is the key. Any input which does not have a corresponding index in 'vocabulary_list' (an out-of-vocabulary entry) is assigned the `default_value` The underlying table must be initialized by calling `session.run(tf.compat.v1.tables_initializer())` or `session.run(table.init())` once. Elements in `vocabulary_list` cannot have duplicates, otherwise when executing the table initializer op, it will throw a `FailedPreconditionError`. Sample Usages: python vocabulary_list = tf.constant([""emerson"", ""lake"", ""palmer""]) indices = tf.constant([1, 5], tf.int64) table = tf.lookup.index_to_string_table_from_tensor( vocabulary_list, default_value=""UNKNOWN"") values = table.lookup(indices) ... tf.compat.v1.tables_initializer().run() values.eval() ==> [""lake"", ""UNKNOWN""] Args: vocabulary_list: A 1-D string `Tensor` that specifies the strings to map from indices. default_value: The value to use for out-of-vocabulary indices. name: A name for this op (optional). Returns: The lookup table to map a string values associated to a given index `int64` `Tensors`. Raises: ValueError: when `vocabulary_list` is not set.",torch.utils.data._utils.collate.collate,"General collate function that handles collection type of element within each batch. The function also opens function registry to deal with specific element types. `default_collate_fn_map` provides default collate functions for tensors, numpy arrays, numbers and strings. Args: batch: a single batch to be collated collate_fn_map: Optional dictionary mapping from element type to the corresponding collate function. If the element type isn't present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key. Examples: >>> def collate_tensor_fn(batch, *, collate_fn_map): ... # Extend this function to handle batch of tensors ... return torch.stack(batch, 0) >>> def custom_collate(batch): ... collate_map = {torch.Tensor: collate_tensor_fn} ... return collate(batch, collate_fn_map=collate_map) >>> # Extend `default_collate` by in-place modifying `default_collate_fn_map` >>> default_collate_fn_map.update({torch.Tensor: collate_tensor_fn}) Note: Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as `collate_fn_map`.",0.95619684
tensorflow.python.debug.lib.debug_data.find,"Find dumped tensor data by a certain predicate. Args: predicate: A callable that takes two input arguments: python def predicate(debug_tensor_datum, tensor): # returns a bool where `debug_tensor_datum` is an instance of `DebugTensorDatum`, which carries the metadata, such as the `Tensor`'s node name, output slot timestamp, debug op name, etc.; and `tensor` is the dumped tensor value as a `numpy.ndarray`. first_n: (`int`) return only the first n `DebugTensotDatum` instances (in time order) for which the predicate returns True. To return all the `DebugTensotDatum` instances, let first_n be <= 0. device_name: optional device name. exclude_node_names: Optional regular expression to exclude nodes with names matching the regular expression. Returns: A list of all `DebugTensorDatum` objects in this `DebugDumpDir` object for which predicate returns True, sorted in ascending order of the timestamp.",torch.distributed._tensor.api.redistribute,"`redistribute` performs necessary collective operations that redistribute the current DTensor from its current placements to a new placements, or from is current DeviceMesh to a new DeviceMesh. i.e. we can turn a Sharded DTensor to a Replicated DTensor by specifying a Replicate placement for each dimension of the DeviceMesh. Args: device_mesh (:class:`DeviceMesh`, optional): DeviceMesh to place the DTensor, if not specified, must be called under a DeviceMesh context manager, default: None placements (List[:class:`Placement`], optional): the new placements that describes how to place the DTensor into the DeviceMesh, must have the same number of elements as `device_mesh.ndim`. Keyword args: async_op (bool, optional): whether to perform the DTensor redistribute operation asynchronously or not. Default: False Returns: A :class:`DTensor` object .. note:: `redistribute` is differentiable.",0.9561374
tensorflow.python.ops.numerics.add_check_numerics_ops,"Connect a `tf.debugging.check_numerics` to every floating point tensor. `check_numerics` operations themselves are added for each `half`, `float`, or `double` tensor in the current default graph. For all ops in the graph, the `check_numerics` op for all of its (`half`, `float`, or `double`) inputs is guaranteed to run before the `check_numerics` op on any of its outputs. Note: This API is not compatible with the use of `tf.cond` or `tf.while_loop`, and will raise a `ValueError` if you attempt to call it in such a graph. Returns: A `group` op depending on all `check_numerics` ops added. Raises: ValueError: If the graph contains any numeric operations in a control flow structure. RuntimeError: If called with eager execution enabled. @compatibility(eager) Not compatible with eager execution. To check for `Inf`s and `NaN`s under eager execution, call `tf.debugging.enable_check_numerics()` once before executing the checked operations. @end_compatibility",torch.distributed.distributed_c10d.register_backend,"Register a new backend with the given name and instantiating function. This class method is used by 3rd party ``ProcessGroup`` extension to register new backends. Args: name (str): Backend name of the ``ProcessGroup`` extension. It should match the one in ``init_process_group()``. func (function): Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including ``store``, ``rank``, ``world_size``, and ``timeout``. extended_api (bool, optional): Whether the backend supports extended argument structure. Default: ``False``. If set to ``True``, the backend will get an instance of ``c10d::DistributedBackendOptions``, and a process group options object as defined by the backend implementation. device (str or list of str, optional): device type this backend supports, e.g. ""cpu"", ""cuda"", etc. If `None`, assuming both ""cpu"" and ""cuda"" .. note:: This support of 3rd party backend is experimental and subject to change.",0.9561349
tensorflow.python.tools.optimize_for_inference_lib.fuse_resize_and_conv,"Merges preceding resize and mirror pad ops into a specialized convolution. There's a common pattern of enlarging the input to a convolution using a resize operation, and also using MirrorPad to extend the boundaries to that zero edge pixels don't bleed inwards when convolving. This routine looks for that pattern of operations, and fuses them together into a Conv2DWithResizeOp. Args: input_graph_def: A GraphDef containing a model. output_node_names: A list of names of the nodes that produce the final results. Returns: Modified graph with resize and pad ops merged. Raises: ValueError: If the graph is badly formed with duplicate node names.",torch.distributed._spmd.partial_lower.partial_lower,"Lower Inductor compatible portions of the graph module to Inductor. Args: node_predicate: user predicate for determining whether to consider a node for lowering. subgraph_predicate: user predicate for determining whether to consider a list of candidate nodes for lowering. dumper: a callback for dumping subgraphs for human digestion. For exmaple, it can be a function that writes to disk/blob storage and returns the path/handle. The returned path/handle for each subgraph will be made available in the subgraph call node in the parent graph, as well as the label of the profiler block for the subgraph.",0.95613396
tensorflow.python.keras.utils.conv_utils.conv_kernel_mask,"Compute a mask representing the connectivity of a convolution operation. Assume a convolution with given parameters is applied to an input having N spatial dimensions with `input_shape = (d_in1, ..., d_inN)` to produce an output with shape `(d_out1, ..., d_outN)`. This method returns a boolean array of shape `(d_in1, ..., d_inN, d_out1, ..., d_outN)` with `True` entries indicating pairs of input and output locations that are connected by a weight. Example: >>> input_shape = (4,) >>> kernel_shape = (2,) >>> strides = (1,) >>> padding = ""valid"" >>> conv_kernel_mask(input_shape, kernel_shape, strides, padding) array([[ True, False, False], [ True, True, False], [False, True, True], [False, False, True]]) where rows and columns correspond to inputs and outputs respectively. Args: input_shape: tuple of size N: `(d_in1, ..., d_inN)`, spatial shape of the input. kernel_shape: tuple of size N, spatial shape of the convolutional kernel / receptive field. strides: tuple of size N, strides along each spatial dimension. padding: type of padding, string `""same""` or `""valid""`. `""valid""` means no padding. `""same""` results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. Returns: A boolean 2N-D `np.ndarray` of shape `(d_in1, ..., d_inN, d_out1, ..., d_outN)`, where `(d_out1, ..., d_outN)` is the spatial shape of the output. `True` entries in the mask represent pairs of input-output locations that are connected by a weight. Raises: ValueError: if `input_shape`, `kernel_shape` and `strides` don't have the same number of dimensions. NotImplementedError: if `padding` is not in {`""same""`, `""valid""`}.",torch.ao.nn.quantized.functional.upsample,"Upsamples the input to either the given :attr:`size` or the given :attr:`scale_factor` .. warning:: This function is deprecated in favor of :func:`torch.ao.nn.quantized.functional.interpolate`. This is equivalent with ``nn.quantized.functional.interpolate(...)``. See :func:`torch.nn.functional.interpolate` for implementation details. The input dimensions are interpreted in the form: `mini-batch x channels x [optional depth] x [optional height] x width`. .. note:: The input quantization parameters propagate to the output. .. note:: Only 2D input is supported for quantized inputs .. note:: Only the following modes are supported for the quantized inputs: - `bilinear` - `nearest` Args: input (Tensor): quantized input tensor size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (float or Tuple[float]): multiplier for spatial size. Has to be an integer. mode (str): algorithm used for upsampling: ``'nearest'`` | ``'bilinear'`` align_corners (bool, optional): Geometrically, we consider the pixels of the input and output as squares rather than points. If set to ``True``, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to ``False``, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation *independent* of input size when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode` is ``'bilinear'``. Default: ``False`` .. warning:: With ``align_corners = True``, the linearly interpolating modes (`bilinear`) don't proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is ``align_corners = False``. See :class:`~torch.nn.Upsample` for concrete examples ",0.95611906
tensorflow.python.keras.backend.ones_like,"Instantiates an all-ones variable of the same shape as another tensor. Args: x: Keras variable or tensor. dtype: String, dtype of returned Keras variable. None uses the dtype of x. name: String, name for the variable to create. Returns: A Keras variable with the shape of x filled with ones. Example: >>> kvar = tf.keras.backend.variable(np.random.random((2,3))) >>> kvar_ones = tf.keras.backend.ones_like(kvar) >>> tf.keras.backend.eval(kvar_ones) array([[1., 1., 1.], [1., 1., 1.]], dtype=float32)",torch.nn.modules.module.named_parameters,"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. remove_duplicate (bool, optional): whether to remove the duplicated parameters in the result. Defaults to True. Yields: (str, Parameter): Tuple containing the name and parameter Example:: >>> # xdoctest: +SKIP(""undefined vars"") >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size())",0.95602405
tensorflow.python.data.experimental.ops.map_defun.map_defun,"Map a function on the list of tensors unpacked from `elems` on dimension 0. Args: fn: A function (`function.defun`) that takes a list of tensors and returns another list of tensors. The output list has the same types as output_dtypes. The elements of the output list have the same dimension 0 as `elems`, and the remaining dimensions correspond to those of `fn_output_shapes`. elems: A list of tensors. output_dtypes: A list of dtypes corresponding to the output types of the function. output_shapes: A list of `TensorShape`s corresponding to the output shapes from each invocation of the function on slices of inputs. max_intra_op_parallelism: An integer. If positive, sets the max parallelism limit of each function call to this. Raises: ValueError: if any of the inputs are malformed. Returns: A list of `Tensor` objects with the same types as `output_dtypes`.",torch.distributions.utils.broadcast_all,"Given a list of values (possibly containing numbers), returns a list where each value is broadcasted based on the following rules: - `torch.*Tensor` instances are broadcasted as per :ref:`_broadcasting-semantics`. - numbers.Number instances (scalars) are upcast to tensors having the same size and type as the first tensor passed to `values`. If all the values are scalars, then they are upcasted to scalar Tensors. Args: values (list of `numbers.Number`, `torch.*Tensor` or objects implementing __torch_function__) Raises: ValueError: if any of the values is not a `numbers.Number` instance, a `torch.*Tensor` instance, or an instance implementing __torch_function__",0.95599174
tensorflow.python.ops.collective_ops.broadcast_send_v2,"Broadcasts one tensor to a group of others, across devices. Args: t: the tensor to be sent. group_size: an int32 tensor. One plus the number of receiving tensors, i.e. the total number of devices participating. Each tensor must reside on a different device. group_key: an int32 tensor identifying the group of devices. instance_key: an int32 tensor identifying the participating group of Ops. communication_hint: preferred collective communication. The implementation may fall back to another mechanism. Options include `auto`, `ring`, and `nccl`. timeout: If set to a non zero, set a completion timeout to detect staleness. If the timer goes off, a DeadlineExceededError is raised. The timeout value in seconds. This feature is experimental. Returns: An Op implementing the distributed broadcast send.",torch.distributed.distributed_c10d.recv,"Receives a tensor synchronously. .. warning:: ``tag`` is not supported with the NCCL backend. Args: tensor (Tensor): Tensor to fill with received data. src (int, optional): Source rank on global process group (regardless of ``group`` argument). Will receive from any process if unspecified. group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. tag (int, optional): Tag to match recv with remote send Returns: Sender rank -1, if not part of the group",0.95593315
tensorflow.python.framework.composite_tensor_gradient.get_flat_tensors_for_gradients,"Returns a flat list of Tensors that should be differentiated for `xs`. Args: xs: A list of `Tensor`s or `CompositeTensor`s. Returns: A flat list of `Tensor`s constructed from `xs`, where `Tensor` values are left as-is, and `CompositeTensor`s are replaced with `_get_tensors_for_gradient(x)`.",torch.autograd.graph.get_gradient_edge,"Get the gradient edge for computing the gradient of the given Tensor. In particular, it is equivalent to call ``g = autograd.grad(loss, input)`` and ``g = autograd.grad(loss, get_gradient_edge(input))``.",0.9559326
tensorflow.python.ops.template.func,Returns the func given to this Template.,torch.onnx._internal.fx.passes.modularization.module_class,Returns the module class of the top module.,0.95593
tensorflow.python.ops.array_ops.parallel_stack,"Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor in parallel. Requires that the shape of inputs be known at graph construction time. Packs the list of tensors in `values` into a tensor with rank one higher than each tensor in `values`, by packing them along the first dimension. Given a list of length `N` of tensors of shape `(A, B, C)`; the `output` tensor will have the shape `(N, A, B, C)`. For example: python x = tf.constant([1, 4]) y = tf.constant([2, 5]) z = tf.constant([3, 6]) tf.parallel_stack([x, y, z]) # [[1, 4], [2, 5], [3, 6]] The difference between `stack` and `parallel_stack` is that `stack` requires all the inputs be computed before the operation will begin but doesn't require that the input shapes be known during graph construction. `parallel_stack` will copy pieces of the input into the output as they become available, in some situations this can provide a performance benefit. Unlike `stack`, `parallel_stack` does NOT support backpropagation. This is the opposite of unstack. The numpy equivalent is tf.parallel_stack([x, y, z]) = np.asarray([x, y, z]) @compatibility(eager) parallel_stack is not compatible with eager execution. @end_compatibility Args: values: A list of `Tensor` objects with the same shape and type. name: A name for this operation (optional). Returns: output: A stacked `Tensor` with the same type as `values`. Raises: RuntimeError: if executed in eager mode.",torch._functorch.eager_transforms.linearize,"Returns the value of ``func`` at ``primals`` and linear approximation at ``primals``. Args: func (Callable): A Python function that takes one or more arguments. primals (Tensors): Positional arguments to ``func`` that must all be Tensors. These are the values at which the function is linearly approximated. Returns: Returns a ``(output, jvp_fn)`` tuple containing the output of ``func`` applied to ``primals`` and a function that computes the jvp of ``func`` evaluated at ``primals``. linearize is useful if jvp is to be computed multiple times at ``primals``. However, to achieve this, linearize saves intermediate computation and has higher memory requirements than directly applying `jvp`. So, if all the ``tangents`` are known, it maybe more efficient to compute vmap(jvp) instead of using linearize. .. note:: linearize evaluates ``func`` twice. Please file an issue for an implementation with a single evaluation. Example:: >>> import torch >>> from torch.func import linearize >>> def fn(x): ... return x.sin() ... >>> output, jvp_fn = linearize(fn, torch.zeros(3, 3)) >>> jvp_fn(torch.ones(3, 3)) tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.]]) >>>",0.95591855
tensorflow.python.ops.tensor_array_ops.size,Return the size of the TensorArray.,torch.distributed.distributed_c10d.get_pg_count,Return the number of process groups.,0.9559156
tensorflow.python.keras.backend.deprecated_internal_learning_phase_scope,"An internal-only version of `learning_phase_scope`. Unlike the public method, this method does not raise a deprecation warning. This is needed because saved model saving needs to set learning phase to maintain compatibility with code that sets/gets the learning phase, but saved model saving itself shouldn't raise a deprecation warning. We can get rid of this method and its usages when the public API is removed. Args: value: Learning phase value, either 0 or 1 (integers). 0 = test, 1 = train Yields: None. Raises: ValueError: if `value` is neither `0` nor `1`.",torch.onnx._internal.diagnostics.infra.context.log_and_raise_if_error,"Logs a diagnostic and raises an exception if it is an error. Use this method for logging non inflight diagnostics where diagnostic level is not known or lower than ERROR. If it is always expected raise, use `log` and explicit `raise` instead. Otherwise there is no way to convey the message that it always raises to Python intellisense and type checking tools. This method should be used only after all the necessary information for the diagnostic has been collected. Args: diagnostic: The diagnostic to add.",0.9558852
tensorflow.python.autograph.pyct.parser.parse,"Returns the AST of given piece of code. Args: src: Text preamble_len: Int, indicates leading nodes in the parsed AST which should be dropped. single_node: Bool, whether `src` is assumed to be represented by exactly one AST node. Returns: ast.AST",torch.ao.pruning._experimental.data_scheduler.base_data_scheduler.load_state_dict,Loads the schedulers state. Note: Remember to restore the state of the data_sparsifier before the scheduler. Args: state_dict (dict): scheduler state. Should be an object returned from a call to :meth:`state_dict`.,0.95587915
tensorflow.python.checkpoint.checkpoint_management.latest_checkpoint,"The prefix of the most recent checkpoint in `directory`. Equivalent to `tf.train.latest_checkpoint(directory)` where `directory` is the constructor argument to `CheckpointManager`. Suitable for passing to `tf.train.Checkpoint.restore` to resume training. Returns: The checkpoint prefix. If there are no checkpoints, returns `None`.",torch.distributed.optim.post_localSGD_optimizer.load_state_dict,"This is the same as :class:`torch.optim.Optimizer` :meth:`load_state_dict`, but also restores model averager's step value to the one saved in the provided ``state_dict``. If there is no ``""step""`` entry in ``state_dict``, it will raise a warning and initialize the model averager's step to 0.",0.95587265
tensorflow.python.ops.distributions.util.assert_integer_form,"Assert that x has integer components (or floats equal to integers). Args: x: Floating-point `Tensor` data: The tensors to print out if the condition is `False`. Defaults to error message and first few entries of `x` and `y`. summarize: Print this many entries of each tensor. message: A string to prefix to the default message. int_dtype: A `tf.dtype` used to cast the float to. The default (`None`) implies the smallest possible signed int will be used for casting. name: A name for this operation (optional). Returns: Op raising `InvalidArgumentError` if `cast(x, int_dtype) != x`.",torch.fx.node.update_arg,"Update an existing positional argument to contain the new value ``arg``. After calling, ``self.args[idx] == arg``. Args: idx (int): The index into ``self.args`` of the element to update arg (Argument): The new argument value to write into ``args``",0.9558079
tensorflow.python.tpu.tensor_tracer_report.create_report_proto,"Creates and returns a proto that stores tensor tracer configuration. Args: tt_config: TensorTracerConfig object holding information about the run environment (device, # cores, # hosts), and tensor tracer version information. tt_parameters: TTParameters objects storing the user provided parameters for tensor tracer. tensor_trace_order: TensorTraceOrder object storing a topological order of the graph. tensor_trace_points: Progromatically added trace_points/checkpoints. collected_signature_types: The signature types collected, e,g, norm, max, min, mean... Returns: TensorTracerReport proto.",torch.onnx.symbolic_opset9.cat,Implement concatenation of pytorch tensors in ONNX along the specified `dim` dimension. Parameters: g (jit_utils.GraphContext): Graph context. tensor_list (List[torch.Tensor]): List of tensors to concatenate. dim (int): Dimension along which to concatenate the tensors. Returns: ONNX graph node representing the concatenated tensor.,0.95579684
tensorflow.python.eager.context.remove_function,Remove a function from the context.,torch.nn.modules.container.clear,Remove all items from the ParameterDict.,0.9557696
tensorflow.python.util.nest.pack_sequence_as,"Returns a given flattened sequence packed into a given structure. Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest) for the definition of a structure. If `structure` is an atom, `flat_sequence` must be a single-item list; in this case the return value is `flat_sequence[0]`. If `structure` is or contains a dict instance, the keys will be sorted to pack the flat sequence in deterministic order. This is true also for `OrderedDict` instances: their sequence order is ignored, the sorting order of keys is used instead. The same convention is followed in `flatten`. This correctly repacks dicts and `OrderedDict`s after they have been flattened, and also allows flattening an `OrderedDict` and then repacking it back using a corresponding plain dict, or vice-versa. Dictionaries with non-sortable keys cannot be flattened. Examples: 1. Python dict: >>> structure = { ""key3"": """", ""key1"": """", ""key2"": """" } >>> flat_sequence = [""value1"", ""value2"", ""value3""] >>> tf.nest.pack_sequence_as(structure, flat_sequence) {'key3': 'value3', 'key1': 'value1', 'key2': 'value2'} 2. For a nested python tuple: >>> structure = (('a','b'), ('c','d','e'), 'f') >>> flat_sequence = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0] >>> tf.nest.pack_sequence_as(structure, flat_sequence) ((1.0, 2.0), (3.0, 4.0, 5.0), 6.0) 3. For a nested dictionary of dictionaries: >>> structure = { ""key3"": {""c"": ('alpha', 'beta'), ""a"": ('gamma')}, ... ""key1"": {""e"": ""val1"", ""d"": ""val2""} } >>> flat_sequence = ['val2', 'val1', 3.0, 1.0, 2.0] >>> tf.nest.pack_sequence_as(structure, flat_sequence) {'key3': {'c': (1.0, 2.0), 'a': 3.0}, 'key1': {'e': 'val1', 'd': 'val2'}} 4. Numpy array (considered a scalar): >>> structure = ['a'] >>> flat_sequence = [np.array([[1, 2], [3, 4]])] >>> tf.nest.pack_sequence_as(structure, flat_sequence) [array([[1, 2], [3, 4]])] 5. tf.Tensor (considered a scalar): >>> structure = ['a'] >>> flat_sequence = [tf.constant([[1., 2., 3.], [4., 5., 6.]])] >>> tf.nest.pack_sequence_as(structure, flat_seque",torch.fx.experimental.unification.unification_tools.update_in,"Update value in a (potentially) nested dictionary inputs: d - dictionary on which to operate keys - list or tuple giving the location of the value to be changed in d func - function to operate on that value If keys == [k0,..,kX] and d[k0]..[kX] == v, update_in returns a copy of the original dictionary with v replaced by func(v), but does not mutate the original dictionary. If k0 is not a key in d, update_in creates nested dictionaries to the depth specified by the keys, with the innermost value set to func(default). >>> inc = lambda x: x + 1 >>> update_in({'a': 0}, ['a'], inc) {'a': 1} >>> transaction = {'name': 'Alice', ... 'purchase': {'items': ['Apple', 'Orange'], ... 'costs': [0.50, 1.25]}, ... 'credit card': '5555-1234-1234-1234'} >>> update_in(transaction, ['purchase', 'costs'], sum) # doctest: +SKIP {'credit card': '5555-1234-1234-1234', 'name': 'Alice', 'purchase': {'costs': 1.75, 'items': ['Apple', 'Orange']}} >>> # updating a value when k0 is not in d >>> update_in({}, [1, 2, 3], str, default=""bar"") {1: {2: {3: 'bar'}}} >>> update_in({1: 'foo'}, [2, 3, 4], inc, 0) {1: 'foo', 2: {3: {4: 1}}}",0.9556003
tensorflow.python.client.session.list_devices,"Lists available devices in this session. python devices = sess.list_devices() for d in devices: print(d.name) Where: Each element in the list has the following properties name: A string with the full name of the device. ex: `/job:worker/replica:0/task:3/device:CPU:0` device_type: The type of the device (e.g. `CPU`, `GPU`, `TPU`.) memory_limit: The maximum amount of memory available on the device. Note: depending on the device, it is possible the usable memory could be substantially less. Raises: tf.errors.OpError: If it encounters an error (e.g. session is in an invalid state, or network errors occur). Returns: A list of devices in the session.",torch.random.fork_rng,"Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in. Args: devices (iterable of Device IDs): devices for which to fork the RNG. CPU RNG state is always forked. By default, :meth:`fork_rng` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed enabled (bool): if ``False``, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it. device_type (str): device type str, default is `cuda`. As for custom device, see details in [Note: support the custom device with privateuse1]",0.95559156
tensorflow.python.debug.lib.debug_events_reader.read_graph_op_creation_stack_trace,"Read the stack trace of a given graph op creation object. Args: graph_op_creation_digest: The GraphOpCreationDigest object of interest. Returns: A tuple consisting of: 1. The host name. 2. The stack trace, as a list of (file_path, lineno, func) tuples.",torch.distributed._spmd.graph_optimization.get_comm_block,"Find out all the nodes belong to this communcation given a collective node (e.g., allreduce). Args: comm_node(fx.Node): The target communication/collective node. Returns: The CommBlock that encapsulates the related nodes (e.g., wait_node) of the given comm_node.",0.95559096
tensorflow.python.framework.config.get_memory_info,"Get memory info for the chosen device, as a dict. This function returns a dict containing information about the device's memory usage. For example: >>> if tf.config.list_physical_devices('GPU'): ... # Returns a dict in the form {'current': <current mem usage>, ... # 'peak': <peak mem usage>} ... tf.config.experimental.get_memory_info('GPU:0') Currently returns the following keys: - `'current'`: The current memory used by the device, in bytes. - `'peak'`: The peak memory used by the device across the run of the program, in bytes. Can be reset with `tf.config.experimental.reset_memory_stats`. More keys may be added in the future, including device-specific keys. Currently only supports GPU and TPU. If called on a CPU device, an exception will be raised. For GPUs, TensorFlow will allocate all the memory by default, unless changed with `tf.config.experimental.set_memory_growth`. The dict specifies only the current and peak memory that TensorFlow is actually using, not the memory that TensorFlow has allocated on the GPU. Args: device: Device string to get the memory information for, e.g. `""GPU:0""`, `""TPU:0""`. See https://www.tensorflow.org/api_docs/python/tf/device for specifying device strings. Returns: A dict with keys `'current'` and `'peak'`, specifying the current and peak memory usage respectively. Raises: ValueError: No device found with the device name, like '""nonexistent""'. ValueError: Invalid device name, like '""GPU""', '""CPU:GPU""', '""CPU:""'. ValueError: Multiple devices matched with the device name. ValueError: Memory statistics not tracked, like '""CPU:0""'.",torch.utils.backend_registration.rename_privateuse1_backend,"Rename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs. The steps are: (1) (In C++) implement kernels for various torch operations, and register them to the PrivateUse1 dispatch key. (2) (In python) call torch.utils.rename_privateuse1_backend(""foo"") You can now use ""foo"" as an ordinary device string in python. Note: this API can only be called once per process. Attempting to change the external backend after it's already been set will result in an error. Note(AMP): If you want to support AMP on your device, you can register a custom backend module. The backend must register a custom backend module with ``torch._register_device_module(""foo"", BackendModule)``. BackendModule needs to have the following API's: (1) ``get_amp_supported_dtype() -> List[torch.dtype]`` get the supported dtypes on your ""foo"" device in AMP, maybe the ""foo"" device supports one more dtype. Note(random): If you want to support to set seed for your device, BackendModule needs to have the following API's: (1) ``_is_in_bad_fork() -> bool`` Return ``True`` if now it is in bad_fork, else return ``False``. (2) ``manual_seed_all(seed int) -> None`` Sets the seed for generating random numbers for your devices. (3) ``device_count() -> int`` Returns the number of ""foo""s available. (4) ``get_rng_state(device: Union[int, str, torch.device] = 'foo') -> Tensor`` Returns a list of ByteTensor representing the random number states of all devices. (5) ``set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = 'foo') -> None`` Sets the random number generator state of the specified ""foo"" device. And there are some common funcs: (1) ``is_available() -> bool`` Returns a bool indicating if ""foo"" is currently available. (2) ``current_device() -> int`` Returns the index of a currently selected device. For more details, see https://pytorch.org/tutorials/advanced/extend_dispatcher.html#get-a-dispatch-key-for-your-backend For an existing example, see htt",0.9555709
tensorflow.python.debug.lib.debug_events_reader.read_execution_stack_trace,"Read the stack trace of a given Execution object. Args: execution: The Execution object of interest. Returns: 1. The host name. 2. The stack trace, as a list of (file_path, lineno, func) tuples.",torch.distributed.elastic.rendezvous.utils.parse_rendezvous_endpoint,Extract the hostname and the port number from a rendezvous endpoint. Args: endpoint: A string in format <hostname>[:<port>]. default_port: The port number to use if the endpoint does not include one. Returns: A tuple of hostname and port number.,0.9555377
tensorflow.python.debug.cli.debugger_cli_common.get_help,"Compile help information into a RichTextLines object. Args: cmd_prefix: Optional command prefix. As the prefix itself or one of its aliases. Returns: A RichTextLines object containing the help information. If cmd_prefix is None, the return value will be the full command-line help. Otherwise, it will be the help information for the specified command.",torch.distributed.elastic.utils.logging.get_logger,"Util function to set up a simple logger that writes into stderr. The loglevel is fetched from the LOGLEVEL env. variable or WARNING as default. The function will use the module name of the caller if no name is provided. Args: name: Name of the logger. If no name provided, the name will be derived from the call stack.",0.9554909
tensorflow.python.ops.distributions.distribution.param_shapes,Shapes of parameters given the desired shape of a call to `sample()`. This is a class method that describes what key/value arguments are required to instantiate the given `Distribution` so that a particular shape is returned for that instance's call to `sample()`. Subclasses should override class method `_param_shapes`. Args: sample_shape: `Tensor` or python list/tuple. Desired shape of a call to `sample()`. name: name to prepend ops with. Returns: `dict` of parameter name to `Tensor` shapes.,torch.nn.modules.module.register_parameter,"Add a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (str): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter or None): parameter to be added to the module. If ``None``, then operations that run on parameters, such as :attr:`cuda`, are ignored. If ``None``, the parameter is **not** included in the module's :attr:`state_dict`.",0.95547867
tensorflow.python.ops.array_ops.ones_like,"Creates a tensor with all elements set to 1. See also `tf.ones`. Given a single tensor (`tensor`), this operation returns a tensor of the same type and shape as `tensor` with all elements set to 1. Optionally, you can specify a new type (`dtype`) for the returned tensor. For example: python tensor = tf.constant([[1, 2, 3], [4, 5, 6]]) tf.ones_like(tensor) # [[1, 1, 1], [1, 1, 1]] Args: tensor: A `Tensor`. dtype: A type for the returned `Tensor`. Must be `float32`, `float64`, `int8`, `uint8`, `int16`, `uint16`, `int32`, `int64`, `complex64`, `complex128` or `bool`. name: A name for the operation (optional). optimize: if true, attempt to statically determine the shape of 'tensor' and encode it as a constant. Returns: A `Tensor` with all elements set to 1.",torch.nn.utils.prune.identity,"Apply pruning reparametrization without pruning any units. Applies pruning reparametrization to the tensor corresponding to the parameter called ``name`` in ``module`` without actually pruning any units. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called ``name+'_mask'`` corresponding to the binary mask applied to the parameter ``name`` by the pruning method. 2) replacing the parameter ``name`` by its pruned version, while the original (unpruned) parameter is stored in a new parameter named ``name+'_orig'``. Note: The mask is a tensor of ones. Args: module (nn.Module): module containing the tensor to prune. name (str): parameter name within ``module`` on which pruning will act. Returns: module (nn.Module): modified (i.e. pruned) version of the input module Examples: >>> # xdoctest: +SKIP >>> m = prune.identity(nn.Linear(2, 3), 'bias') >>> print(m.bias_mask) tensor([1., 1., 1.])",0.9554724
tensorflow.python.util.tf_export.get_v1_names,Get a list of TF 1.* names for this symbol. Args: symbol: symbol to get API names for. Returns: List of all API names for this symbol.,torch.testing._internal.opinfo.definitions.signal.sample_inputs_window,"Base function used to create sample inputs for windows. For additional required args you should use *args, as well as **kwargs for additional keyword arguments.",0.95541
tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.master,"Get the Master string to be used for the session. In the normal case, this returns the grpc path (grpc://1.2.3.4:8470) of first instance in the ClusterSpec returned by the cluster_spec function. If a non-TPU name is used when constructing a TPUClusterResolver, that will be returned instead (e.g. If the tpus argument's value when constructing this TPUClusterResolver was 'grpc://10.240.1.2:8470', 'grpc://10.240.1.2:8470' will be returned). Args: task_type: (Optional, string) The type of the TensorFlow task of the master. task_id: (Optional, integer) The index of the TensorFlow task of the master. rpc_layer: (Optional, string) The RPC protocol TensorFlow should use to communicate with TPUs. Returns: string, the connection string to use when creating a session. Raises: ValueError: If none of the TPUs specified exists.",torch.distributed._shard.api.shard_module,"Shards a given module according to the provided sharding `plan`. This method first shards all the parameters according to the given sharding `plan`. Then if `output_plan` and `return_local_tensor` are specified in the sharding `plan`, it will tag the output of modules according `output_plan`, convert the module's output back to data parallel according to `return_local_tensor`. Needs to be called on all ranks in an SPMD fashion. Args: module (:class:`torch.nn.Module`): The module to apply sharding to plan (:class:`torch.distributed._shard.sharding_plan.ShardingPlan`): The ShardingPlan which specified param name to ShardingSpec to apply to each parameter. Keyword args: src_rank (int, optional): The source rank which is used as the ground truth of the data for the module that would be sharded and scattered across the rest of the ranks. Default: 0. process_group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used.",0.95537055
tensorflow.python.ops.distributions.distribution.log_cdf,"Log cumulative distribution function. Given random variable `X`, the cumulative distribution function `cdf` is: none log_cdf(x) := Log[ P[X <= x] ] Often, a numerical approximation can be used for `log_cdf(x)` that yields a more accurate answer than simply taking the logarithm of the `cdf` when `x << -1`. Args: value: `float` or `double` `Tensor`. name: Python `str` prepended to names of ops created by this function. Returns: logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type `self.dtype`.",torch.nn.functional.dropout3d,"Randomly zero out entire channels (a channel is a 3D feature map). For example, the :math:`j`-th channel of the :math:`i`-th sample in the batched input is a 3D tensor :math:`\text{input}[i, j]` of the input tensor. Each channel will be zeroed out independently on every forward call with probability :attr:`p` using samples from a Bernoulli distribution. See :class:`~torch.nn.Dropout3d` for details. Args: p: probability of a channel to be zeroed. Default: 0.5 training: apply dropout if is ``True``. Default: ``True`` inplace: If set to ``True``, will do this operation in-place. Default: ``False``",0.95534873
tensorflow.python.autograph.operators.slices.set_item,"The slice write operator (i.e. __setitem__). Note: it is unspecified whether target will be mutated or not. In general, if target is mutable (like Python lists), it will be mutated. Args: target: An entity that supports setitem semantics. i: Index to modify. x: The new element value. Returns: Same as target, after the update was performed. Raises: ValueError: if target is not of a supported type.",torch.onnx._internal.registration.register,Registers a symbolic function. Args: name: The qualified name of the function to register. In the form of 'domain::op'. E.g. 'aten::add'. opset: The opset version of the function to register. func: The symbolic function to register. custom: Whether the function is a custom function that overrides existing ones. Raises: ValueError: If the separator '::' is not in the name.,0.9553453
tensorflow.python.distribute.cross_device_ops_test.make_collective,"Returns collectives and other info to be used in tests. Args: num_processes: an integer indicating the number of processes that participate in the collective. gpu_per_process: number of GPUs (0 if no GPUs) used by each process. Returns: A tuple of (collective, devices, pid) where collective is a instance of `CollectiveAllReduce`, devices are a list of local devices (str) attached to the current process, and pid is the id of this process among all participant processes.",torch.distributed.distributed_c10d.get_rank,"Return the rank of the current process in the provided ``group``, default otherwise. Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to ``world_size``. Args: group (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Returns: The rank of the process group -1, if not part of the group",0.95518607
tensorflow.python.ops.linalg.linear_operator_util.is_ref,"Evaluates if the object has reference semantics. An object is deemed ""reference"" if it is a `tf.Variable` instance or is derived from a `tf.Module` with `dtype` and `shape` properties. Args: x: Any object. Returns: is_ref: Python `bool` indicating input is has nonreference semantics, i.e., is a `tf.Variable` or a `tf.Module` with `dtype` and `shape` properties.",torch.nn.modules.container.update,"Update the :class:`~torch.nn.ModuleDict` with key-value pairs from a mapping, overwriting existing keys. .. note:: If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or an iterable of key-value pairs, the order of new elements in it is preserved. Args: modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`, or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)",0.95518404
tensorflow.python.distribute.cluster_resolver.cluster_resolver.num_accelerators,"Returns the number of accelerator cores per worker. The SimpleClusterResolver does not do automatic detection of accelerators, and thus all arguments are unused and we simply return the value provided in the constructor. Args: task_type: Unused. task_id: Unused. config_proto: Unused.",torch.random.set_rng_state,"Sets the random number generator state. .. note:: This function only works for CPU. For CUDA, please use :func:`torch.manual_seed`, which works for both CPU and CUDA. Args: new_state (torch.ByteTensor): The desired state",0.9551477
tensorflow.python.framework.ops.as_graph_def,"Returns a serialized `GraphDef` representation of this graph. The serialized `GraphDef` can be imported into another `Graph` (using `tf.import_graph_def`) or used with the [C++ Session API](../../api_docs/cc/index.md). This method is thread-safe. Args: from_version: Optional. If this is set, returns a `GraphDef` containing only the nodes that were added to this graph since its `version` property had the given value. add_shapes: If true, adds an ""_output_shapes"" list attr to each node with the inferred shapes of each of its outputs. use_pybind11_proto: If true, If true, uses the c++ pybind11_proto api to get the GraphDef proto directly from c++, instead of through a TF buffer. See https://github.com/pybind/pybind11_protobuf for reference. Returns: A [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto) protocol buffer. Raises: ValueError: If the `graph_def` would be too large.",torch.cuda.graphs.capture_begin,"Begin capturing CUDA work on the current stream. Typically, you shouldn't call ``capture_begin`` yourself. Use :class:`~torch.cuda.graph` or :func:`~torch.cuda.make_graphed_callables`, which call ``capture_begin`` internally. Arguments: pool (optional): Token (returned by :func:`~torch.cuda.graph_pool_handle` or :meth:`other_Graph_instance.pool()<torch.cuda.CUDAGraph.pool>`) that hints this graph may share memory with the indicated pool. See :ref:`Graph memory management<graph-memory-management>`. capture_error_mode (str, optional): specifies the cudaStreamCaptureMode for the graph capture stream. Can be ""global"", ""thread_local"" or ""relaxed"". During cuda graph capture, some actions, such as cudaMalloc, may be unsafe. ""global"" will error on actions in other threads, ""thread_local"" will only error for actions in the current thread, and ""relaxed"" will not error on these actions. Do NOT change this setting unless you're familiar with `cudaStreamCaptureMode <https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html#group__CUDART__STREAM_1g9d0535d93a214cbf126835257b16ba85>`_",0.9550995
tensorflow.python.keras.callbacks.on_test_begin,Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Args: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future.,torch.cuda.streams.wait_stream,Synchronize with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete. Args: stream (Stream): a stream to synchronize. .. note:: This function returns without waiting for currently enqueued kernels in :attr:`stream`: only future operations are affected.,0.95507854
tensorflow.python.tools.saved_model_utils.read_saved_model,"Reads the saved_model.pb or saved_model.pbtxt file containing `SavedModel`. Args: saved_model_dir: Directory containing the SavedModel file. Returns: A `SavedModel` protocol buffer. Raises: IOError: If the file does not exist, or cannot be successfully parsed.",torch.distributed.fsdp.fully_sharded_data_parallel.named_buffers,"Return an iterator over module buffers, yielding both the name of the buffer and the buffer itself. Intercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix when inside the :meth:`summon_full_params` context manager.",0.9550415
tensorflow.python.ops.metrics_impl.auc,"Computes the approximate AUC via a Riemann sum. The `auc` function creates four local variables, `true_positives`, `true_negatives`, `false_positives` and `false_negatives` that are used to compute the AUC. To discretize the AUC curve, a linearly spaced set of thresholds is used to compute pairs of recall and precision values. The area under the ROC-curve is therefore computed using the height of the recall values by the false positive rate, while the area under the PR-curve is the computed using the height of the precision values by the recall. This value is ultimately returned as `auc`, an idempotent operation that computes the area under a discretized curve of precision versus recall values (computed using the aforementioned variables). The `num_thresholds` variable controls the degree of discretization with larger numbers of thresholds more closely approximating the true AUC. The quality of the approximation may vary dramatically depending on `num_thresholds`. For best results, `predictions` should be distributed approximately uniformly in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC approximation may be poor if this is not the case. Setting `summation_method` to 'minoring' or 'majoring' can help quantify the error in the approximation by providing lower or upper bound estimate of the AUC. The `thresholds` parameter can be used to manually specify thresholds which split the predictions more evenly. For estimation of the metric over a stream of data, the function creates an `update_op` operation that updates these variables and returns the `auc`. If `weights` is `None`, weights default to 1. Use weights of 0 to mask values. Args: labels: A `Tensor` whose shape matches `predictions`. Will be cast to `bool`. predictions: A floating point `Tensor` of arbitrary shape and whose values are in the range `[0, 1]`. weights: Optional `Tensor` whose rank is either 0, or the same rank as `labels`, and must be broadcastable to `labels` (i.e., all dime",torch.autograd.gradcheck.gradcheck,"Check gradients computed via small finite differences against analytical gradients wrt tensors in :attr:`inputs` that are of floating point or complex type and with ``requires_grad=True``. The check between numerical and analytical gradients uses :func:`~torch.allclose`. For most of the complex functions we consider for optimization purposes, no notion of Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient computation is done under the assumption that the overall function has a real-valued output, we treat functions with complex output in a special way. For these functions, gradcheck is applied to two real-valued functions corresponding to taking the real components of the complex outputs for the first, and taking the imaginary components of the complex outputs for the second. For more details, check out :ref:`complex_autograd-doc`. .. note:: The default values are designed for :attr:`input` of double precision. This check will likely fail if :attr:`input` is of less precision, e.g., ``FloatTensor``. .. note:: Gradcheck may fail when evaluated on non-differentiable points because the numerically computed gradients via finite differencing may differ those computed analytically (not necessarily because either is incorrect). For more context, see :ref:`non-differentiable-func-grad`. .. warning:: If any checked tensor in :attr:`input` has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from :func:`torch.expand`), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address. Args: func (function): a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors inputs (tuple of Tensor or Tensor): inputs to the function eps (float, optional): perturbation for finite differen",0.95498693
tensorflow.python.ops.composite_tensor_ops.composite_tensor_from_variant,Returns the `ExtensionType` value encoded by a variant scalar tensor. Args: encoded: A Tensor returned by `composite_tensor_to_variants`. type_spec: The `TypeSpec` of the original value. This is used to determine the number and types of the component tensors that comprise the decoded value. Must be compatible with the `TypeSpec` serilized in `encoded`. name: Optional name for the operation. Returns: An `ExtensionType` value that is compatible with `TypeSpec`. Raises: TypeError: If `encoded` is not a Tensor with dtype=variant. InvalidArgumentError: If `encoded` is not compatible with `type_spec`.,torch.nn.utils.rnn.to,"Perform dtype and/or device conversion on `self.data`. It has similar signature as :meth:`torch.Tensor.to`, except optional arguments like `non_blocking` and `copy` should be passed as kwargs, not args, or they will not apply to the index tensors. .. note:: If the ``self.data`` Tensor already has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned. Otherwise, returns a copy with the desired configuration.",0.9549506
tensorflow.python.framework.op_callbacks.should_invoke_op_callbacks,Determine if op callbacks are present and should be invoked. Returns: A thread-local result (boolean) indicating whether any op callback(s) exist and should be invoked.,torch.xpu.streams.query,Check if all the work submitted has been completed. Returns: A boolean indicating if all kernels in this stream are completed.,0.9549373
tensorflow.python.ops.math_ops.exp,"Computes exponential of x element-wise. \\(y = e^x\\). This function computes the exponential of the input tensor element-wise. i.e. `math.exp(x)` or \\(e^x\\), where `x` is the input tensor. \\(e\\) denotes Euler's number and is approximately equal to 2.718281. Output is positive for any real input. >>> x = tf.constant(2.0) >>> tf.math.exp(x) <tf.Tensor: shape=(), dtype=float32, numpy=7.389056> >>> x = tf.constant([2.0, 8.0]) >>> tf.math.exp(x) <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 7.389056, 2980.958 ], dtype=float32)> For complex numbers, the exponential value is calculated as $$ e^{x+iy} = {e^x} {e^{iy}} = {e^x} ({\cos (y) + i \sin (y)}) $$ For `1+1j` the value would be computed as: $$ e^1 (\cos (1) + i \sin (1)) = 2.7182817 \times (0.5403023+0.84147096j) $$ >>> x = tf.constant(1 + 1j) >>> tf.math.exp(x) <tf.Tensor: shape=(), dtype=complex128, numpy=(1.4686939399158851+2.2873552871788423j)> Args: x: A `tf.Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `complex64`, `complex128`. name: A name for the operation (optional). Returns: A `tf.Tensor`. Has the same type as `x`. @compatibility(numpy) Equivalent to np.exp @end_compatibility",torch.functional.cdist,"Computes batched the p-norm distance between each pair of the two collections of row vectors. Args: x1 (Tensor): input tensor of shape :math:`B \times P \times M`. x2 (Tensor): input tensor of shape :math:`B \times R \times M`. p: p value for the p-norm distance to calculate between each vector pair :math:`\in [0, \infty]`. compute_mode: 'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary. If x1 has shape :math:`B \times P \times M` and x2 has shape :math:`B \times R \times M` then the output will have shape :math:`B \times P \times R`. This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)` if :math:`p \in (0, \infty)`. When :math:`p = 0` it is equivalent to `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \infty`, the closest scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`. Example: >>> a = torch.tensor([[0.9041, 0.0196], [-0.3108, -2.4423], [-0.4821, 1.059]]) >>> a tensor([[ 0.9041, 0.0196], [-0.3108, -2.4423], [-0.4821, 1.0590]]) >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986, 1.3702]]) >>> b tensor([[-2.1763, -0.4713], [-0.6986, 1.3702]]) >>> torch.cdist(a, b, p=2) tensor([[3.1193, 2.0959], [2.7138, 3.8322], [2.2830, 0.3791]])",0.95491296
tensorflow.python.ops.resource_variables_toggle.resource_variables_enabled,"Returns `True` if resource variables are enabled. Resource variables are improved versions of TensorFlow variables with a well-defined memory model. Accessing a resource variable reads its value, and all ops which access a specific read value of the variable are guaranteed to see the same value for that tensor. Writes which happen after a read (by having a control or data dependency on the read) are guaranteed not to affect the value of the read tensor, and similarly writes which happen before a read are guaranteed to affect the value. No guarantees are made about unordered read/write pairs. Calling tf.enable_resource_variables() lets you opt-in to this TensorFlow 2.0 feature.",torch.nn.modules.module.get_extra_state,Return any extra state to include in the module's state_dict. Implement this and a corresponding :func:`set_extra_state` for your module if you need to store extra state. This function is called when building the module's `state_dict()`. Note that extra state should be picklable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: object: Any extra state to store in the module's state_dict,0.95487064
tensorflow.python.data.experimental.ops.pad_to_cardinality.pad_to_cardinality,"Pads a dataset with fake elements to reach the desired cardinality. The dataset to pad must have a known and finite cardinality and contain dictionary elements. The `mask_key` will be added to differentiate between real and padding elements -- real elements will have a `<mask_key>=True` entry while padding elements will have a `<mask_key>=False` entry. Example usage: >>> ds = tf.data.Dataset.from_tensor_slices({'a': [1, 2]}) >>> ds = ds.apply(tf.data.experimental.pad_to_cardinality(3)) >>> list(ds.as_numpy_iterator()) [{'a': 1, 'valid': True}, {'a': 2, 'valid': True}, {'a': 0, 'valid': False}] This can be useful, e.g. during eval, when partial batches are undesirable but it is also important not to drop any data. ds = ... # Round up to the next full batch. target_cardinality = -(-ds.cardinality() // batch_size) * batch_size ds = ds.apply(tf.data.experimental.pad_to_cardinality(target_cardinality)) # Set `drop_remainder` so that batch shape will be known statically. No data # will actually be dropped since the batch size divides the cardinality. ds = ds.batch(batch_size, drop_remainder=True) Args: cardinality: The cardinality to pad the dataset to. mask_key: The key to use for identifying real vs padding elements. Returns: A dataset transformation that can be applied via `Dataset.apply()`.",torch.autograd.graph.register_hook,"Register a backward hook. The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:: hook(grad_inputs: Tuple[Tensor], grad_outputs: Tuple[Tensor]) -> Tuple[Tensor] or None The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of :attr:`grad_inputs`. This function returns a handle with a method ``handle.remove()`` that removes the hook from the module. .. note:: See :ref:`backward-hooks-execution` for more information on how when this hook is executed, and how its execution is ordered relative to other hooks. Example:: >>> import torch >>> a = torch.tensor([0., 0., 0.], requires_grad=True) >>> b = a.clone() >>> assert isinstance(b.grad_fn, torch.autograd.graph.Node) >>> handle = b.grad_fn.register_hook(lambda gI, gO: (gO[0] * 2,)) >>> b.sum().backward(retain_graph=True) >>> print(a.grad) tensor([2., 2., 2.]) >>> handle.remove() # Removes the hook >>> a.grad = None >>> b.sum().backward(retain_graph=True) >>> print(a.grad) tensor([1., 1., 1.])",0.95486736
tensorflow.python.eager.polymorphic_function.function_type_utils.to_function_type,Generates FunctionType and default values from fullargspec.,torch.ao.quantization.quantize.get_default_custom_config_dict,Defines the default custom config dict.,0.95486635
tensorflow.python.keras.utils.data_utils.is_generator_or_sequence,Check if `x` is a Keras generator type.,torch._inductor.codegen.cpp.index_indirect_depends_on,Check if an index has free symbol CppCSEVariable that depends on `itervar`.,0.95483613
tensorflow.python.keras.backend.get_session,"Returns the TF session to be used by the backend. If a default TensorFlow session is available, we will return it. Else, we will return the global Keras session assuming it matches the current graph. If no global Keras session exists at this point: we will create a new global session. Note that you can manually set the global session via `K.set_session(sess)`. Args: op_input_list: An option sequence of tensors or ops, which will be used to determine the current graph. Otherwise the default graph will be used. Returns: A TensorFlow session.",torch.fx.experimental.symbolic_shapes.compute_unbacked_bindings,"After having run fake tensor propagation and producing example_value result, traverse example_value looking for freshly bound unbacked symbols and record their paths for later. It is an error if we have allocated an unbacked SymInt but it cannot be found in example_value. (NB: this means if you have a multi-output function, you must call this on the tuple of tensor output, you cannot wait!) The peek parameter lets you check out what the bindings are without changing the affected list. This is primarily useful for ensuring unbacked_var_to_val is promptly populated when propagate_real_tensors is on.",0.954833
tensorflow.python.util.deprecation.deprecated_endpoints,Decorator for marking endpoints deprecated. This decorator does not print deprecation messages. TODO(annarev): eventually start printing deprecation warnings when @deprecation_endpoints decorator is added. Args: *args: Deprecated endpoint names. Returns: A function that takes symbol as an argument and adds _tf_deprecated_api_names to that symbol. _tf_deprecated_api_names would be set to a list of deprecated endpoint names for the symbol.,torch.distributed.rpc.backend_registry.register_backend,Registers a new RPC backend. Args: backend_name (str): backend string to identify the handler. construct_rpc_backend_options_handler (function): Handler that is invoked when rpc_backend.construct_rpc_backend_options(**dict) is called. init_backend_handler (function): Handler that is invoked when the `_init_rpc_backend()` function is called with a backend. This returns the agent.,0.95482457
tensorflow.python.keras.regularizers.from_config,"Creates a regularizer from its config. This method is the reverse of `get_config`, capable of instantiating the same regularizer from the config dictionary. This method is used by saving and loading models to HDF5 formats, Keras model cloning, some visualization utilities, and exporting models to and from JSON. Args: config: A Python dictionary, typically the output of get_config. Returns: A regularizer instance.",torch._inductor.codegen.cuda.cuda_template.generate,Generates the CUDA template caller object for the given GEMM template and operation. This CUDATemplateCaller may be used to call and benchmark the generated CUDA kernel in a standalone manner to enable Autotuning. Args: kwargs: Additional keyword arguments. Returns: A CUDATemplateCaller object representing the generated CUDA template caller.,0.9547892
tensorflow.python.tpu.tpu_replication.outside_compilation,"Builds part of a computation outside any current TPU replicate scope. `tf.tpu.outside_compilation()` is used to run ops in `computation` on CPU instead of running on TPU. For example, users can run ops that are not supported on TPU's (e.g. tf.summary.write()) by explicitly placing those ops on CPU's. Below usage of outside compilation will place ops in `computation_with_string_ops` on CPU. Example usage: python def computation_with_string_ops(x): # strings types are not supported on TPU's and below ops must # run on CPU instead. output = tf.strings.format('1{}', x) return tf.strings.to_number(output) def tpu_computation(): # Expected output is 11. output = tf.tpu.outside_compilation(computation_with_string_ops, 1) Outside compilation should be called inside TPUReplicateContext. That is, `tf.tpu.outside_compilation()` should be called inside a function that is passed to `tpu.split_compile_and_replicate()` -- this is implied when outside compilation is invoked inside a function passed to TPUStrategy `run()`. If invoked outside of TPUReplicateContext, then this simply returns the result of `computation`, and therefore, would be a no-op. Note that outside compilation is different from `tf.distribute.experimental.TPUStrategy.merge_call()` as logic in outside compilation is replicated and executed separately for each replica. On the other hand, `merge_call()` requires a `merge_fn` to aggregate the inputs from different replicas and is executed only once. For variables placed in TPU device, which includes variables created inside TPUStrategy scope, outside compilation logic must not include variable read/write. For variables placed on host, variable read/write is only allowed if the variable is not accessed by any other ops in the TPU computation. Variable read/write from outside compilation cluster is not visible from TPU computation and vice versa. Therefore, if outside compilation logic contains such host variables read/write ops and if the variables are accessed by TPU",torch.distributed.fsdp.fully_sharded_data_parallel.summon_full_params,"Expose full params for FSDP instances with this context manager. Can be useful *after* forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the ``recurse`` argument. .. note:: This can be used on inner FSDPs. .. note:: This can *not* be used within a forward or backward pass. Nor can forward and backward be started from within this context. .. note:: Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward. .. note:: The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless ``writeback=False``, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when ``world_size == 1``, or ``NO_SHARD`` config, the modification is persisted regardless of ``writeback``. .. note:: This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units. .. warning:: Note that ``rank0_only=True`` in conjunction with ``writeback=True`` is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited. .. warning:: Note that ``offload_to_cpu`` and ``rank0_only=False`` will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use ``offload_to_cpu`` with ``rank0_only=True``. Args: recurse (bool, Optional): recursively summon all params for nested FSDP instances (default: True). writeback (bool, Optional): if ``False``, modifications to params are ",0.9547578
tensorflow.python.lib.io.tf_record_test.testReadGrowingFile_preservesReadOffset,"Verify that tf_record_iterator preserves read offset even after EOF. When a file is iterated to EOF, the iterator should raise StopIteration but not actually close the reader. Then if later new data is appended, the iterator should start returning that new data on the next call to next(), preserving the read offset. This behavior is required by TensorBoard.",torch.xpu.random.seed,"Set the seed for generating random numbers to a random number for the current GPU. It's safe to call this function if XPU is not available; in that case, it is silently ignored. .. warning:: If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use :func:`seed_all`.",0.9547508
tensorflow.python.autograph.operators.variables.ldu,"Load variable operator that returns Undefined when failing to evaluate. Note: the name (""load or return undefined"") is abbreviated to minimize the amount of clutter in generated code. This variant of `ld` is useful when loading symbols that may be undefined at runtime, such as composite symbols, and whether they are defined or not cannot be determined statically. For example `d['a']` is undefined when `d` is an empty dict. Args: load_v: Lambda that executes the actual read. name: Human-readable name of the symbol being read. Returns: Either the value of the symbol, or Undefined, if the symbol is not fully defined.",torch._dynamo.bytecode_transformation.create_load_global,"`name` is the name of the global to be loaded. `push_null` specifies whether or not a NULL should be pushed to the stack before the global (Python 3.11+ only). Python 3.11 changed the LOAD_GLOBAL instruction in that the first bit of the instruction arg specifies whether a NULL should be pushed to the stack before the global. The remaining bits of the instruction arg contain the name index. See `create_call_function` for why this NULL is needed. The instruction's `arg` is actually computed when assembling the bytecode. For Python 3.11, push_null information is propagated through the arg. NOTE: we don't use create_instruction since LOAD_GLOBAL is the only instruction where both arg and argval need to be specified.",0.95469505
tensorflow.python.feature_column.feature_column_v2.add_resource,"Creates a new resource. Resources can be things such as tables, variables, trackables, etc. Args: feature_column: A `FeatureColumn` object this resource corresponds to. name: Name of the resource. resource: The resource. Returns: The created resource.",torch.package.package_importer.load_binary,"Load raw bytes. Args: package (str): The name of module package (e.g. ``""my_package.my_subpackage""``). resource (str): The unique name for the resource. Returns: bytes: The loaded data.",0.9546875
tensorflow.python.ops.nn_impl_distribute.compute_average_loss,"Scales per-example losses with sample_weights and computes their average. Usage with distribution strategy and custom training loop: python with strategy.scope(): def compute_loss(labels, predictions, sample_weight=None): # If you are using a `Loss` class instead, set reduction to `NONE` so that # we can do the reduction afterwards and divide by global batch size. per_example_loss = tf.keras.losses.sparse_categorical_crossentropy( labels, predictions) # Compute loss that is scaled by sample_weight and by global batch size. return tf.nn.compute_average_loss( per_example_loss, sample_weight=sample_weight, global_batch_size=GLOBAL_BATCH_SIZE) Args: per_example_loss: Per-example loss. sample_weight: Optional weighting for each example. global_batch_size: Optional global batch size value. Defaults to (size of first dimension of `losses`) * (number of replicas). Returns: Scalar loss value, obtained by summing the `per_example_loss` and dividing by `global_batch_size`. If `global_batch_size` is zero, the result is zero.",torch.utils.benchmark.utils.timer.adaptive_autorange,"Similar to `blocked_autorange` but also checks for variablility in measurements and repeats until iqr/median is smaller than `threshold` or `max_run_time` is reached. At a high level, adaptive_autorange executes the following pseudo-code:: `setup` times = [] while times.sum < max_run_time start = timer() for _ in range(block_size): `stmt` times.append(timer() - start) enough_data = len(times)>3 and times.sum > min_run_time small_iqr=times.iqr/times.mean<threshold if enough_data and small_iqr: break Args: threshold: value of iqr/median threshold for stopping min_run_time: total runtime needed before checking `threshold` max_run_time: total runtime for all measurements regardless of `threshold` Returns: A `Measurement` object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)",0.9546797
tensorflow.python.keras.distribute.distributed_training_utils_v1.unwrap_outputs,"Unwrap the list of outputs contained in the PerReplica parameters. This function calls `flatten_per_replica_values` to parse each of the input parameters into a list of outputs on the different devices. If we set `with_loss_tensor` to be True, we also call `reduce` on the list of losses on the different devices to give us one loss tensor. Args: distribution_strategy: DistributionStrategy used to distribute training and validation. grouped_outputs: PerReplica outputs returned from the train or test function that we ran on each device. with_loss_tensor: Boolean that indicates if we need to add the reduced loss tensor as one of the outputs. Returns: Values of each of the PerReplica outputs.",torch._functorch.partitioners.min_cut_rematerialization_partition,"Partitions the joint graph such that the backward recomputes the forward. Recomputing helps in trading off memory bandwidth with computation. To create the fwd and bwd graph, we copy the joint graph, manually set the outputs to just original forward or backward outputs. And then we run the resulting graphs through dead code elimination. .. warning:: This API is experimental and likely to change. Args: joint_module(fx.GraphModule): The joint forward and backward graph. This is the result of AOT Autograd tracing. _joint_inputs: The inputs to the joint graph. This is unused. compiler: This option determines the default set of recomputable ops. Currently, there are two options: ``nvfuser`` and ``inductor``. recomputable_ops: This is an optional set of recomputable ops. If this is not None, then this set of ops will be used instead of the default set of ops. num_fwd_outputs: The number of outputs from the forward graph. Returns: Returns the generated forward and backward Fx graph modules.",0.95466214
tensorflow.python.training.monitored_session.run_step_fn,"Run ops using a step function. Args: step_fn: A function or a method with a single argument of type `StepContext`. The function may use methods of the argument to perform computations with access to a raw session. The returned value of the `step_fn` will be returned from `run_step_fn`, unless a stop is requested. In that case, the next `should_stop` call will return True. Example usage: python with tf.Graph().as_default(): c = tf.compat.v1.placeholder(dtypes.float32) v = tf.add(c, 4.0) w = tf.add(c, 0.5) def step_fn(step_context): a = step_context.session.run(fetches=v, feed_dict={c: 0.5}) if a <= 4.5: step_context.request_stop() return step_context.run_with_hooks(fetches=w, feed_dict={c: 0.1}) with tf.MonitoredSession() as session: while not session.should_stop(): a = session.run_step_fn(step_fn) Hooks interact with the `run_with_hooks()` call inside the `step_fn` as they do with a `MonitoredSession.run` call. Returns: Returns the returned value of `step_fn`. Raises: StopIteration: if `step_fn` has called `request_stop()`. It may be caught by `with tf.MonitoredSession()` to close the session. ValueError: if `step_fn` doesn't have a single argument called `step_context`. It may also optionally have `self` for cases when it belongs to an object.",torch.distributed.checkpoint.state_dict_saver.async_save,"Asynchronous version of ``save``. This code first de-stages the state_dict on CPU, and then calls `save` in a separate thread. .. warning:: This feature is experimental and subject to change. Args: state_dict (Dict[str, Any]): The state_dict to save. checkpoint_id (Union[str, os.PathLike, None]): The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: ``None``) storage_writer (Optional[StorageWriter]): Instance of StorageWriter used to perform writes. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: ``None``) planner (Optional[SavePlanner]): Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: ``None``) process_group (Optional[ProcessGroup]): ProcessGroup to be used for cross-rank synchronization. (Default: ``None``) Returns: Future: A future holding the resultant Metadata object from `save`. Example: >>> # xdoctest: +SKIP >>> my_model = MyModule() >>> state_dict = {""model"": my_model} >>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(""/checkpoint/1"") >>> checkpoint_future = torch.distributed.checkpoint.async_save( >>> state_dict=state_dict, >>> storage_writer=fs_storage_writer, >>> ) >>> >>> # ... do some work ... >>> >>> checkpoint_future.result()",0.9546536
tensorflow.python.ops.ragged.ragged_to_tensor_op_test.rebuild_ragged_tensor_with_value_rowids,"Returns a copy of `rt`, built using `from_value_rowids`. This ensures that RaggedTensor._cached_value_rowids is populated, which triggers a different code-path for converting ragged tensors to tensors. If `feed_dict` and `sess` are specified, then build the new `RaggedTensor` using placeholder tensors, and populate a feed dictionary that can be used to feed the placeholders. Args: rt: The RaggedTensor to copy. feed_dict: If specified, then build the new `RaggedTensor` using placeholders, and populate this dict with entries to feed those placeholders. sess: A session used to evaluate tensors; required if feed_dict is specified. Returns: A copy of `rt`, built using `from_value_rowids`.",torch.package.package_exporter.save_source_string,"Adds ``src`` as the source code for ``module_name`` in the exported package. Args: module_name (str): e.g. ``my_package.my_subpackage``, code will be saved to provide code for this package. src (str): The Python source code to save for this package. is_package (bool, optional): If ``True``, this module is treated as a package. Packages are allowed to have submodules (e.g. ``my_package.my_subpackage.my_subsubpackage``), and resources can be saved inside them. Defaults to ``False``. dependencies (bool, optional): If ``True``, we scan the source for dependencies.",0.9546517
tensorflow.python.keras.utils.tf_inspect.getfullargspec,"TFDecorator-aware replacement for `inspect.getfullargspec`. This wrapper emulates `inspect.getfullargspec` in[^)]* Python2. Args: obj: A callable, possibly decorated. Returns: The `FullArgSpec` that describes the signature of the outermost decorator that changes the callable's signature. If the callable is not decorated, `inspect.getfullargspec()` will be called directly on the callable.",torch.package.package_exporter.register_mock_hook,"Registers a mock hook on the exporter. The hook will be called each time a module matches against a :meth:`mock` pattern. It should have the following signature:: hook(exporter: PackageExporter, module_name: str) -> None Hooks will be called in order of registration. Returns: :class:`torch.utils.hooks.RemovableHandle`: A handle that can be used to remove the added hook by calling ``handle.remove()``.",0.95460904
tensorflow.python.keras.backend.less,Element-wise truth value of (x < y). Args: x: Tensor or variable. y: Tensor or variable. Returns: A bool tensor.,torch.ao.ns.fx.utils.compute_normalized_l2_error,Computes the normalized L2 error between `x` and `y`. Args: x: Tensor or tuple of tensors y: Tensor or tuple of tensors Return: float or tuple of floats,0.9544973
tensorflow.python.tpu.tpu_embedding_v3.shape,Returns the shape of the embedding variable for the current context.,torch.export.exported_program.named_parameters,"Returns an iterator over original module parameters, yielding both the name of the parameter as well as the parameter itself.",0.95445055
tensorflow.python.keras.engine.base_layer.input_spec,"`InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: python self.input_spec = tf.keras.layers.InputSpec(ndim=4) Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof.",torch.nn.functional.softmax,"Apply a softmax function. Softmax is defined as: :math:`\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}` It is applied to all slices along dim, and will re-scale them so that the elements lie in the range `[0, 1]` and sum to 1. See :class:`~torch.nn.Softmax` for more details. Args: input (Tensor): input dim (int): A dimension along which softmax will be computed. dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. If specified, the input tensor is casted to :attr:`dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None. .. note:: This function doesn't work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it's faster and has better numerical properties).",0.9544488
tensorflow.python.saved_model.path_helpers.get_debug_dir,Returns path to the debug sub-directory in the SavedModel.,torch.mps.event.record,Records the event in the default stream.,0.9544348
tensorflow.python.ops.stateful_random_ops.create_rng_state,"Creates a RNG state from an integer or a vector. Example: >>> tf.random.create_rng_state( ... 1234, ""philox"") <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234, 0, 0])> >>> tf.random.create_rng_state( ... [12, 34], ""threefry"") <tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])> Args: seed: an integer or 1-D numpy array. alg: the RNG algorithm. Can be a string, an `Algorithm` or an integer. Returns: a 1-D numpy array whose size depends on the algorithm.",torch.utils.data.dataset.random_split,"Randomly split a dataset into non-overlapping new datasets of given lengths. If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided. After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left. Optionally fix the generator for reproducible results, e.g.: Example: >>> # xdoctest: +SKIP >>> generator1 = torch.Generator().manual_seed(42) >>> generator2 = torch.Generator().manual_seed(42) >>> random_split(range(10), [3, 7], generator=generator1) >>> random_split(range(30), [0.3, 0.3, 0.4], generator=generator2) Args: dataset (Dataset): Dataset to be split lengths (sequence): lengths or fractions of splits to be produced generator (Generator): Generator used for the random permutation.",0.9544129
tensorflow.python.keras.backend.normalize_batch_in_training,"Computes mean and std for batch then apply batch_normalization on batch. Args: x: Input tensor or variable. gamma: Tensor by which to scale the input. beta: Tensor with which to center the input. reduction_axes: iterable of integers, axes over which to normalize. epsilon: Fuzz factor. Returns: A tuple length of 3, `(normalized_tensor, mean, variance)`.",torch.distributed._shard.sharded_tensor._ops.init.uniform_,"Fills the Tensor in tensor.local_shards with values drawn from the uniform distribution :math:`\mathcal{U}(a, b)`. Args: tensor: tensor sharded across devices a: the lower bound of the uniform distribution b: the upper bound of the uniform distribution",0.9543778
tensorflow.python.util.type_annotations.is_forward_ref,Returns true if `tp` is a typing forward reference.,torch._dynamo.comptime.is_python_constant,Returns True if as_python_constant would succeed.,0.9543673
tensorflow.python.framework.func_graph.check_func_mutation,Checks that the arguments to a function are not modified.,torch.testing._internal.jit_utils.checkScript,Checks that a given script generates the same output as the Python version using the given inputs.,0.9543356
tensorflow.python.eager.context.enable_graph_collection,Enables graph collection of executed functions. To retrieve the accumulated graphs call context.export_run_metadata() and to stop collecting graphs call context.disable_graph_collection().,torch.cuda.graphs.debug_dump,Arguments: debug_path (required): Path to dump the graph to. Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode(),0.9542705
tensorflow.python.training.monitored_session.MonitoredTrainingSession,"Creates a `MonitoredSession` for training. For a chief, this utility sets proper session initializer/restorer. It also creates hooks related to checkpoint and summary saving. For workers, this utility sets proper session creator which waits for the chief to initialize/restore. Please check `tf.compat.v1.train.MonitoredSession` for more information. @compatibility(TF2) This API is not compatible with eager execution and `tf.function`. To migrate to TF2, rewrite the code to be compatible with eager execution. Check the [migration guide](https://www.tensorflow.org/guide/migrate#1_replace_v1sessionrun_calls) on replacing `Session.run` calls. In Keras, session hooks can be replaced by Callbacks e.g. [logging hook notebook]( https://github.com/tensorflow/docs/blob/master/site/en/guide/migrate/logging_stop_hook.ipynb) For more details please read [Better performance with tf.function](https://www.tensorflow.org/guide/function). @end_compatibility Args: master: `String` the TensorFlow master to use. is_chief: If `True`, it will take care of initialization and recovery the underlying TensorFlow session. If `False`, it will wait on a chief to initialize or recover the TensorFlow session. checkpoint_dir: A string. Optional path to a directory where to restore variables. scaffold: A `Scaffold` used for gathering or building supportive ops. If not specified, a default one is created. It's used to finalize the graph. hooks: Optional list of `SessionRunHook` objects. chief_only_hooks: list of `SessionRunHook` objects. Activate these hooks if `is_chief==True`, ignore otherwise. save_checkpoint_secs: The frequency, in seconds, that a checkpoint is saved using a default checkpoint saver. If both `save_checkpoint_steps` and `save_checkpoint_secs` are set to `None`, then the default checkpoint saver isn't used. If both are provided, then only `save_checkpoint_secs` is used. Default 600. save_summaries_steps: The frequency, in number of global steps, that the summaries are written to dis",torch.utils.cpp_extension.CUDAExtension,"Create a :class:`setuptools.Extension` for CUDA/C++. Convenience method that creates a :class:`setuptools.Extension` with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library. All arguments are forwarded to the :class:`setuptools.Extension` constructor. Full list arguments can be found at https://setuptools.pypa.io/en/latest/userguide/ext_modules.html#extension-api-reference Example: >>> # xdoctest: +SKIP >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CPP_EXT) >>> from setuptools import setup >>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension >>> setup( ... name='cuda_extension', ... ext_modules=[ ... CUDAExtension( ... name='cuda_extension', ... sources=['extension.cpp', 'extension_kernel.cu'], ... extra_compile_args={'cxx': ['-g'], ... 'nvcc': ['-O2']}, ... extra_link_flags=['-Wl,--no-as-needed', '-lcuda']) ... ], ... cmdclass={ ... 'build_ext': BuildExtension ... }) Compute capabilities: By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that's newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX). You can override the default behavior using `TORCH_CUDA_ARCH_LIST` to explicitly specify which CCs you want the extension to support: ``TORCH_CUDA_ARCH_LIST=""6.1 8.6"" python build_my_extension.py`` ``TORCH_CUDA_ARCH_LIST=""5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX"" python build_my_extension.py`` The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the ",0.95424867
tensorflow.python.keras.backend.zeros_like,"Instantiates an all-zeros variable of the same shape as another tensor. Args: x: Keras variable or Keras tensor. dtype: dtype of returned Keras variable. `None` uses the dtype of `x`. name: name for the variable to create. Returns: A Keras variable with the shape of `x` filled with zeros. Example: python from tensorflow.keras import backend as K kvar = K.variable(np.random.random((2,3))) kvar_zeros = K.zeros_like(kvar) K.eval(kvar_zeros) # array([[ 0., 0., 0.], [ 0., 0., 0.]], dtype=float32)",torch.nn.functional.upsample_nearest,"Upsamples the input, using nearest neighbours' pixel values. .. warning:: This function is deprecated in favor of :func:`torch.nn.functional.interpolate`. This is equivalent with ``nn.functional.interpolate(..., mode='nearest')``. Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional). Args: input (Tensor): input size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia size. scale_factor (int): multiplier for spatial size. Has to be an integer. Note: {backward_reproducibility_note}",0.954239
tensorflow.python.distribute.distribute_lib.non_slot_devices,"Device(s) for non-slot variables. DEPRECATED: TF 1.x ONLY. This method returns non-slot devices where non-slot variables are placed. Users can create non-slot variables on these devices by using a block: python with tf.distribute.StrategyExtended.colocate_vars_with(tf.distribute.StrategyExtended.non_slot_devices(...)): ... Args: var_list: The list of variables being optimized, needed with the default `tf.distribute.Strategy`. Returns: A sequence of devices for non-slot variables.",torch.distributed._tensor.random.is_rng_supported_mesh,Checks if the current device of `device_mesh` supports DTensor's random APIs. Currently DTensor Random APIs only supports cuda/cuda-like devices. We suggest users call this API to test the availability before using our random APIs. Args: device_mesh (:class:`DeviceMesh`): The device mesh on which we check if the random ops APIs are supported. Returns: A bool value. True if `device_mesh` supports DTensor Random APIs; False otherwise. .. warning:: Currently we only support correct RNG on cuda/cuda-like devices.,0.95423764
tensorflow.python.framework.ops.backing_device,"Returns the name of the device holding this tensor's memory. `.backing_device` is usually the same as `.device`, which returns the device on which the kernel of the operation that produced this tensor ran. However, some operations can produce tensors on a different device (e.g., an operation that executes on the GPU but produces output tensors in host memory).",torch.distributed.checkpoint.planner.commit_tensor,Call once the StorageReader finished loading data into ``tensor``. The provided tensor is the same one returned by the call to ``resolve_tensor``. This method is only needed if this LoadPlanner needs to post process ``tensor`` prior to copying it back to the one in the state_dict. The contents of tensor will follow its device synchronization model.,0.95421606
tensorflow.python.tools.api.generator.create_python_api.get_canonical_import,"Obtain one single import from a set of possible sources of a symbol. One symbol might come from multiple places as it is being imported and reexported. To simplify API changes, we always use the same import for the same module, and give preference based on higher priority and alphabetical ordering. Args: import_set: (set) Imports providing the same symbol. This is a set of tuples in the form (import, priority). We want to pick an import with highest priority. Returns: A module name to import",torch._export.serde.serialize.canonicalize,"Normalize a serialized ExportedProgram, so that different eager program which shares the same semantics can get a single representation on disk. This function canonicalizes an ExportedProgram by: 1. Sorting nodes in topological order. 2. Rename nodes to have unique names. 3. Remove unstable fields. 4. Aggregate the above program fields. 5. Recurse in subgraphs. Args: ep (ExportedProgram): The ExportedProgram to canonicalize. Returns: ExportedProgram: The canonicalized exported program.",0.954212
tensorflow.python.framework.ops.register_proto_function,"Registers `to_proto` and `from_proto` functions for collection_name. `to_proto` function converts a Python object to the corresponding protocol buffer, and returns the protocol buffer. `from_proto` function converts protocol buffer into a Python object, and returns the object.. Args: collection_name: Name of the collection. proto_type: Protobuf type, such as `saver_pb2.SaverDef`, `variable_pb2.VariableDef`, `queue_runner_pb2.QueueRunnerDef`.. to_proto: Function that implements Python object to protobuf conversion. from_proto: Function that implements protobuf to Python object conversion.",torch.distributed._composable.fsdp.fully_shard.register_fsdp_forward_method,"Registers a method on ``module`` to be a forward method for FSDP. FSDP only knows to run its pre-forward and post-forward hooks on the default :meth:`nn.Module.forward` method. This function patches a user specified method to run the pre/post-forward hooks before/after the method, respectively. If ``module`` is not an :class:`FSDPModule`, then this is a no-op. Args: module (nn.Module): Module to register the forward method on. method_name (str): Name of the forward method.",0.9541817
tensorflow.python.keras.engine.training.flatten_metrics_in_order,Turns the `logs` dict into a list as per key order of `metrics_names`.,torch.serialization.clear_safe_globals,Clears the list of globals that are safe for ``weights_only`` load.,0.95415974
tensorflow.python.data.kernel_tests.test_base.default_test_combinations,Returns the default test combinations for tf.data tests.,torch.testing._internal.opinfo.core.formatted_name,Returns a formatted full name for this OpInfo that can be used in test names.,0.9541525
tensorflow.python.util.dispatch.dispatch_for_api,"Decorator that overrides the default implementation for a TensorFlow API. The decorated function (known as the ""dispatch target"") will override the default implementation for the API when the API is called with parameters that match a specified type signature. Signatures are specified using dictionaries that map parameter names to type annotations. E.g., in the following example, `masked_add` will be called for `tf.add` if both `x` and `y` are `MaskedTensor`s: >>> class MaskedTensor(tf.experimental.ExtensionType): ... values: tf.Tensor ... mask: tf.Tensor >>> @dispatch_for_api(tf.math.add, {'x': MaskedTensor, 'y': MaskedTensor}) ... def masked_add(x, y, name=None): ... return MaskedTensor(x.values + y.values, x.mask & y.mask) >>> mt = tf.add(MaskedTensor([1, 2], [True, False]), MaskedTensor(10, True)) >>> print(f""values={mt.values.numpy()}, mask={mt.mask.numpy()}"") values=[11 12], mask=[ True False] If multiple type signatures are specified, then the dispatch target will be called if any of the signatures match. For example, the following code registers `masked_add` to be called if `x` is a `MaskedTensor` *or* `y` is a `MaskedTensor`. >>> @dispatch_for_api(tf.math.add, {'x': MaskedTensor}, {'y':MaskedTensor}) ... def masked_add(x, y): ... x_values = x.values if isinstance(x, MaskedTensor) else x ... x_mask = x.mask if isinstance(x, MaskedTensor) else True ... y_values = y.values if isinstance(y, MaskedTensor) else y ... y_mask = y.mask if isinstance(y, MaskedTensor) else True ... return MaskedTensor(x_values + y_values, x_mask & y_mask) The type annotations in type signatures may be type objects (e.g., `MaskedTensor`), `typing.List` values, or `typing.Union` values. For example, the following will register `masked_concat` to be called if `values` is a list of `MaskedTensor` values: >>> @dispatch_for_api(tf.concat, {'values': typing.List[MaskedTensor]}) ... def masked_concat(values, axis): ... return MaskedTensor(tf.concat([v.values for v in values], axis), ... tf.co",torch._functorch.functional_call.functional_call,"Performs a functional call on the module by replacing the module parameters and buffers with the provided ones. .. note:: If the module has active parametrizations, passing a value in the :attr:`parameter_and_buffer_dicts` argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as ``{submodule_name}.parametrizations.{parameter_name}.original``. .. note:: If the module performs in-place operations on parameters/buffers, these will be reflected in the ``parameter_and_buffer_dicts`` input. Example:: >>> a = {'foo': torch.zeros(())} >>> # xdoctest: +SKIP >>> mod = Foo() # does self.foo = self.foo + 1 >>> print(mod.foo) # tensor(0.) >>> functional_call(mod, a, torch.ones(())) >>> print(mod.foo) # tensor(0.) >>> print(a['foo']) # tensor(1.) .. note:: If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag. Example:: >>> a = {'foo': torch.zeros(())} >>> # xdoctest: +SKIP >>> mod = Foo() # has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied >>> print(mod.foo) # tensor(1.) >>> mod(torch.zeros(())) # tensor(2.) >>> functional_call(mod, a, torch.zeros(())) # tensor(0.) since it will change self.foo_tied too >>> functional_call(mod, a, torch.zeros(()), tie_weights=False) # tensor(1.)--self.foo_tied is not updated >>> new_a = {'foo': torch.zeros(()), 'foo_tied': torch.zeros(())} >>> functional_call(mod, new_a, torch.zeros()) # tensor(0.) An example of passing multiple dictionaries .. code-block:: python a = ({'weight': torch.ones(1, 1)}, {'buffer': torch.zeros(1)}) # two separate dictionaries mod = nn.Bar(1, 1) # return self.weight @ x + self.buffer print(mod.weight) # tensor(...) print(mod.buffer) # tensor(...) x = torch.randn((1, 1)) print(x) functional_call(mod, a, x) # same as x print(mod.weight) # same as before functional_call And here is an",0.9541442
tensorflow.python.eager.context.async_clear_error,"Clear pending operations and error statuses in async execution. In async execution mode, an error in op/function execution can lead to errors in subsequent ops/functions that are scheduled but not yet executed. Calling this method clears all pending operations and reset the async execution state. Example: while True: try: # Step function updates the metric `loss` internally train_step_fn() except tf.errors.OutOfRangeError: tf.experimental.async_clear_error() break logging.info('loss = %s', loss.numpy())",torch.distributed.collective_utils.all_gather,"A simple all_gather primitive with basic synchronization guard logic, by checking payload from all ranks has the same stage name. Args: data_or_fn: the data to be all gathered across ranks or function to be executed stage_name: the sync stage name for out-of-sync protection pg: the process group for sync Throws: RuntimeError from original exception trace Returns: a list of synced data from all ranks Example usage: >> all_ids = all_gather(data_or_fn=allocate_id, pg=ext_pg.my_pg)",0.9541248
tensorflow.python.framework.registry.register,"Registers a Python object ""candidate"" for the given ""name"". Args: candidate: The candidate object to add to the registry. name: An optional string specifying the registry key for the candidate. If None, candidate.__name__ will be used. Raises: KeyError: If same name is used twice.",torch.fx.graph.create_name,"Create a unique name. Arguments: candidate: used as the basis for the unique name, relevant to the user. obj: If not None, an object that will be associated with the unique name.",0.9541141
tensorflow.python.ops.image_ops_impl.psnr,"Returns the Peak Signal-to-Noise Ratio between a and b. This is intended to be used on signals (or images). Produces a PSNR value for each image in batch. The last three dimensions of input are expected to be [height, width, depth]. Example: python # Read images from file. im1 = tf.decode_png('path/to/im1.png') im2 = tf.decode_png('path/to/im2.png') # Compute PSNR over tf.uint8 Tensors. psnr1 = tf.image.psnr(im1, im2, max_val=255) # Compute PSNR over tf.float32 Tensors. im1 = tf.image.convert_image_dtype(im1, tf.float32) im2 = tf.image.convert_image_dtype(im2, tf.float32) psnr2 = tf.image.psnr(im1, im2, max_val=1.0) # psnr1 and psnr2 both have type tf.float32 and are almost equal. Args: a: First set of images. b: Second set of images. max_val: The dynamic range of the images (i.e., the difference between the maximum the and minimum allowed values). name: Namespace to embed the computation in. Returns: The scalar PSNR between a and b. The returned tensor has type `tf.float32` and shape [batch_size, 1].",torch.utils.tensorboard.writer.add_images,"Add batched image data to summary. Note that this requires the ``pillow`` package. Args: tag (str): Data identifier img_tensor (torch.Tensor, numpy.ndarray, or string/blobname): Image data global_step (int): Global step value to record walltime (float): Optional override default walltime (time.time()) seconds after epoch of event dataformats (str): Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc. Shape: img_tensor: Default is :math:`(N, 3, H, W)`. If ``dataformats`` is specified, other shape will be accepted. e.g. NCHW or NHWC. Examples:: from torch.utils.tensorboard import SummaryWriter import numpy as np img_batch = np.zeros((16, 3, 100, 100)) for i in range(16): img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i writer = SummaryWriter() writer.add_images('my_image_batch', img_batch, 0) writer.close() Expected result: .. image:: _static/img/tensorboard/add_images.png :scale: 30 %",0.9541048
tensorflow.python.keras.backend.categorical_crossentropy,"Categorical crossentropy between an output tensor and a target tensor. Args: target: A tensor of the same shape as `output`. output: A tensor resulting from a softmax (unless `from_logits` is True, in which case `output` is expected to be the logits). from_logits: Boolean, whether `output` is the result of a softmax, or is a tensor of logits. axis: Int specifying the channels axis. `axis=-1` corresponds to data format `channels_last`, and `axis=1` corresponds to data format `channels_first`. Returns: Output tensor. Raises: ValueError: if `axis` is neither -1 nor one of the axes of `output`. Example: >>> a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3]) >>> print(a) tf.Tensor( [[1. 0. 0.] [0. 1. 0.] [0. 0. 1.]], shape=(3, 3), dtype=float32) >>> b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], shape=[3,3]) >>> print(b) tf.Tensor( [[0.9 0.05 0.05] [0.05 0.89 0.06] [0.05 0.01 0.94]], shape=(3, 3), dtype=float32) >>> loss = tf.keras.backend.categorical_crossentropy(a, b) >>> print(np.around(loss, 5)) [0.10536 0.11653 0.06188] >>> loss = tf.keras.backend.categorical_crossentropy(a, a) >>> print(np.around(loss, 5)) [0. 0. 0.]",torch.sparse.semi_structured.to_sparse_semi_structured,"This function converts a dense tensor into a sparse semi-structured tensor. It will return a SparseSemiStructuredTensor, a subclass of torch.Tensor. This function will check to ensure the dense tensor has the right dtype, size, dims, and device. We currently only support semi-structured sparse tensors for 2d CUDA tensors. Additionally, your tensor must be a positive multiple of the mininum sparse block size, given in `_DTYPE_TO_SHAPE_CONSTRAINTS` for each dtype (float32, float16, bfloat16, int8). Args: original_tensor (Tensor): the dense tensor to convert transposed (bool, optional): deprecated arg to be removed in another release. Do not use. Returns: SparseSemiStructuredTensor: A sparse semi-structured tensor created from the given original_tensor Raises: None Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) >>> A = torch.Tensor([0, 0, 1, 1]).tile((128, 32)).half().cuda() tensor([[0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], ..., [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.], [0., 0., 1., ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16) >>> A_sparse = to_sparse_semi_structured(A) SparseSemiStructuredTensor(shape=torch.Size([128, 128])) >>> A_sparse.values() tensor([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), >>> A_sparse.indices() tensor([[-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], ..., [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370], [-4370, -4370, -4370, ..., -4370, -4370, -4370]], device='cuda:0', dtype=torch.int16))",0.95409083
tensorflow.python.ops.array_ops.quantize_and_dequantize_v2,"Quantizes then dequantizes a tensor. Updates the gradient definition for quantization that is outside the range to be 0.To simulate the V1 the behavior of tf.quantization.quantize_and_dequantize(...) use tf.grad_pass_through(tf.quantization.quantize_and_dequantize_v2)(...). Example usage: python def getQuantizeOp(input): input_tensor = tf.placeholder(tf.float32, shape=[4, 4]) net = tf.quantization.quantize_and_dequantize(input, input_min=min_threshold, input_max=max_threshold, range_given=True) To simulate v1 behavior: def testDecomposeQuantizeDequantize(self): def f(input_tensor): return tf.quantization.quantize_and_dequantize_v2(input_tensor, input_min = 5.0, input_max= -10.0, range_given=True) input_tensor = tf.placeholder(tf.float32, shape=[4, 4]) net = tf.grad_pass_through(f)(input_tensor) Args: input: A `Tensor` to quantize and dequantize. input_min: If range_given=True, the minimum input value, that needs to be represented in the quantized representation. If axis is specified, this should be a vector of minimum values for each slice along axis. input_max: If range_given=True, the maximum input value that needs to be represented in the quantized representation. If axis is specified, this should be a vector of maximum values for each slice along axis. signed_input: True if the quantization is signed or unsigned. num_bits: The bitwidth of the quantization. range_given: If true use `input_min` and `input_max` for the range of the input, otherwise determine min and max from the input `Tensor`. round_mode: Rounding mode when rounding from float values to quantized ones. one of ['HALF_TO_EVEN', 'HALF_UP'] name: Optional name for the operation. narrow_range: If true, then the absolute value of the quantized minimum value is the same as the quantized maximum value, instead of 1 greater. i.e. for 8 bit quantization, the minimum value is -127 instead of -128. axis: Integer. If specified, refers to a dimension of the input tensor, such that quantization will be per slice",torch.testing._internal.common_quantization.checkGraphModeFxOp,"Quantizes model with graph mode quantization on fx and check if the quantized model contains the quantized_node Args: model: floating point torch.nn.Module inputs: one positional sample input arguments for model expected_node: NodeSpec e.g. NodeSpec.call_function(torch.quantize_per_tensor) expected_node_occurrence: a dict from NodeSpec to expected number of occurrences (int) e.g. {NodeSpec.call_function(torch.quantize_per_tensor) : 1, NodeSpec.call_method('dequantize'): 1} expected_node_list: a list of NodeSpec, used to check the order of the occurrence of Node e.g. [NodeSpec.call_function(torch.quantize_per_tensor), NodeSpec.call_module(nnq.Conv2d), NodeSpec.call_function(F.hardtanh_), NodeSpec.call_method('dequantize')] is_reference: if True, enables reference mode print_debug_info: if True, prints debug info custom_qconfig_dict: overrides default qconfig_dict prepare_expected_node: same as expected_node, but for prepare prepare_expected_node_occurrence: same as expected_node_occurrence, but for prepare prepare_expected_node_list: same as expected_node_list, but for prepare Returns: A dictionary with the following structure: { ""prepared"": ..., # the prepared model ""quantized"": ..., # the quantized non-reference model ""quantized_reference"": ..., # the quantized reference model ""result"": ..., # the result for either quantized or # quantized_reference model depending on the # is_reference argument }",0.95404685
tensorflow.python.keras.mixed_precision.loss_scale_optimizer.update,"Updates the value of the loss scale. Args: grads: A nested structure of unscaled gradients, each which is an all-reduced gradient of the loss with respect to a weight. Returns: update_op: In eager mode, None. In graph mode, an op to update the loss scale. should_apply_gradients: Either a bool or a scalar boolean tensor. If False, the caller should skip applying `grads` to the variables this step.",torch.distributed._composable.fsdp.fully_shard.set_requires_gradient_sync,"Sets if the module should sync gradients. This can be used to implement gradient accumulation without communication. For HSDP, this controls both reduce-scatter and all-reduce together. Args: requires_gradient_sync (bool): Whether to reduce gradients for the module's parameters. recurse (bool): Whether to set for all submodules or just the passed-in module.",0.9540411
tensorflow.python.distribute.multi_worker_util.collective_leader,"Return the job name for the leader of for collective ops. Args: cluster_spec: a dict, `ClusterDef` or `ClusterSpec` object specifying the cluster configurations. task_type: the task type in the cluster. task_id: the task id in the cluster. Returns: a string indicating the leader job name or empty string if no need to set leader job.",torch.distributed.distributed_c10d.get_backend_config,"Return the backend configuration of the given process group. Args: group (ProcessGroup, optional): The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of :attr:`group`. Returns: The backend configuration of the given process group as a lower case string.",0.9540006
tensorflow.python.tpu.tpu_test_wrapper.run_user_main,"Runs the ""if __name__ == '__main__'"" at the bottom of a module. TensorFlow practice is to have a main if at the bottom of the module which might call an API compat function before calling test.main(). Since this is a statement, not a function, we can't cleanly reference it, but we can inspect it from the user module and run it in the context of that module so all imports and variables are available to it. Args: wrapped_test_module: The user-provided test code to run. Raises: NotImplementedError: If main block was not found in module. This should not be caught, as it is likely an error on the user's part -- absltest is all too happy to report a successful status (and zero tests executed) if a user forgets to end a class with ""test.main()"".",torch.serialization.check_module_version_greater_or_equal,"Check if a module's version satisfies requirements Usually, a module's version string will be like 'x.y.z', which would be represented as a tuple (x, y, z), but sometimes it could be an unexpected format. If the version string does not match the given tuple's format up to the length of the tuple, then error and exit or emit a warning. Args: module: the module to check the version of req_version_tuple: tuple (usually of ints) representing the required version error_if_malformed: whether we should exit if module version string is malformed Returns: requirement_is_met: bool",0.95399797
tensorflow.python.keras.layers.merge.average,"Functional interface to the `tf.keras.layers.Average` layer. Example: >>> x1 = np.ones((2, 2)) >>> x2 = np.zeros((2, 2)) >>> y = tf.keras.layers.Average()([x1, x2]) >>> y.numpy().tolist() [[0.5, 0.5], [0.5, 0.5]] Usage in a functional model: >>> input1 = tf.keras.layers.Input(shape=(16,)) >>> x1 = tf.keras.layers.Dense(8, activation='relu')(input1) >>> input2 = tf.keras.layers.Input(shape=(32,)) >>> x2 = tf.keras.layers.Dense(8, activation='relu')(input2) >>> avg = tf.keras.layers.Average()([x1, x2]) >>> out = tf.keras.layers.Dense(4)(avg) >>> model = tf.keras.models.Model(inputs=[input1, input2], outputs=out) Args: inputs: A list of input tensors (at least 2). **kwargs: Standard layer keyword arguments. Returns: A tensor, the average of the inputs. Raises: ValueError: If there is a shape mismatch between the inputs and the shapes cannot be broadcasted to match.",torch.nn.grad.conv2d_input,"Compute the gradient of conv2d with respect to the input of the convolution. This is same as the 2D transposed convolution operator under the hood but requires the shape of the gradient w.r.t. input to be specified explicitly. Args: input_size : Shape of the input gradient tensor weight: weight tensor (out_channels x in_channels/groups x kH x kW) grad_output : output gradient tensor (minibatch x out_channels x oH x oW) stride (int or tuple, optional): Stride of the convolution. Default: 1 padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0 dilation (int or tuple, optional): Spacing between kernel elements. Default: 1 groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1 Examples:: >>> input = torch.randn(1, 1, 3, 3, requires_grad=True) >>> weight = torch.randn(1, 1, 1, 2, requires_grad=True) >>> output = F.conv2d(input, weight) >>> grad_output = torch.randn(output.shape) >>> grad_input = torch.autograd.grad(output, input, grad_output) >>> F.grad.conv2d_input(input.shape, weight, grad_output)",0.95398355
tensorflow.python.training.supervisor.session_manager,Return the SessionManager used by the Supervisor. Returns: A SessionManager object.,torch.autograd.profiler_util.total_average,Averages all events. Returns: A FunctionEventAvg object.,0.953977
tensorflow.python.data.ops.dataset_ops.take,"Creates a `Dataset` with at most `count` elements from this dataset. >>> dataset = tf.data.Dataset.range(10) >>> dataset = dataset.take(3) >>> list(dataset.as_numpy_iterator()) [0, 1, 2] Args: count: A `tf.int64` scalar `tf.Tensor`, representing the number of elements of this dataset that should be taken to form the new dataset. If `count` is -1, or if `count` is greater than the size of this dataset, the new dataset will contain all elements of this dataset. name: (Optional.) A name for the tf.data operation. Returns: A new `Dataset` with the transformation applied as described above.",torch.nn.functional.softmin,"Apply a softmin function. Note that :math:`\text{Softmin}(x) = \text{Softmax}(-x)`. See softmax definition for mathematical formula. See :class:`~torch.nn.Softmin` for more details. Args: input (Tensor): input dim (int): A dimension along which softmin will be computed (so every slice along dim will sum to 1). dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor. If specified, the input tensor is casted to :attr:`dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",0.9539596
tensorflow.python.autograph.core.converter.get_definition_directive,"Returns the unique directive argument for a symbol. See lang/directives.py for details on directives. Example: # Given a directive in the code: ag.foo_directive(bar, baz=1) # One can write for an AST node Name(id='bar'): get_definition_directive(node, ag.foo_directive, 'baz') Args: node: ast.AST, the node representing the symbol for which the directive argument is needed. directive: Callable[..., Any], the directive to search. arg: str, the directive argument to return. default: Any Raises: ValueError: if conflicting annotations have been found",torch.fx.interpreter.call_module,Execute a ``call_module`` node and return the result. Args: target (Target): The call target for this node. See `Node <https://pytorch.org/docs/main/fx.html#torch.fx.Node>`__ for details on semantics args (Tuple): Tuple of positional args for this invocation kwargs (Dict): Dict of keyword arguments for this invocation Return Any: The value returned by the module invocation,0.9539578
tensorflow.python.framework.meta_graph.copy_scoped_meta_graph,"Copies a sub-meta_graph from one scope to another. Args: from_scope: `String` name scope containing the subgraph to be copied. to_scope: `String` name scope under which the copied subgraph will reside. from_graph: Optional `Graph` from which to copy the subgraph. If `None`, the default graph is use. to_graph: Optional `Graph` to which to copy the subgraph. If `None`, the default graph is used. Returns: A dictionary of `Variables` that has been copied into `to_scope`. Raises: ValueError: If `from_scope` and `to_scope` are the same while `from_graph` and `to_graph` are also the same.",torch.fx.graph.graph_copy,"Copy all nodes from a given graph into ``self``. Args: g (Graph): The source graph from which to copy Nodes. val_map (Dict[Node, Node]): a dictionary that will be populated with a mapping from nodes in ``g`` to nodes in ``self``. Note that ``val_map`` can be passed in with values in it already to override copying of certain values. Returns: The value in ``self`` that is now equivalent to the output value in ``g``, if ``g`` had an ``output`` node. ``None`` otherwise.",0.9539553
tensorflow.python.keras.engine.training_utils_v1.get_composite_shape,Returns the shape of the passed composite tensor.,torch.distributed._shard.sharded_tensor.api.sharding_spec,Returns the ShardingSpec for the tensor.,0.9538808
tensorflow.python.ops.metrics_impl.mean_squared_error,"Computes the mean squared error between the labels and predictions. The `mean_squared_error` function creates two local variables, `total` and `count` that are used to compute the mean squared error. This average is weighted by `weights`, and it is ultimately returned as `mean_squared_error`: an idempotent operation that simply divides `total` by `count`. For estimation of the metric over a stream of data, the function creates an `update_op` operation that updates these variables and returns the `mean_squared_error`. Internally, a `squared_error` operation computes the element-wise square of the difference between `predictions` and `labels`. Then `update_op` increments `total` with the reduced sum of the product of `weights` and `squared_error`, and it increments `count` with the reduced sum of `weights`. If `weights` is `None`, weights default to 1. Use weights of 0 to mask values. Args: labels: A `Tensor` of the same shape as `predictions`. predictions: A `Tensor` of arbitrary shape. weights: Optional `Tensor` whose rank is either 0, or the same rank as `labels`, and must be broadcastable to `labels` (i.e., all dimensions must be either `1`, or the same as the corresponding `labels` dimension). metrics_collections: An optional list of collections that `mean_squared_error` should be added to. updates_collections: An optional list of collections that `update_op` should be added to. name: An optional variable_scope name. Returns: mean_squared_error: A `Tensor` representing the current mean, the value of `total` divided by `count`. update_op: An operation that increments the `total` and `count` variables appropriately and whose value matches `mean_squared_error`. Raises: ValueError: If `predictions` and `labels` have mismatched shapes, or if `weights` is not `None` and its shape doesn't match `predictions`, or if either `metrics_collections` or `updates_collections` are not a list or tuple. RuntimeError: If eager execution is enabled.",torch.nn.functional.cross_entropy,"Compute the cross entropy loss between input logits and target. See :class:`~torch.nn.CrossEntropyLoss` for details. Args: input (Tensor) : Predicted unnormalized logits; see Shape section below for supported shapes. target (Tensor) : Ground truth class indices or class probabilities; see Shape section below for supported shapes. weight (Tensor, optional): a manual rescaling weight given to each class. If given, has to be a Tensor of size `C` size_average (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field :attr:`size_average` is set to ``False``, the losses are instead summed for each minibatch. Ignored when reduce is ``False``. Default: ``True`` ignore_index (int, optional): Specifies a target value that is ignored and does not contribute to the input gradient. When :attr:`size_average` is ``True``, the loss is averaged over non-ignored targets. Note that :attr:`ignore_index` is only applicable when the target contains class indices. Default: -100 reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per batch element instead and ignores :attr:`size_average`. Default: ``True`` reduction (str, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied, ``'mean'``: the sum of the output will be divided by the number of elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average` and :attr:`reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override :attr:`reduction`. Default: ``'mean'`` label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount of smoothing when com",0.95387906
tensorflow.python.eager.polymorphic_function.compiler_ir.compiler_ir_generator,"Gets the compiler IR bytes. Args: stage: The exported stage for the given function. device_name: The name of the device with the form as ""/job:localhost/replica:0/task:0/device:CPU:0"", ""/device:TPU:0"" etc. When this is used, actual device is needed for getting the compiler IR. platform_name: The name of the platform, e.g. ""TPU"". See the comment in `get_compiler_ir` in `context.py`. Returns: The compiler IR bytes.",torch.cuda.memory.memory_summary,"Return a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions. Args: device (torch.device or int, optional): selected device. Returns printout for the current device, given by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None`` (default). abbreviated (bool, optional): whether to return an abbreviated summary (default: False). .. note:: See :ref:`cuda-memory-management` for more details about GPU memory management.",0.9538411
tensorflow.python.ops.filesystem_ops.filesystem_set_configuration,Set configuration of the file system. Args: scheme: File system scheme. key: The name of the configuration option. value: The value of the configuration option. name: A name for the operation (optional). Returns: None.,torch.onnx._internal.exporter.get_op_functions,"Returns a list of ONNXFunctions for the given op: torch.ops.<namespace>.<op_name>.<overload>. The list is ordered by the time of registration. The custom operators should be in the second half of the list. Args: namespace: The namespace of the operator to get. op_name: The name of the operator to get. overload: The overload of the operator to get. If it's default overload, leave it to None. Returns: A list of ONNXFunctions corresponding to the given name, or None if the name is not in the registry.",0.9538317
tensorflow.python.ops.sparse_ops.from_dense,"Converts a dense tensor into a sparse tensor. Only elements not equal to zero will be present in the result. The resulting `SparseTensor` has the same dtype and shape as the input. >>> sp = tf.sparse.from_dense([0, 0, 3, 0, 1]) >>> sp.shape.as_list() [5] >>> sp.values.numpy() array([3, 1], dtype=int32) >>> sp.indices.numpy() array([[2], [4]]) Args: tensor: A dense `Tensor` to be converted to a `SparseTensor`. name: Optional name for the op. Returns: The `SparseTensor`.",torch.nn.init.orthogonal_,"Fill the input `Tensor` with a (semi) orthogonal matrix. Described in `Exact solutions to the nonlinear dynamics of learning in deep linear neural networks` - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened. Args: tensor: an n-dimensional `torch.Tensor`, where :math:`n \geq 2` gain: optional scaling factor generator: the torch Generator to sample from (default: None) Examples: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_LAPACK) >>> w = torch.empty(3, 5) >>> nn.init.orthogonal_(w)",0.95380205
tensorflow.python.eager.context.device,"Context-manager to force placement of operations and Tensors on a device. Example: python with tf.device('gpu:0'): with tf.device('cpu:0'): shape = tf.constant([], dtype=tf.int32) x = tf.random.truncated_normal(shape, tf.float32) will ensure that the `shape` Tensor is on CPU but the `truncated_normal` operation runs on GPU 0. Args: name: Name of the device (see context().devices()), or None to perform automatic placement. Returns: Context manager for setting the device.",torch.utils.tensorboard.writer.add_video,"Add video data to summary. Note that this requires the ``moviepy`` package. Args: tag (str): Data identifier vid_tensor (torch.Tensor): Video data global_step (int): Global step value to record fps (float or int): Frames per second walltime (float): Optional override default walltime (time.time()) seconds after epoch of event Shape: vid_tensor: :math:`(N, T, C, H, W)`. The values should lie in [0, 255] for type `uint8` or [0, 1] for type `float`.",0.9537916
tensorflow.python.data.ops.dataset_ops.get_legacy_output_types,"Returns the output shapes for elements of the input dataset / iterator. Args: dataset_or_iterator: A `tf.data.Dataset` or `tf.data.Iterator`. Returns: A (nested) structure of `tf.DType` objects matching the structure of dataset / iterator elements and specifying the shape of the individual components. @compatibility(TF2) This is a legacy API for inspecting the type signature of dataset elements. In TF 2, you should use the `tf.data.Dataset.element_spec` attribute instead. @end_compatibility",torch.distributed.checkpoint.state_dict.get_model_state_dict,"Return the model state_dict of ``model``. See ``get_state_dict`` for the detail usage. Args: model (nn.Module): the nn.Module to the model. submodules (deprecated): Optional[Set[nn.Module]]: only return the model parameters that belong to the submodules. options (StateDictOptions): the options to control how model state_dict and optimizer state_dict should be returned. See `StateDictOptions` for the details. Returns: The state_dict for ``model``. :rtype: typing.Dict[str, ValueType]",0.9537676
tensorflow.python.ops.linalg_ops.matrix_solve_ls,"Solves one or more linear least-squares problems. `matrix` is a tensor of shape `[..., M, N]` whose inner-most 2 dimensions form `M`-by-`N` matrices. Rhs is a tensor of shape `[..., M, K]` whose inner-most 2 dimensions form `M`-by-`K` matrices. The computed output is a `Tensor` of shape `[..., N, K]` whose inner-most 2 dimensions form `M`-by-`K` matrices that solve the equations `matrix[..., :, :] * output[..., :, :] = rhs[..., :, :]` in the least squares sense. Below we will use the following notation for each pair of matrix and right-hand sides in the batch: `matrix`=\\(A \in \Re^{m \times n}\\), `rhs`=\\(B \in \Re^{m \times k}\\), `output`=\\(X \in \Re^{n \times k}\\), `l2_regularizer`=\\(\lambda\\). If `fast` is `True`, then the solution is computed by solving the normal equations using Cholesky decomposition. Specifically, if \\(m \ge n\\) then \\(X = (A^T A + \lambda I)^{-1} A^T B\\), which solves the least-squares problem \\(X = \mathrm{argmin}_{Z \in \Re^{n \times k}} ||A Z - B||_F^2 + \lambda ||Z||_F^2\\). If \\(m \lt n\\) then `output` is computed as \\(X = A^T (A A^T + \lambda I)^{-1} B\\), which (for \\(\lambda = 0\\)) is the minimum-norm solution to the under-determined linear system, i.e. \\(X = \mathrm{argmin}_{Z \in \Re^{n \times k}} ||Z||_F^2 \\), subject to \\(A Z = B\\). Notice that the fast path is only numerically stable when \\(A\\) is numerically full rank and has a condition number \\(\mathrm{cond}(A) \lt \frac{1}{\sqrt{\epsilon_{mach}}}\\) or\\(\lambda\\) is sufficiently large. If `fast` is `False` an algorithm based on the numerically robust complete orthogonal decomposition is used. This computes the minimum-norm least-squares solution, even when \\(A\\) is rank deficient. This path is typically 6-7 times slower than the fast path. If `fast` is `False` then `l2_regularizer` is ignored. Args: matrix: `Tensor` of shape `[..., M, N]`. rhs: `Tensor` of shape `[..., M, K]`. l2_regularizer: 0-D `double` `Tensor`. Ignored if `fast=False`. fast: b",torch.nn.utils.parametrizations.orthogonal,"Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Letting :math:`\mathbb{K}` be :math:`\mathbb{R}` or :math:`\mathbb{C}`, the parametrized matrix :math:`Q \in \mathbb{K}^{m \times n}` is **orthogonal** as .. math:: \begin{align*} Q^{\text{H}}Q &= \mathrm{I}_n \mathrlap{\qquad \text{if }m \geq n}\\ QQ^{\text{H}} &= \mathrm{I}_m \mathrlap{\qquad \text{if }m < n} \end{align*} where :math:`Q^{\text{H}}` is the conjugate transpose when :math:`Q` is complex and the transpose when :math:`Q` is real-valued, and :math:`\mathrm{I}_n` is the `n`-dimensional identity matrix. In plain words, :math:`Q` will have orthonormal columns whenever :math:`m \geq n` and orthonormal rows otherwise. If the tensor has more than two dimensions, we consider it as a batch of matrices of shape `(..., m, n)`. The matrix :math:`Q` may be parametrized via three different ``orthogonal_map`` in terms of the original tensor: - ``""matrix_exp""``/``""cayley""``: the :func:`~torch.matrix_exp` :math:`Q = \exp(A)` and the `Cayley map`_ :math:`Q = (\mathrm{I}_n + A/2)(\mathrm{I}_n - A/2)^{-1}` are applied to a skew-symmetric :math:`A` to give an orthogonal matrix. - ``""householder""``: computes a product of Householder reflectors (:func:`~torch.linalg.householder_product`). ``""matrix_exp""``/``""cayley""`` often make the parametrized weight converge faster than ``""householder""``, but they are slower to compute for very thin or very wide matrices. If ``use_trivialization=True`` (default), the parametrization implements the ""Dynamic Trivialization Framework"", where an extra matrix :math:`B \in \mathbb{K}^{n \times n}` is stored under ``module.parametrizations.weight[0].base``. This helps the convergence of the parametrized layer at the expense of some extra memory use. See `Trivializations for Gradient-Based Optimization on Manifolds`_ . Initial value of :math:`Q`: If the original tensor is not parametrized and ``use_trivialization=True`` (default), the initial value of :math:`Q` is ",0.9536848
tensorflow.python.training.training_util.global_step,"Small helper to get the global step. python # Create a variable to hold the global_step. global_step_tensor = tf.Variable(10, trainable=False, name='global_step') # Create a session. sess = tf.compat.v1.Session() # Initialize the variable sess.run(global_step_tensor.initializer) # Get the variable value. print('global_step: %s' % tf.compat.v1.train.global_step(sess, global_step_tensor)) global_step: 10 Args: sess: A TensorFlow `Session` object. global_step_tensor: `Tensor` or the `name` of the operation that contains the global step. Returns: The global step value.",torch.library.impl,"Registers the function implementation for an operator defined in the library. Args: op_name: operator name (along with the overload) or OpOverload object. fn: function that's the operator implementation for the input dispatch key or :func:`~fallthrough_kernel` to register a fallthrough. dispatch_key: dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with. Example:: >>> my_lib = Library(""aten"", ""IMPL"") >>> def div_cpu(self, other): >>> return self * (1 / other) >>> my_lib.impl(""div.Tensor"", div_cpu, ""CPU"")",0.9536228
tensorflow.python.debug.lib.debug_events_reader.graph_by_id,Get a DebuggedGraph object by its ID.,torch.onnx.verification.find_partition,Find the `GraphInfo` object with the given id.,0.9536037
tensorflow.python.distribute.device_util.current,Return a string (not canonicalized) for the current device.,torch.cuda.random.get_rng_state_all,Return a list of ByteTensor representing the random number states of all devices.,0.95353496
tensorflow.python.eager.remote.connect_to_remote_host,"Connects to a single machine to enable remote execution on it. Will make devices on the remote host available to use. Note that calling this more than once will work, but will invalidate any tensor handles on the old remote devices. Using the default job_name of worker, you can schedule ops to run remotely as follows: python # When eager execution is enabled, connect to the remote host. tf.config.experimental_connect_to_host(""exampleaddr.com:9876"") with ops.device(""job:worker/replica:0/task:1/device:CPU:0""): # The following tensors should be resident on the remote device, and the op # will also execute remotely. x1 = array_ops.ones([2, 2]) x2 = array_ops.ones([2, 2]) y = math_ops.matmul(x1, x2) Args: remote_host: a single or a list the remote server addr in host-port format. job_name: The job name under which the new server will be accessible. Raises: ValueError: if remote_host is None.",torch.utils.tensorboard.writer.add_scalars,"Add many scalar data to summary. Args: main_tag (str): The parent name for the tags tag_scalar_dict (dict): Key-value pair storing the tag and corresponding values global_step (int): Global step value to record walltime (float): Optional override default walltime (time.time()) seconds after epoch of event Examples:: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() r = 5 for i in range(100): writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r), 'xcosx':i*np.cos(i/r), 'tanx': np.tan(i/r)}, i) writer.close() # This call adds three values to the same scalar plot with the tag # 'run_14h' in TensorBoard's scalar section. Expected result: .. image:: _static/img/tensorboard/add_scalars.png :scale: 50 %",0.95349646
tensorflow.python.keras.optimizer_v2.learning_rate_schedule.from_config,Instantiates a `LearningRateSchedule` from its config. Args: config: Output of `get_config()`. Returns: A `LearningRateSchedule` instance.,torch.distributed._shard.sharded_optim.api.load_state_dict,Loads the ShardedOptimizer state. Args: state_dict (dict): ShardedOptimizer state. Should be an object returned from a call to :meth:`state_dict`.,0.9534707
tensorflow.python.eager.polymorphic_function.concrete_function.inference_fn,Return the inference function associated with this ConcreteFunction.,torch.nn.modules.container.values,Return an iterable of the ParameterDict values.,0.95346725
tensorflow.python.keras.engine.sequential.track_nodes_created_by_last_call,Adds to `created_nodes` the nodes created by the last call to `layer`.,torch.ao.quantization.fx.utils.create_node_from_old_node_preserve_meta,Creates `new_node` and copies the necessary metadata to it from `old_node`.,0.9534496
tensorflow.python.debug.cli.profile_analyzer_cli.value,"Get the content of a cell of the table. Args: row: (int) row index. col: (int) column index. device_name_filter: Regular expression to filter by device name. node_name_filter: Regular expression to filter by node name. op_type_filter: Regular expression to filter by op type. Returns: A debuggre_cli_common.RichLine object representing the content of the cell, potentially with a clickable MenuItem. Raises: IndexError: if row index is out of range.",torch._inductor.codegen.cuda.gemm_template.add_cutlass_gemm_choices,"Adds Cutlass GEMM configurations choices to the auto-tuning list. This function mutates the passed list of choices by appending the choices for Cutlass GEMM configs to it. Args: choices (list): The list to which choices are appended. layout (ir.Layout): The layout configuration. input_nodes (list): The list of input nodes. alpha (float,int): Scaling factor, defaults to 1. beta (float,int): Offset, defaults to 0. input_reorder (list, optional): Order of the inputs, defaults to None. **extra_kwargs: Additional keyword arguments.",0.9534319
tensorflow.python.ops.image_ops_impl.non_max_suppression_padded_v2,"Non-maximum suppression. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval `[0, 1]`) or absolute. The bounding box coordinates are cannonicalized to `[y_min, x_min, y_max, x_max]`, where `(y_min, x_min)` and `(y_max, x_mas)` are the coordinates of the lower left and upper right corner. User may indiciate the input box coordinates are already canonicalized to eliminate redundant work by setting canonicalized_coordinates to `True`. Note that this algorithm is agnostic to where the origin is in the coordinate system. Note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. Similar to tf.image.non_max_suppression, non_max_suppression_padded implements hard NMS but can operate on a batch of images and improves performance by titling the bounding boxes. Non_max_suppression_padded should be preferred over tf.image_non_max_suppression when running on devices with abundant parallelsim for higher computation speed. For soft NMS, refer to tf.image.non_max_suppression_with_scores. While a serial NMS algorithm iteratively uses the highest-scored unprocessed box to suppress boxes, this algorithm uses many boxes to suppress other boxes in parallel. The key idea is to partition boxes into tiles based on their score and suppresses boxes tile by tile, thus achieving parallelism within a tile. The tile size determines the degree of parallelism. In cross suppression (using boxes of tile A to suppress boxes of tile B), all boxes in A can independently suppress boxes in B. Self suppression (suppressing boxes of the same tile) needs to be iteratively applie",torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook,"Implement simplified PowerSGD algorithm. This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the `paper <https://arxiv.org/abs/1905.13727>`_. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is **faster** than :meth:`powerSGD_hook`, but usually results in a **much lower accuracy**, unless ``matrix_approximation_rank`` is 1. .. warning :: Increasing ``matrix_approximation_rank`` here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider :meth:`powerSGD_hook` first, and only consider this variant when a satisfactory accuracy can be achieved when ``matrix_approximation_rank`` is 1. Once gradient tensors are aggregated across all workers, this hook applies compression as follows: 1. Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings; 2. Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; 3. Computes P, which is equal to MQ; 4. Allreduces P; 5. Orthogonalizes P; 6. Computes Q, which is approximately equal to M^TP; 7. Allreduces Q; 8. Computes M, which is approximately equal to PQ^T. 9. Truncates the input tensor to the original length. Note that this communication hook enforces vanilla allreduce for the first ``state.start_powerSGD_iter`` iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers. Args: state (PowerSGDState): State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune ``matrix_a",0.95337737
tensorflow.python.training.device_setter.replica_device_setter,"Return a `device function` to use when building a Graph for replicas. Device Functions are used in `with tf.device(device_function):` statement to automatically assign devices to `Operation` objects as they are constructed, Device constraints are added from the inner-most context first, working outwards. The merging behavior adds constraints to fields that are yet unset by a more inner context. Currently the fields are (job, task, cpu/gpu). If `cluster` is `None`, and `ps_tasks` is 0, the returned function is a no-op. Otherwise, the value of `ps_tasks` is derived from `cluster`. By default, only Variable ops are placed on ps tasks, and the placement strategy is round-robin over all ps tasks. A custom `ps_strategy` may be used to do more intelligent placement, such as `tf.contrib.training.GreedyLoadBalancingStrategy`. For example, python # To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker # jobs on hosts worker0, worker1 and worker2. cluster_spec = { ""ps"": [""ps0:2222"", ""ps1:2222""], ""worker"": [""worker0:2222"", ""worker1:2222"", ""worker2:2222""]} with tf.compat.v1.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)): # Build your graph v1 = tf.Variable(...) # assigned to /job:ps/task:0 v2 = tf.Variable(...) # assigned to /job:ps/task:1 v3 = tf.Variable(...) # assigned to /job:ps/task:0 # Run compute Args: ps_tasks: Number of tasks in the `ps` job. Ignored if `cluster` is provided. ps_device: String. Device of the `ps` job. If empty no `ps` job is used. Defaults to `ps`. worker_device: String. Device of the `worker` job. If empty no `worker` job is used. merge_devices: `Boolean`. If `True`, merges or only sets a device if the device constraint is completely unset. merges device specification rather than overriding them. cluster: `ClusterDef` proto or `ClusterSpec`. ps_ops: List of strings representing `Operation` types that need to be placed on `ps` devices. If `None`, defaults to `STANDARD_PS_OPS`. ps_strategy: A callable invoked for ",torch.distributed.distributed_c10d.send_object_list,"Sends picklable objects in ``object_list`` synchronously. Similar to :func:`send`, but Python objects can be passed in. Note that all objects in ``object_list`` must be picklable in order to be sent. Args: object_list (List[Any]): List of input objects to sent. Each object must be picklable. Receiver must provide lists of equal sizes. dst (int): Destination rank to send ``object_list`` to. Destination rank is based on global process group (regardless of ``group`` argument) group: (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is ``None``. device (``torch.device``, optional): If not None, the objects are serialized and converted to tensors which are moved to the ``device`` before sending. Default is ``None``. Returns: ``None``. .. note:: For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by ``torch.cuda.current_device()`` and it is the user's responsibility to ensure that this is set so that each rank has an individual GPU, via ``torch.cuda.set_device()``. .. warning:: :func:`send_object_list` uses ``pickle`` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. .. warning:: Calling :func:`send_object_list` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using :func:`send` instead. Example:: >>> # xdoctest: +SKIP(""need process group init"") >>> # Note: Process group initialization omitted on each rank. >>> import torch.distributed as dist >>> # Assumes backend is not NCCL >>> device = torch.device(""cpu"") >>> if dist.get_rank() == 0: >>> # Assumes world_size of 2. >>> objects = [""foo"", 12, {1: 2}] # any picklable object >>> dist.send_object_list(objec",0.9533407
tensorflow.python.framework.config.tensor_float_32_execution_enabled,"Returns whether TensorFloat-32 is enabled. By default, TensorFloat-32 is enabled, but this can be changed with `tf.config.experimental.enable_tensor_float_32_execution`. Returns: True if TensorFloat-32 is enabled (the default) and False otherwise",torch.distributed._shard.sharded_tensor._ops.tensor_ops.sharded_type_as_check,Perform extra checks for the sharded_type_as op such as the input needs to be either a Tensor or ShardedTensor. Args: same as ``torch.Tensor.type_as``. Return: None,0.9533113
tensorflow.python.distribute.coordinator.cluster_coordinator.get,"Return a closure from the queue to be executed. It will try to fetch an item from the queue with the given tag. If this queue is empty, it will then check the global queue. Args: timeout: timeout when waiting for a closure to be put. tag: optional tag to specify which queue to query first before querying the global queue. Returns: a closure or None after timeout.",torch.distributed.elastic.rendezvous.dynamic_rendezvous.run,"Execute a rendezvous operation. An operation is run inside a state machine and is expected to transition the rendezvous from one state to another. Args: state_handler: A callable that is expected to return the next state transition action based on the current state of the rendezvous. deadline: The time, in seconds, at which the operation will be considered timed-out. update_deadline: Function to generate a new operation deadline if the current node may participate in the next rendezvous.",0.9533013
tensorflow.python.ops.linalg.linear_operator_util.dtype_name,Returns the string name for this `dtype`.,torch.ao.quantization.backend_config.tensorrt.get_tensorrt_backend_config_dict,Return the `BackendConfig` for the TensorRT backend in dictionary form.,0.9532574
tensorflow.python.checkpoint.checkpoint_view.match,"Returns all matching trackables between CheckpointView and Trackable. Matching trackables represents trackables with the same name and position in graph. Args: obj: `Trackable` root. Returns: Dictionary containing all overlapping trackables that maps `node_id` to `Trackable`. Example usage: >>> class SimpleModule(tf.Module): ... def __init__(self, name=None): ... super().__init__(name=name) ... self.a_var = tf.Variable(5.0) ... self.b_var = tf.Variable(4.0) ... self.vars = [tf.Variable(1.0), tf.Variable(2.0)] >>> root = SimpleModule(name=""root"") >>> leaf = root.leaf = SimpleModule(name=""leaf"") >>> leaf.leaf3 = tf.Variable(6.0, name=""leaf3"") >>> leaf.leaf4 = tf.Variable(7.0, name=""leaf4"") >>> ckpt = tf.train.Checkpoint(root) >>> save_path = ckpt.save('/tmp/tf_ckpts') >>> checkpoint_view = tf.train.CheckpointView(save_path) >>> root2 = SimpleModule(name=""root"") >>> leaf2 = root2.leaf2 = SimpleModule(name=""leaf2"") >>> leaf2.leaf3 = tf.Variable(6.0) >>> leaf2.leaf4 = tf.Variable(7.0) Pass `node_id=0` to `tf.train.CheckpointView.children()` to get the dictionary of all children directly linked to the checkpoint root. >>> checkpoint_view_match = checkpoint_view.match(root2).items() >>> for item in checkpoint_view_match: ... print(item) (0, ...) (1, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0>) (2, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=4.0>) (3, ListWrapper([<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>])) (6, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>) (7, <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>)",torch.nn.modules.batchnorm.convert_sync_batchnorm,"Converts all :attr:`BatchNorm*D` layers in the model to :class:`torch.nn.SyncBatchNorm` layers. Args: module (nn.Module): module containing one or more :attr:`BatchNorm*D` layers process_group (optional): process group to scope synchronization, default is the whole world Returns: The original :attr:`module` with the converted :class:`torch.nn.SyncBatchNorm` layers. If the original :attr:`module` is a :attr:`BatchNorm*D` layer, a new :class:`torch.nn.SyncBatchNorm` layer object will be returned instead. Example:: >>> # Network with nn.BatchNorm layer >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA) >>> module = torch.nn.Sequential( >>> torch.nn.Linear(20, 100), >>> torch.nn.BatchNorm1d(100), >>> ).cuda() >>> # creating process group (optional) >>> # ranks is a list of int identifying rank ids. >>> ranks = list(range(8)) >>> r1, r2 = ranks[:4], ranks[4:] >>> # Note: every rank calls into new_group for every >>> # process group created, even if that rank is not >>> # part of the group. >>> # xdoctest: +SKIP(""distributed"") >>> process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]] >>> process_group = process_groups[0 if dist.get_rank() <= 3 else 1] >>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)",0.9532299
tensorflow.python.ops.signal.window_ops.kaiser_window,"Generate a [Kaiser window][kaiser]. Args: window_length: A scalar `Tensor` indicating the window length to generate. beta: Beta parameter for Kaiser window, see reference below. dtype: The data type to produce. Must be a floating point type. name: An optional name for the operation. Returns: A `Tensor` of shape `[window_length]` of type `dtype`. [kaiser]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.kaiser.html",torch.cuda.streams.wait_event,Make all future work submitted to the stream wait for an event. Args: event (torch.cuda.Event): an event to wait for. .. note:: This is a wrapper around ``cudaStreamWaitEvent()``: see `CUDA Stream documentation`_ for more info. This function returns without waiting for :attr:`event`: only future operations are affected. .. _CUDA Stream documentation: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html,0.95322645
tensorflow.python.util.tf_export.get_canonical_name_for_symbol,"Get canonical name for the API symbol. Example: python from tensorflow.python.util import tf_export cls = tf_export.get_symbol_from_name('keras.optimizers.Adam') # Gives `<class 'keras.optimizer_v2.adam.Adam'>` print(cls) # Gives `keras.optimizers.Adam` print(tf_export.get_canonical_name_for_symbol(cls, api_name='keras')) Args: symbol: API function or class. api_name: API name. Currently, only `tensorflow`. add_prefix_to_v1_names: Specifies whether a name available only in V1 should be prefixed with compat.v1. Returns: Canonical name for the API symbol (for e.g. initializers.zeros) if canonical name could be determined. Otherwise, returns None.",torch.overrides.handle_torch_function,"Implement a function with checks for ``__torch_function__`` overrides. See torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation. Arguments --------- public_api : function Function exposed by the public torch API originally called like ``public_api(*args, **kwargs)`` on which arguments are now being checked. relevant_args : iterable Iterable of arguments to check for __torch_function__ methods. args : tuple Arbitrary positional arguments originally passed into ``public_api``. kwargs : tuple Arbitrary keyword arguments originally passed into ``public_api``. Returns ------- object Result from calling ``implementation`` or an ``__torch_function__`` method, as appropriate. Raises ------ TypeError : if no implementation is found. Example ------- >>> def func(a): ... if has_torch_function_unary(a): ... return handle_torch_function(func, (a,), a) ... return a + 0",0.9532248
tensorflow.python.kernel_tests.random.stateful_random_ops_test.testCreateRNGStateIntSeed,Tests `create_rng_state` when `seed` is int.,torch.nn.utils.rnn.is_cuda,Return true if `self.data` stored on a gpu.,0.95321256
tensorflow.python.compiler.tensorrt.trt_convert.save,"Save the converted SavedModel. Args: output_saved_model_dir: directory to saved the converted SavedModel. save_gpu_specific_engines: whether to save TRT engines that have been built. When True, all engines are saved and when False, the engines are not saved and will be rebuilt at inference time. By using save_gpu_specific_engines=False after doing INT8 calibration, inference can be done on different GPUs than the GPU that the model was calibrated and saved on. options: `tf.saved_model.SaveOptions` object for configuring save options. Raises: RuntimeError: if the needed calibration hasn't been done.",torch.testing._internal.dist_utils.dist_init,"We use this decorator for setting up and tearing down state since MultiProcessTestCase runs each `test*` method in a separate process and each process just runs the `test*` method without actually calling 'setUp' and 'tearDown' methods of unittest. Note: pass the string representation of MessageTypes that should be used with the faulty agent's send function. By default, all retriable messages (""RREF_FORK_REQUEST"", ""RREF_CHILD_ACCEPT"", ""RREF_USER_DELETE"", ""CLEANUP_AUTOGRAD_CONTEXT_REQ"") will use the faulty send (this default is set from faulty_rpc_agent_test_fixture.py).",0.95311904
tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.initialize_tpu_system,"Initialize the TPU devices. Args: cluster_resolver: A tf.distribute.cluster_resolver.TPUClusterResolver, which provides information about the TPU cluster. Returns: The tf.tpu.Topology object for the topology of the TPU cluster. If called inside tf.function, it returns the serialized topology object instead. Raises: RuntimeError: If running inside a tf.function. NotFoundError: If no TPU devices found in eager mode.",torch.distributed.elastic.rendezvous.dynamic_rendezvous.from_backend,Create a new :py:class:`DynamicRendezvousHandler`. Args: run_id: The run id of the rendezvous. store: The C10d store to return as part of the rendezvous. backend: The backend to use to hold the rendezvous state. min_nodes: The minimum number of nodes to admit to the rendezvous. max_nodes: The maximum number of nodes to admit to the rendezvous. local_addr: The local node address. timeout: The timeout configuration of the rendezvous.,0.9531087
tensorflow.python.training.session_run_hook.before_run,"Called before each call to run(). You can return from this call a `SessionRunArgs` object indicating ops or tensors to add to the upcoming `run()` call. These ops/tensors will be run together with the ops/tensors originally passed to the original run() call. The run args you return can also contain feeds to be added to the run() call. The `run_context` argument is a `SessionRunContext` that provides information about the upcoming `run()` call: the originally requested op/tensors, the TensorFlow Session. At this point graph is finalized and you can not add ops. Args: run_context: A `SessionRunContext` object. Returns: None or a `SessionRunArgs` object.",torch.onnx.utils.unconvertible_ops,"Returns an approximated list of all ops that are yet supported by :mod:`torch.onnx`. The list is approximated because some ops may be removed during the conversion process and don't need to be converted. Some other ops may have partial support that will fail conversion with particular inputs. Please open a Github Issue for op support requests. Args: model: Same as the `model` parameter in :func:`torch.onnx.export`. args: Same as the `args` parameter in :func:`torch.onnx.export`. training: Same as the `training` parameter in :func:`torch.onnx.export`. opset_version: Same as the `opset_version` parameter in :func:`torch.onnx.export`. Returns: The JIT graph and a list of unconvertible ops in the format of ""domain::op"".",0.95301634
tensorflow.python.keras.backend.batch_flatten,"Turn a nD tensor into a 2D tensor with same 0th dimension. In other words, it flattens each data samples of a batch. Args: x: A tensor or variable. Returns: A tensor. Examples: Flattening a 3D tensor to 2D by collapsing the last dimension. >>> x_batch = tf.keras.backend.ones(shape=(2, 3, 4, 5)) >>> x_batch_flatten = batch_flatten(x_batch) >>> tf.keras.backend.int_shape(x_batch_flatten) (2, 60)",torch.testing._internal.common_utils.random_hermitian_pd_matrix,"Returns a batch of random Hermitian positive-definite matrices. The shape of the result is batch_dims + (matrix_size, matrix_size) The following example creates a tensor of size 2 x 4 x 3 x 3 >>> # xdoctest: +SKIP(""undefined variables"") >>> matrices = random_hermitian_pd_matrix(3, 2, 4, dtype=dtype, device=device)",0.95301616
tensorflow.python.keras.testing_utils.run_v2_only,"Execute the decorated test only if running in v2 mode. This function is intended to be applied to tests that exercise v2 only functionality. If the test is run in v1 mode it will simply be skipped. See go/tf-test-decorator-cheatsheet for the decorators to use in different v1/v2/eager/graph combinations. Args: func: function to be annotated. If `func` is None, this method returns a decorator the can be applied to a function. If `func` is not None this returns the decorator applied to `func`. Returns: Returns a decorator that will conditionally skip the decorated test method.",torch._functorch.aot_autograd.aot_export_joint_simple,"A simplified version of export. Used by higher order operators. This function makes a high-level ""no calling convention changes"" guarantee: - If no inputs require grad (so we export an inference graph), there are *no* calling convention change between the exported graph, and ""func"". - If at least one input requires grad (so we trace out and export a joint fw-bw graph), Then if you were partition the graph into a separate forward and backward graph, The forward graph will have no calling convention changes compared to ""func"". The above also relies on some strong restrictions around which functions this API accepts: (1) `args` cannot contain any pytrees (they must have been pytree_flattened already) (2) `func` cannot mutate any inputs (3) The outputs of `func` cannot alias any inputs. Note: this function is only lightly tested today. It will probably be tested more heavily by higher order ops.",0.95300376
tensorflow.python.ops.gradient_checker_v2.compute_gradient,"Computes the theoretical and numeric Jacobian of `f`. With y = f(x), computes the theoretical and numeric Jacobian dy/dx. Args: f: the function. x: the arguments for the function as a list or tuple of values convertible to a Tensor. delta: (optional) perturbation used to compute numeric Jacobian. Returns: A pair of lists, where the first is a list of 2-d numpy arrays representing the theoretical Jacobians for each argument, and the second list is the numerical ones. Each 2-d array has ""y_size"" rows and ""x_size"" columns where ""x_size"" is the number of elements in the corresponding argument and ""y_size"" is the number of elements in f(x). Raises: ValueError: If result is empty but the gradient is nonzero. ValueError: If x is not list, but any other type. Example: >>> @tf.function ... def test_func(x): ... return x*x ... >>> >>> class MyTest(tf.test.TestCase): ... ... def test_gradient_of_test_func(self): ... theoretical, numerical = tf.test.compute_gradient(test_func, [1.0]) ... # ((array([[2.]], dtype=float32),), ... # (array([[2.000004]], dtype=float32),)) ... self.assertAllClose(theoretical, numerical)",torch._numpy.testing.utils.assert_array_almost_equal_nulp,"Compare two arrays relatively to their spacing. This is a relatively robust method to compare two arrays whose amplitude is variable. Parameters ---------- x, y : array_like Input arrays. nulp : int, optional The maximum number of unit in the last place for tolerance (see Notes). Default is 1. Returns ------- None Raises ------ AssertionError If the spacing between `x` and `y` for one or more elements is larger than `nulp`. See Also -------- assert_array_max_ulp : Check that all items of arrays differ in at most N Units in the Last Place. spacing : Return the distance between x and the nearest adjacent number. Notes ----- An assertion is raised if the following condition is not met:: abs(x - y) <= nulp * spacing(maximum(abs(x), abs(y))) Examples -------- >>> x = np.array([1., 1e-10, 1e-20]) >>> eps = np.finfo(x.dtype).eps >>> np.testing.assert_array_almost_equal_nulp(x, x*eps/2 + x) # doctest: +SKIP >>> np.testing.assert_array_almost_equal_nulp(x, x*eps + x) # doctest: +SKIP Traceback (most recent call last): ... AssertionError: X and Y are not equal to 1 ULP (max is 2)",0.9529572
tensorflow.python.keras.utils.generic_utils.register_keras_serializable,"Registers an object with the Keras serialization framework. This decorator injects the decorated class or function into the Keras custom object dictionary, so that it can be serialized and deserialized without needing an entry in the user-provided custom object dict. It also injects a function that Keras will call to get the object's serializable string key. Note that to be serialized and deserialized, classes must implement the `get_config()` method. Functions do not have this requirement. The object will be registered under the key 'package>name' where `name`, defaults to the object name if not passed. Args: package: The package that this class belongs to. name: The name to serialize this class under in this package. If None, the class' name will be used. Returns: A decorator that registers the decorated class with the passed names.",torch.testing._internal.distributed.rpc_utils.generate_tests,"Mix in the classes needed to autogenerate the tests based on the params. Takes a series of test suites, each written against a ""generic"" agent (i.e., derived from the abstract RpcAgentTestFixture class), as the `tests` args. Takes a concrete subclass of RpcAgentTestFixture, which specializes it for a certain agent, as the `mixin` arg. Produces all combinations of them. Returns a dictionary of class names to class type objects which can be inserted into the global namespace of the calling module. The name of each test will be a concatenation of the `prefix` arg and the original name of the test suite. The `module_name` should be the name of the calling module so that the classes can be fixed to make it look like they belong to it, which is necessary for pickling to work on them.",0.9529495
tensorflow.python.eager.context.add_c_function,"Add a C API TF_Function to the context. Once added, the function (identified by its name) can be executed like any other operation. Args: c_func: A wrapped TF_Function (returned from TF_GraphToFunction_wrapper).",torch.fx.passes.infra.pass_manager.pass_result_wrapper,"Wrapper for passes which currently do not return a PassResult. This wrapper makes them return a PassResult containing the modified object and True for the ""modified"" flag. Args: fn (Callable[Module, Any]) Returns: wrapped_fn (Callable[Module, PassResult])",0.9529431
tensorflow.python.keras.engine.keras_tensor.ref,Returns a hashable reference object to this KerasTensor. The primary use case for this API is to put KerasTensors in a set/dictionary. We can't put tensors in a set/dictionary as `tensor.__hash__()` is not available and tensor equality (`==`) is supposed to produce a tensor representing if the two inputs are equal. See the documentation of `tf.Tensor.ref()` for more info.,torch.nn.functional.relu,"relu(input, inplace=False) -> Tensor Applies the rectified linear unit function element-wise. See :class:`~torch.nn.ReLU` for more details.",0.95290756
tensorflow.python.util.tf_should_use.should_use_result,"Function wrapper that ensures the function's output is used. If the output is not used, a `logging.error` is logged. If `error_in_function` is set, then a `RuntimeError` will be raised at the end of function tracing if the output is not used by that point. An output is marked as used if any of its attributes are read, modified, or updated. Examples when the output is a `Tensor` include: - Using it in any capacity (e.g. `y = t + 0`, `sess.run(t)`) - Accessing a property (e.g. getting `t.name` or `t.op`). - Calling `t.mark_used()`. Note, certain behaviors cannot be tracked - for these the object may not be marked as used. Examples include: - `t != 0`. In this case, comparison is done on types / ids. - `isinstance(t, tf.Tensor)`. Similar to above. Args: fn: The function to wrap. warn_in_eager: Whether to create warnings in Eager as well. error_in_function: Whether to raise an error when creating a tf.function. Returns: The wrapped function.",torch.optim.optimizer.zero_grad,"Resets the gradients of all optimized :class:`torch.Tensor` s. Args: set_to_none (bool): instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests ``zero_grad(set_to_none=True)`` followed by a backward pass, ``.grad``\ s are guaranteed to be None for params that did not receive a gradient. 3. ``torch.optim`` optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).",0.9528623
tensorflow.python.keras.utils.generic_utils.get_custom_objects,"Retrieves a live reference to the global dictionary of custom objects. Updating and clearing custom objects using `custom_object_scope` is preferred, but `get_custom_objects` can be used to directly access the current collection of custom objects. Example: python get_custom_objects().clear() get_custom_objects()['MyObject'] = MyObject Returns: Global dictionary of names to classes (`_GLOBAL_CUSTOM_OBJECTS`).",torch.utils.tensorboard.writer.add_custom_scalars_multilinechart,"Shorthand for creating multilinechart. Similar to ``add_custom_scalars()``, but the only necessary argument is *tags*. Args: tags (list): list of tags that have been used in ``add_scalar()`` Examples:: writer.add_custom_scalars_multilinechart(['twse/0050', 'twse/2330'])",0.95283294
tensorflow.python.training.quantize_training.do_quantize_training_on_graphdef,"A general quantization scheme is being developed in `tf.contrib.quantize`. Consider using that instead, though since it is in the tf.contrib namespace, it is not subject to backward compatibility guarantees. Args: input_graph: A `GraphDef`. num_bits: The number of bits for quantize training. Returns: The graph with quantize training done.",torch.nn.modules.module.zero_grad,"Reset gradients of all model parameters. See similar function under :class:`torch.optim.Optimizer` for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth:`torch.optim.Optimizer.zero_grad` for details.",0.95282245
tensorflow.python.training.saver.import_meta_graph,"Recreates a Graph saved in a `MetaGraphDef` proto. This function takes a `MetaGraphDef` protocol buffer as input. If the argument is a file containing a `MetaGraphDef` protocol buffer , it constructs a protocol buffer from the file content. The function then adds all the nodes from the `graph_def` field to the current graph, recreates all the collections, and returns a saver constructed from the `saver_def` field. In combination with `export_meta_graph()`, this function can be used to * Serialize a graph along with other Python objects such as `QueueRunner`, `Variable` into a `MetaGraphDef`. * Restart training from a saved graph and checkpoints. * Run inference from a saved graph and checkpoints. Python ... # Create a saver. saver = tf.compat.v1.train.Saver(...variables...) # Remember the training_op we want to run by adding it to a collection. tf.compat.v1.add_to_collection('train_op', train_op) sess = tf.compat.v1.Session() for step in range(1000000): sess.run(train_op) if step % 1000 == 0: # Saves checkpoint, which by default also exports a meta_graph # named 'my-model-global_step.meta'. saver.save(sess, 'my-model', global_step=step) Later we can continue training from this saved `meta_graph` without building the model from scratch. Python with tf.Session() as sess: new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta') new_saver.restore(sess, 'my-save-dir/my-model-10000') # tf.get_collection() returns a list. In this example we only want # the first one. train_op = tf.get_collection('train_op')[0] for step in range(1000000): sess.run(train_op) NOTE: Restarting training from saved `meta_graph` only works if the device assignments have not changed. Example: Variables, placeholders, and independent operations can also be stored, as shown in the following example. Python # Saving contents and operations. v1 = tf.placeholder(tf.float32, name=""v1"") v2 = tf.placeholder(tf.float32, name=""v2"") v3 = tf.math.multiply(v1, v2) vx = tf.Variable(10.0, name=""",torch.ao.quantization.quantize_pt2e.prepare_pt2e,"Prepare a model for post training quantization Args: * `model` (torch.fx.GraphModule): a model captured by `torch.export` API in the short term we are using `torch._export.capture_pre_autograd_graph`, in the long term we'll migrate to some `torch.export` API * `quantizer`: A backend specific quantizer that conveys how user want the model to be quantized. Tutorial for how to write a quantizer can be found here: https://pytorch.org/tutorials/prototype/pt2e_quantizer.html Return: A GraphModule with observer (based on quantizer annotation), ready for calibration Example:: import torch from torch.ao.quantization.quantize_pt2e import prepare_pt2e from torch._export import capture_pre_autograd_graph from torch.ao.quantization.quantizer import ( XNNPACKQuantizer, get_symmetric_quantization_config, ) class M(torch.nn.Module): def __init__(self): super().__init__() self.linear = torch.nn.Linear(5, 10) def forward(self, x): return self.linear(x) # initialize a floating point model float_model = M().eval() # define calibration function def calibrate(model, data_loader): model.eval() with torch.no_grad(): for image, target in data_loader: model(image) # Step 1. program capture # NOTE: this API will be updated to torch.export API in the future, but the captured # result shoud mostly stay the same m = capture_pre_autograd_graph(m, *example_inputs) # we get a model with aten ops # Step 2. quantization # backend developer will write their own Quantizer and expose methods to allow # users to express how they # want the model to be quantized quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config()) m = prepare_pt2e(m, quantizer) # run calibration # calibrate(m, sample_inference_data)",0.9527585
tensorflow.python.keras.backend.dropout,"Sets entries in `x` to zero at random, while scaling the entire tensor. Args: x: tensor level: fraction of the entries in the tensor that will be set to 0. noise_shape: shape for randomly generated keep/drop flags, must be broadcastable to the shape of `x` seed: random seed to ensure determinism. Returns: A tensor.",torch.nn.init.eye_,"Fill the 2-dimensional input `Tensor` with the identity matrix. Preserves the identity of the inputs in `Linear` layers, where as many inputs are preserved as possible. Args: tensor: a 2-dimensional `torch.Tensor` Examples: >>> w = torch.empty(3, 5) >>> nn.init.eye_(w)",0.9527461
tensorflow.python.debug.cli.analyzer_cli_test.check_main_menu,Check the main menu annotation of an output.,torch.distributed.distributed_c10d.is_initialized,Check if the default process group has been initialized.,0.9527285
tensorflow.python.framework.test_util.get_temp_dir,"Returns a unique temporary directory for the test to use. If you call this method multiple times during in a test, it will return the same folder. However, across different runs the directories will be different. This will ensure that across different runs tests will not be able to pollute each others environment. If you need multiple unique directories within a single test, you should use tempfile.mkdtemp as follows: tempfile.mkdtemp(dir=self.get_temp_dir()): Returns: string, the path to the unique temporary directory created for this test.",torch.testing._internal.common_utils.assertExpected,"Test that a string matches the recorded contents of a file derived from the name of this test and subname. This file is placed in the 'expect' directory in the same directory as the test script. You can automatically update the recorded test output using --accept. If you call this multiple times in a single function, you must give a unique subname each time.",0.95271474
tensorflow.python.keras.backend.log,Element-wise log. Args: x: Tensor or variable. Returns: A tensor.,torch.distributions.distribution.icdf,Returns the inverse cumulative density/mass function evaluated at `value`. Args: value (Tensor):,0.95270514
tensorflow.python.keras.backend.constant,Creates a constant tensor. Args: value: A constant value (or list) dtype: The type of the elements of the resulting tensor. shape: Optional dimensions of resulting tensor. name: Optional name for the tensor. Returns: A Constant Tensor.,torch.distributed.nn.functional.gather,"Gathers a list of tensors in a single process. Arguments: tensor (Tensor): Input tensor. dst (int, optional): Destination rank (default is 0). group (ProcessGroup, optional): The process group to work on. Returns: tuple[Tensor]: List of appropriately-sized tensors with the gathered data.",0.95269495
tensorflow.python.keras.engine.training.build,"Builds the model based on input shapes received. This is to be used for subclassed models, which do not know at instantiation time what their inputs look like. This method only exists for users who want to call `model.build()` in a standalone way (as a substitute for calling the model on real data to build it). It will never be called by the framework (and thus it will never throw unexpected errors in an unrelated workflow). Args: input_shape: Single tuple, TensorShape, or list/dict of shapes, where shapes are tuples, integers, or TensorShapes. Raises: ValueError: 1. In case of invalid user-provided data (not of type tuple, list, TensorShape, or dict). 2. If the model requires call arguments that are agnostic to the input shapes (positional or kwarg in call signature). 3. If not all layers were properly built. 4. If float type inputs are not supported within the layers. In each of these cases, the user should build their model by calling it on real tensor data.",torch.onnx.verification.check_export_model_diff,"Verify exported model discrepancy between different groups of inputs. A graph is exported for each group of inputs. The exported graphs are then compared to each other, and discrepancies of first pair of nodes are reported. This function first checks the jit graph. If no discrepancies were found, it then checks the onnx graph. Unless otherwise specified, the jit/ONNX graph is expected to be the same, regardless of the inputs used for exporting. A discrepancy implies the graph exported is not accurate when run on other groups of inputs, which will typically results in runtime errors or mismatching output. Args: model (torch.nn.Module or torch.jit.ScriptModule): The model to be exported. test_input_groups (Sequence[Tuple[Tuple[Any, ...], Mapping[str, Any]]]): A sequence of input groups to be used to export the model. Each input group is a pair of (args, kwargs). export_options (_experimental.ExportOptions, optional): An _experimental.ExportOptions object that controls the export behavior. Returns: str: A string containing the diff of the exported models.",0.9526698
tensorflow.python.ops.structured.structured_tensor.to_pyval,"Returns this StructuredTensor as a nested Python dict or list of dicts. Converts this `StructuredTensor` to a nested python value: * `StructTensors` with `rank=0` are converted into a dictionary, with an entry for each field. Field names are used as keys and field values are converted to python values. In particular: * Scalar Tensor fields are converted to simple values (such as `int` or `float` or `string`) * Non-scalar Tensor fields and RaggedTensor fields are converted to nested lists of simple values. * StructuredTensor fields are converted recursively using `to_pyval`. * `StructTensors` with `rank>0` are converted to nested python `list`s, containing one dictionary for each structure (where each structure's dictionary is defined as described above). Requires that all fields are Eager tensors. >>> tf.experimental.StructuredTensor.from_fields( ... {'a': [1, 2, 3]}, [3]).to_pyval() [{'a': 1}, {'a': 2}, {'a': 3}] Note that `StructuredTensor.from_pyval(pyval).to_pyval() == pyval`. Returns: A nested Python dict or list of dicts.",torch.utils.bundled_inputs.augment_model_with_bundled_inputs,"Add bundled sample inputs to a model for the forward function. Models with bundled inputs can be invoked in a uniform manner by benchmarking and code coverage tools. Augmented models will support the following methods: `get_all_bundled_inputs() -> List[Tuple[Any, ...]]` Returns a list of tuples suitable for passing to the model like `for inp in model.get_all_bundled_inputs(): model(*inp)` `get_num_bundled_inputs() -> int` Equivalent to `len(model.get_all_bundled_inputs())`, but slightly easier to call from C++. `get_bundled_inputs_functions_and_info() -> Dict[str, Dict[str: List[str]]]` Returns a dictionary mapping function names to a metadata dictionary. This nested dictionary maps preset strings like: 'get_inputs_function_name' -> the name of a function attribute in this model that can be run to get back a list of inputs corresponding to that function. 'info' -> the user provided extra information about the bundled inputs Inputs can be specified in one of two ways: - The model can define `_generate_bundled_inputs_for_forward`. If the user chooses this method inputs should be None - `inputs` is a list of inputs of form List[Tuple[Any, ...]]. A list of tuples where the elements of each tuple are the args that make up one input.",0.9526651
tensorflow.python.types.trace.from_tensors,Generates a value of this type from Tensors. Must use the same fixed amount of tensors as `to_tensors`. Args: tensors: An iterator from which the tensors can be pulled. Returns: A value of this type.,torch.distributed.checkpoint.default_planner.create_default_local_save_plan,"Create the ``SavePlan`` used by DefaultSavePlanner. On non-coordinator ranks, this function ignores tensors and non-tensor objects, only producing writes for ShardedTensor objects. On the coordinator rank, produce writes for all values.",0.95264876
tensorflow.python.keras.callbacks.on_train_end,Called at the end of training. Subclasses should override for any actions to run. Args: logs: Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future.,torch.distributed.optim.post_localSGD_optimizer.state_dict,"This is the same as :class:`torch.optim.Optimizer` :meth:`state_dict`, but adds an extra entry to record model averager's step to the checkpoint to ensure reload does not cause unnecessary warm up again.",0.95263773
tensorflow.python.data.experimental.ops.lookup_ops.index_table_from_dataset,"Returns an index lookup table based on the given dataset. This operation constructs a lookup table based on the given dataset of keys. Any lookup of an out-of-vocabulary token will return a bucket ID based on its hash if `num_oov_buckets` is greater than zero. Otherwise it is assigned the `default_value`. The bucket ID range is `[vocabulary size, vocabulary size + num_oov_buckets - 1]`. Sample Usages: >>> ds = tf.data.Dataset.range(100).map(lambda x: tf.strings.as_string(x * 2)) >>> table = tf.data.experimental.index_table_from_dataset( ... ds, key_dtype=dtypes.int64) >>> table.lookup(tf.constant(['0', '2', '4'], dtype=tf.string)).numpy() array([0, 1, 2]) Args: dataset: A dataset of keys. num_oov_buckets: The number of out-of-vocabulary buckets. vocab_size: Number of the elements in the vocabulary, if known. default_value: The value to use for out-of-vocabulary feature values. Defaults to -1. hasher_spec: A `HasherSpec` to specify the hash function to use for assignation of out-of-vocabulary buckets. key_dtype: The `key` data type. name: A name for this op (optional). Returns: The lookup table based on the given dataset. Raises: ValueError: If * `num_oov_buckets` is negative * `vocab_size` is not greater than zero * The `key_dtype` is not integer or string",torch.utils.tensorboard.writer.add_embedding,"Add embedding projector data to summary. Args: mat (torch.Tensor or numpy.ndarray): A matrix which each row is the feature vector of the data point metadata (list): A list of labels, each element will be converted to string label_img (torch.Tensor): Images correspond to each data point global_step (int): Global step value to record tag (str): Name for the embedding metadata_header (list): A list of headers for multi-column metadata. If given, each metadata must be a list with values corresponding to headers. Shape: mat: :math:`(N, D)`, where N is number of data and D is feature dimension label_img: :math:`(N, C, H, W)` Examples:: import keyword import torch meta = [] while len(meta)<100: meta = meta+keyword.kwlist # get some strings meta = meta[:100] for i, v in enumerate(meta): meta[i] = v+str(i) label_img = torch.rand(100, 3, 10, 32) for i in range(100): label_img[i]*=i/100.0 writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img) writer.add_embedding(torch.randn(100, 5), label_img=label_img) writer.add_embedding(torch.randn(100, 5), metadata=meta) .. note:: Categorical (i.e. non-numeric) metadata cannot have more than 50 unique values if they are to be used for coloring in the embedding projector.",0.95262945
tensorflow.python.ops.summary_ops_v2.should_record_summaries,"Returns boolean Tensor which is True if summaries will be recorded. If no default summary writer is currently registered, this always returns False. Otherwise, this reflects the recording condition has been set via `tf.summary.record_if()` (except that it may return False for some replicas when using `tf.distribute.Strategy`). If no recording condition is active, it defaults to True.",torch._dynamo.decorators.mark_static_address,Marks an input tensor whose data_ptr will not change across multiple calls to a dynamo-compiled function. This indicates to cudagraphs that an extra allocation is not needed for this input. The data_ptr will be guarded if guard=True. Note: Tensors marked in this way will be kept alive until `torch._dynamo.reset()` is called.,0.9525507
tensorflow.python.distribute.numpy_dataset.one_host_numpy_dataset,Create a dataset on `colocate_with` from `numpy_input`.,torch.ao.quantization.quantize_jit.script_qconfig_dict,Helper function used by `prepare_jit`. Apply `script_qconfig` for all entries in `qconfig_dict` that is not None.,0.95246243
tensorflow.python.framework.ops.has_default_graph,Returns True if there is a default graph.,torch.ao.ns.fx.pattern_utils.end_node_matches_reversed_fusion,Returns true if a pattern ending with `end_node` matches the fusion pattern.,0.9524465
tensorflow.python.keras.engine.training.save_weights,"Saves all layer weights. Either saves in HDF5 or in TensorFlow format based on the `save_format` argument. When saving in HDF5 format, the weight file has: - `layer_names` (attribute), a list of strings (ordered names of model layers). - For every layer, a `group` named `layer.name` - For every such layer group, a group attribute `weight_names`, a list of strings (ordered names of weights tensor of the layer). - For every weight in the layer, a dataset storing the weight value, named after the weight tensor. When saving in TensorFlow format, all objects referenced by the network are saved in the same format as `tf.train.Checkpoint`, including any `Layer` instances or `Optimizer` instances assigned to object attributes. For networks constructed from inputs and outputs using `tf.keras.Model(inputs, outputs)`, `Layer` instances used by the network are tracked/saved automatically. For user-defined classes which inherit from `tf.keras.Model`, `Layer` instances must be assigned to object attributes, typically in the constructor. See the documentation of `tf.train.Checkpoint` and `tf.keras.Model` for details. While the formats are the same, do not mix `save_weights` and `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be loaded using `Model.load_weights`. Checkpoints saved using `tf.train.Checkpoint.save` should be restored using the corresponding `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over `save_weights` for training checkpoints. The TensorFlow format matches objects and variables by starting at a root object, `self` for `save_weights`, and greedily matching attribute names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this is the `Checkpoint` even if the `Checkpoint` has a model attached. This means saving a `tf.keras.Model` using `save_weights` and loading into a `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match the `Model`'s variables. See the [guide to training checkpoints](https",torch.distributed.distributed_c10d.broadcast_object_list,"Broadcasts picklable objects in ``object_list`` to the whole group. Similar to :func:`broadcast`, but Python objects can be passed in. Note that all objects in ``object_list`` must be picklable in order to be broadcasted. Args: object_list (List[Any]): List of input objects to broadcast. Each object must be picklable. Only objects on the ``src`` rank will be broadcast, but each rank must provide lists of equal sizes. src (int): Source rank from which to broadcast ``object_list``. Source rank is based on global process group (regardless of ``group`` argument) group: (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is ``None``. device (``torch.device``, optional): If not None, the objects are serialized and converted to tensors which are moved to the ``device`` before broadcasting. Default is ``None``. Returns: ``None``. If rank is part of the group, ``object_list`` will contain the broadcasted objects from ``src`` rank. .. note:: For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by ``torch.cuda.current_device()`` and it is the user's responsibility to ensure that this is set so that each rank has an individual GPU, via ``torch.cuda.set_device()``. .. note:: Note that this API differs slightly from the :func:`broadcast` collective since it does not provide an ``async_op`` handle and thus will be a blocking call. .. warning:: :func:`broadcast_object_list` uses ``pickle`` module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust. .. warning:: Calling :func:`broadcast_object_list` with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using :func:`broadcast` instead",0.9524407
tensorflow.python.ops.distributions.util.matrix_diag_transform,"Transform diagonal of [batch-]matrix, leave rest of matrix unchanged. Create a trainable covariance defined by a Cholesky factor: python # Transform network layer into 2 x 2 array. matrix_values = tf.contrib.layers.fully_connected(activations, 4) matrix = tf.reshape(matrix_values, (batch_size, 2, 2)) # Make the diagonal positive. If the upper triangle was zero, this would be a # valid Cholesky factor. chol = matrix_diag_transform(matrix, transform=tf.nn.softplus) # LinearOperatorLowerTriangular ignores the upper triangle. operator = LinearOperatorLowerTriangular(chol) Example of heteroskedastic 2-D linear regression. python tfd = tfp.distributions # Get a trainable Cholesky factor. matrix_values = tf.contrib.layers.fully_connected(activations, 4) matrix = tf.reshape(matrix_values, (batch_size, 2, 2)) chol = matrix_diag_transform(matrix, transform=tf.nn.softplus) # Get a trainable mean. mu = tf.contrib.layers.fully_connected(activations, 2) # This is a fully trainable multivariate normal! dist = tfd.MultivariateNormalTriL(mu, chol) # Standard log loss. Minimizing this will ""train"" mu and chol, and then dist # will be a distribution predicting labels as multivariate Gaussians. loss = -1 * tf.reduce_mean(dist.log_prob(labels)) Args: matrix: Rank `R` `Tensor`, `R >= 2`, where the last two dimensions are equal. transform: Element-wise function mapping `Tensors` to `Tensors`. To be applied to the diagonal of `matrix`. If `None`, `matrix` is returned unchanged. Defaults to `None`. name: A name to give created ops. Defaults to ""matrix_diag_transform"". Returns: A `Tensor` with same shape and `dtype` as `matrix`.",torch.ao.nn.quantized.functional.conv2d,"Applies a 2D convolution over a quantized 2D input composed of several input planes. See :class:`~torch.ao.nn.quantized.Conv2d` for details and output shape. Args: input: quantized input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)` weight: quantized filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)` bias: **non-quantized** bias tensor of shape :math:`(\text{out\_channels})`. The tensor type must be `torch.float`. stride: the stride of the convolving kernel. Can be a single number or a tuple `(sH, sW)`. Default: 1 padding: implicit paddings on both sides of the input. Can be a single number or a tuple `(padH, padW)`. Default: 0 dilation: the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1 groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the number of groups. Default: 1 padding_mode: the padding mode to use. Only ""zeros"" is supported for quantized convolution at the moment. Default: ""zeros"" scale: quantization scale for the output. Default: 1.0 zero_point: quantization zero_point for the output. Default: 0 dtype: quantization data type to use. Default: ``torch.quint8`` Examples:: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_QENGINE) >>> from torch.ao.nn.quantized import functional as qF >>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float) >>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float) >>> bias = torch.randn(8, dtype=torch.float) >>> >>> scale, zero_point = 1.0, 0 >>> dtype_inputs = torch.quint8 >>> dtype_filters = torch.qint8 >>> >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters) >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs) >>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)",0.952387
tensorflow.python.keras.engine.base_layer_v1.call,"This is where the layer's logic lives. Args: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Returns: A tensor or list/tuple of tensors.",torch.fx.passes.shape_prop.propagate,Run `module` via interpretation and return the result and record the shape and type of each node. Args: *args (Tensor): the sample input. Returns: Any: The value returned from executing the Module,0.9523553
tensorflow.python.ops.nn_ops.in_top_k,"Says whether the targets are in the top `K` predictions. This outputs a `batch_size` bool array, an entry `out[i]` is `true` if the prediction for the target class is finite (not inf, -inf, or nan) and among the top `k` predictions among all predictions for example `i`. Note that the behavior of `InTopK` differs from the `TopK` op in its handling of ties; if multiple classes have the same prediction value and straddle the top-`k` boundary, all of those classes are considered to be in the top `k`. More formally, let \\(predictions_i\\) be the predictions for all classes for example `i`, \\(targets_i\\) be the target class for example `i`, \\(out_i\\) be the output for example `i`, $$out_i = predictions_{i, targets_i} \in TopKIncludingTies(predictions_i)$$ Args: predictions: A `Tensor` of type `float32`. A `batch_size` x `classes` tensor. targets: A `Tensor`. Must be one of the following types: `int32`, `int64`. A `batch_size` vector of class ids. k: An `int`. Number of top elements to look at for computing precision. name: A name for the operation (optional). Returns: A `Tensor` of type `bool`. Computed Precision at `k` as a `bool Tensor`.",torch.autograd.function.vmap,"Define the behavior for this autograd.Function underneath :func:`torch.vmap`. For a :func:`torch.autograd.Function` to support :func:`torch.vmap`, you must either override this static method, or set ``generate_vmap_rule`` to ``True`` (you may not do both). If you choose to override this staticmethod: it must accept - an ``info`` object as the first argument. ``info.batch_size`` specifies the size of the dimension being vmapped over, while ``info.randomness`` is the randomness option passed to :func:`torch.vmap`. - an ``in_dims`` tuple as the second argument. For each arg in ``args``, ``in_dims`` has a corresponding ``Optional[int]``. It is ``None`` if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over. - ``*args``, which is the same as the args to :meth:`~Function.forward`. The return of the vmap staticmethod is a tuple of ``(output, out_dims)``. Similar to ``in_dims``, ``out_dims`` should be of the same structure as ``output`` and contain one ``out_dim`` per output that specifies if the output has the vmapped dimension and what index it is in. Please see :ref:`func-autograd-function` for more details.",0.9522314
tensorflow.python.ops.map_fn.compute,"The loop body of map_fn. Args: i: the loop counter tas: the flat TensorArray accumulator list Returns: (i + 1, tas): the updated counter + updated TensorArrays Raises: TypeError: if fn_output_signature and result_value structure don't match ValueType: if fn_output_signature and result_value lengths don't match",torch.fx.experimental.migrate_gradual_types.constraint_transformation.broadcast_dim,"Apply broadcasting to the 'index' dimension of tensor_input1. Args: tensor_input1: should represent [d1, ..., d_index, ...] where d_index = 1 tensor_input2: represents the second input res1: broadcasted result 1 res2: broadcasted result 2 index: the index to broadcast padding: If padding was used, then tensor_input1[index] does not exist Returns:",0.9522143
tensorflow.python.keras.utils.vis_utils.model_to_dot,"Convert a Keras model to dot format. Args: model: A Keras model instance. show_shapes: whether to display shape information. show_dtype: whether to display layer dtypes. show_layer_names: whether to display layer names. rankdir: `rankdir` argument passed to PyDot, a string specifying the format of the plot: 'TB' creates a vertical plot; 'LR' creates a horizontal plot. expand_nested: whether to expand nested models into clusters. dpi: Dots per inch. subgraph: whether to return a `pydot.Cluster` instance. Returns: A `pydot.Dot` instance representing the Keras model or a `pydot.Cluster` instance representing nested model if `subgraph=True`. Raises: ImportError: if graphviz or pydot are not available.",torch.utils.tensorboard.summary.mesh,"Output a merged `Summary` protocol buffer with a mesh/point cloud. Args: tag: A name for this summary operation. vertices: Tensor of shape `[dim_1, ..., dim_n, 3]` representing the 3D coordinates of vertices. faces: Tensor of shape `[dim_1, ..., dim_n, 3]` containing indices of vertices within each triangle. colors: Tensor of shape `[dim_1, ..., dim_n, 3]` containing colors for each vertex. display_name: If set, will be used as the display name in TensorBoard. Defaults to `name`. description: A longform readable description of the summary data. Markdown is supported. config_dict: Dictionary with ThreeJS classes names and configuration. Returns: Merged summary for mesh/point cloud representation.",0.9522124
tensorflow.python.tpu.tpu_system_metadata.get_session_config_with_timeout,Returns a session given a timeout and a cluster configuration.,torch.distributed.algorithms.join.join_process_group,Returns the process group for the collective communications needed by the join context manager itself.,0.9521875
tensorflow.python.ops.array_ops.gather,"Gather slices from params axis `axis` according to indices. Gather slices from `params` axis `axis` according to `indices`. `indices` must be an integer tensor of any dimension (often 1-D). `Tensor.__getitem__` works for scalars, `tf.newaxis`, and [python slices](https://numpy.org/doc/stable/reference/arrays.indexing.html#basic-slicing-and-indexing) `tf.gather` extends indexing to handle tensors of indices. In the simplest case it's identical to scalar indexing: >>> params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5']) >>> params[3].numpy() b'p3' >>> tf.gather(params, 3).numpy() b'p3' The most common case is to pass a single axis tensor of indices (this can't be expressed as a python slice because the indices are not sequential): >>> indices = [2, 0, 2, 5] >>> tf.gather(params, indices).numpy() array([b'p2', b'p0', b'p2', b'p5'], dtype=object) <div style=""width:70%; margin:auto; margin-bottom:10px; margin-top:20px;""> <img style=""width:100%"" src=""https://www.tensorflow.org/images/Gather.png"" alt> </div> The indices can have any shape. When the `params` has 1 axis, the output shape is equal to the input shape: >>> tf.gather(params, [[2, 0], [2, 5]]).numpy() array([[b'p2', b'p0'], [b'p2', b'p5']], dtype=object) The `params` may also have any shape. `gather` can select slices across any axis depending on the `axis` argument (which defaults to 0). Below it is used to gather first rows, then columns from a matrix: >>> params = tf.constant([[0, 1.0, 2.0], ... [10.0, 11.0, 12.0], ... [20.0, 21.0, 22.0], ... [30.0, 31.0, 32.0]]) >>> tf.gather(params, indices=[3,1]).numpy() array([[30., 31., 32.], [10., 11., 12.]], dtype=float32) >>> tf.gather(params, indices=[2,1], axis=1).numpy() array([[ 2., 1.], [12., 11.], [22., 21.], [32., 31.]], dtype=float32) More generally: The output shape has the same shape as the input, with the indexed-axis replaced by the shape of the indices. >>> def result_shape(p_shape, i_shape, axis=0): ... return p_shape[:axis] + i_shape + p_shape[axis",torch.distributed._shard.sharded_tensor.api.reshard,"Reshard a sharded tensor given the ``resharding_spec``. For now, we only support single local shard. If ``resharding_spec`` is same as the original one, this becomes a no-op. If only ``resharding_spec`` shares the same sharding dim with the original one, we swap local shards directly. For more generic cases, we merge different shards across different ranks and split the local shards based on the ``resharding_spec`` via `all_to_all` collective API. Args: resharding_spec (:class:`torch.distributed._shard.sharding_spec.ShardingSpec`): The specification describing how the tensor is sharded. Returns: A :class:`ShardedTensor` object whose local shards are resharded. Examples: >>> # xdoctest: +SKIP >>> # We have 2 process groups, 2 ranks. >>> tensor = torch.arange(4, dtype=torch.int64) + 1 + 2 * rank >>> tensor = torch.stack([tensor, tensor]) >>> tensor tensor([[1, 2, 3, 4], [1, 2, 3, 4]]) # Rank 0 tensor([[3, 4, 5, 6], [3, 4, 5, 6]]) # Rank 1 tensor([[5, 6, 7, 8], [5, 6, 7, 8]]) # Rank 2 tensor([[7, 8, 9, 10], [7, 8, 9, 10]]) # Rank 3 >>> sharding_dim = 0 >>> spec = ChunkShardingSpec( dim=sharding_dim, placements=[ ""rank:0/cuda:0"", ""rank:1/cuda:1"", ""rank:2/cuda:2"", ""rank:3/cuda:3"", ], ) >>> current_offsets = [0] * 2 >>> current_offsets[0] = rank * 2 >>> shard_metadata = ShardMetadata( shard_offsets=copy.deepcopy(current_offsets), shard_sizes=tensor.size(), placement=spec.placements[rank], ) >>> local_shards = [ Shard( tensor=tensor, metadata=shard_metadata, ) ] >>> st = ShardedTensor._init_from_local_shards(local_shards, tensor.size()) >>> sharding_dim = 1 >>> resharding_spec = ChunkShardingSpec( dim=sharding_dim, placements=[ ""rank:0/cuda:0"", ""rank:1/cuda:1"", ""rank:2/cuda:2"", ""rank:3/cuda:3"", ], ) >>> st.reshard(resharding_spec) >>> tensor = st.local_shards()[0].tensor >>> tensor tensor([[1], [1], [3], [3], [5], [5], [7], [7]]) # Rank 0 tensor([[2], [2], [4], [4], [6], [6], [8], [8]]) # Rank 1 tensor([[3], [3], [5], [5], [7], [7], [9], [9]]) # Rank 2 tensor([[4], [4], [6",0.9520979
tensorflow.python.framework.extension_type.pack,Returns a copy of `value` with fields packed in a single Variant. Args: value: An `ExtensionType` object. Returns: An `ExtensionType` object.,torch.distributed._shard.sharder.shard,"Shard a module base on the implementation of this method, and return the sharded version of the module. Args: module (:class:`torch.nn.Module`): The module to apply sharding to. Returns: A :class:`torch.nn.Module` object that represents a module that's already been sharded.",0.9520829
tensorflow.python.ops.stateful_random_ops.get_global_generator,"Retrieves the global generator. This function will create the global generator the first time it is called, and the generator will be placed at the default device at that time, so one needs to be careful when this function is first called. Using a generator placed on a less-ideal device will incur performance regression. Returns: The global `tf.random.Generator` object.",torch.distributed.checkpoint.planner.resolve_tensor,"Return the tensor described by ``read_item`` to be used by the StorageReader to load `read_item`. The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that's not possible, the planner can use the ``commit_tensor`` method to copy the data back to the one in state_dict.",0.95205575
tensorflow.python.debug.lib.debug_events_writer.FlushNonExecutionFiles,Flush the non-execution debug event files.,torch._dynamo.output_graph.compile_subgraph,Generate a subgraph to continue execution on user code. Automatically restore live variables.,0.95201534
tensorflow.python.keras.engine.training_v1.reset_metrics,Resets the state of metrics.,torch._lazy.metrics.reset,Resets all metric counters.,0.95200396
tensorflow.python.framework.test_util.run_gpu_only,"Execute the decorated test only if a GPU is available. This function is intended to be applied to tests that require the presence of a GPU. If a GPU is absent, it will simply be skipped. Args: func: function to be annotated. Returns: Returns a function that will conditionally skip the decorated test method.",torch._inductor.codegen.cuda.gemm_template.test_call_statement,"Helper method to render the Cutlass CUDA C++ code required for calling the GEMM operation in the standalone test runner that might also be generated along with the rest of the code, if the corresponding config is enabled. Returns a C++ statement that calls the GEMM operation with the correct arguments.",0.9519794
tensorflow.python.profiler.profiler_v2.start,"Start profiling TensorFlow performance. Args: logdir: Profiling results log directory. options: `ProfilerOptions` namedtuple to specify miscellaneous profiler options. See example usage below. Raises: AlreadyExistsError: If a profiling session is already running. Example usage: python options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3, python_tracer_level = 1, device_tracer_level = 1) tf.profiler.experimental.start('logdir_path', options = options) # Training code here tf.profiler.experimental.stop() To view the profiling results, launch TensorBoard and point it to `logdir`. Open your browser and go to `localhost:6006/#profile` to view profiling results.",torch.hub.download_url_to_file,"Download object at the given URL to a local path. Args: url (str): URL of the object to download dst (str): Full path where object will be saved, e.g. ``/tmp/temporary_file`` hash_prefix (str, optional): If not None, the SHA256 downloaded file should start with ``hash_prefix``. Default: None progress (bool, optional): whether or not to display a progress bar to stderr Default: True Example: >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB) >>> # xdoctest: +REQUIRES(POSIX) >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')",0.9519257
tensorflow.python.ops.nn_ops.squeeze_batch_dims,Returns `unsqueeze_batch(op(squeeze_batch(inp)))`. Where `squeeze_batch` reshapes `inp` to shape `[prod(inp.shape[:-inner_rank])] + inp.shape[-inner_rank:]` and `unsqueeze_batch` does the reverse reshape but on the output. Args: inp: A tensor with dims `batch_shape + inner_shape` where `inner_shape` is length `inner_rank`. op: A callable that takes a single input tensor and returns a single. output tensor. inner_rank: A python integer. name: A string. Returns: `unsqueeze_batch_op(squeeze_batch(inp))`.,torch.nn.functional.upsample_bilinear,"Upsamples the input, using bilinear upsampling. .. warning:: This function is deprecated in favor of :func:`torch.nn.functional.interpolate`. This is equivalent with ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``. Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo volumetric (5 dimensional) inputs. Args: input (Tensor): input size (int or Tuple[int, int]): output spatial size. scale_factor (int or Tuple[int, int]): multiplier for spatial size Note: {backward_reproducibility_note}",0.9519251
tensorflow.python.debug.lib.source_remote.send_eager_tracebacks,"Send the tracebacks of an eager execution call to debug server(s). Args: destinations: gRPC destination addresses, a `str` or a `list` of `str`s, e.g., ""localhost:4242"". If a `list`, gRPC requests containing the same origin_stack: The traceback of the eager operation invocation. send_source: Whether the source files involved in the op tracebacks but outside the TensorFlow library are to be sent.",torch.distributed.rpc.options.set_devices,"Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this ``List``. Args: devices (List of int, str, or torch.device): local devices used by the TensorPipe RPC agent.",0.9519081
tensorflow.python.framework.test_util.with_eager_op_as_function,Returns the same class. This will be removed once all usages are removed. Args: cls: class to decorate. only_as_function: unused argument. Returns: cls,torch.overrides.resolve_name,Get a human readable string name for a function passed to __torch_function__ Arguments --------- f : Callable Function to resolve the name of. Returns ------- str Name of the function; if eval'ed it should give back the input function.,0.95190376
tensorflow.python.autograph.operators.data_structures.list_stack,"The list stack function. This does not have a direct correspondent in Python. The closest idiom to this is tf.append or np.stack. It's different from those in the sense that it accepts a Tensor list, rather than a list of tensors. It can also accept TensorArray. When the target is anything else, the dispatcher will rely on ctx.original_call for fallback. Args: list_: An entity that supports append semantics. opts: A ListStackOpts object. Returns: The output of the stack operation, typically a Tensor.",torch.library.define,"Defines a new operator and its semantics in the ns namespace. Args: schema: function schema to define a new operator. alias_analysis (optional): Indicates if the aliasing properties of the operator arguments can be inferred from the schema (default behavior) or not (""CONSERVATIVE""). tags (Tag | Sequence[Tag]): one or more torch.Tag to apply to this operator. Tagging an operator changes the operator's behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it. Returns: name of the operator as inferred from the schema. Example:: >>> my_lib = Library(""mylib"", ""DEF"") >>> my_lib.define(""sum(Tensor self) -> Tensor"")",0.9518846
tensorflow.python.ops.resource_variable_ops.assign,"Assigns a new value to this variable. Args: value: A `Tensor`. The new value for this variable. use_locking: If `True`, use locking during the assignment. name: The name to use for the assignment. read_value: A `bool`. Whether to read and return the new value of the variable or not. Returns: If `read_value` is `True`, this method will return the new value of the variable after the assignment has completed. Otherwise, when in graph mode it will return the `Operation` that does the assignment, and when in eager mode it will return `None`.",torch.ao.pruning._experimental.data_sparsifier.base_data_sparsifier.add_data,"Configures and parametrizes the internal container model with name and data. **Note**: 1. If the data with name already exists, it replaces the data. 2. While replacing, the old mask is reused when `reuse_mask=True` 3. If `reuse_mask=True`, then the replacing data needs to have the same shape as that of old data. 4. By default, the config of the replaced data is used as config for the replacing data, unless something is specified in the config dictionary.",0.9518739
tensorflow.python.ops.embedding_ops.embedding_lookup_sparse_v2,"Looks up embeddings for the given ids and weights from a list of tensors. `params` is a dense tensor or a list of dense tensors, and `sp_ids` is a 2D `tf.SparseTensor` or `tf.RaggedTensor` indicating the indices of `params` to gather. This op is best described with an example. Suppose `params` is an embedding table of size `(4, 2)` and `sp_ids` has 3 rows. Since `sp_ids` is sparse or ragged, not every row has the same number of elements. The output has shape (3, 2). Each row of `sp_ids` is a list of indices, where each index selects a row of `params`. For a given row of `sp_ids`, the rows of `params` are gathered based on the indices in `sp_ids`, then combined by taking their sum or mean. >>> params = tf.constant([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=tf.float32) >>> sp_ids = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]], ... values=[0, 1, 3, 2], dense_shape=(3, 2)) >>> tf.nn.embedding_lookup_sparse(params, sp_ids, sp_weights=None, ... combiner='sum').numpy() array([[4., 6.], [7., 8.], [5., 6.]], dtype=float32) In this example, `sp_ids` has 3 rows, so the output has 3 rows. Row 0 of `sp_ids` has values 0 and 1, so it selects rows 0 and 1 from `params`, which are `[1, 2]` and `[3, 4]`. The rows are summed since `combiner='sum'`, resulting in the output row of `[4, 6]`. Since row 1 and 2 of `sp_ids` only have one value each, they simply select the corresponding row from `params` as the output row. Row 1 has value `3` so it selects the `params` elements `[7, 8]` and row 2 has the value 2 so it selects the `params` elements `[5, 6]`. If `sparse_weights` is specified, it must have the same shape as `sp_ids`. `sparse_weights` is used to assign a weight to each slice of `params`. For example: >>> params = tf.constant([[1, 2], [3, 4], [5, 6], [7, 8]], dtype=tf.float32) >>> sp_ids = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]], ... values=[0, 1, 3, 2], dense_shape=(3, 2)) >>> sparse_weights = tf.SparseTensor(indices=[[0, 0], [0, 1], [1, 0], [2, 0]], .",torch.distributed._shard.sharding_spec.chunk_sharding_spec_ops.embedding_bag.sharded_embedding_bag,"Handles ``__torch_function__`` dispatch for ``torch.nn.functional.embedding_bag``. This method computes a sharded embedding bag aggregation and has the following limitations: 1. Supports only sharding of ``weight``. 2. Supports only ``ChunkShardingSpec``. 3. Supports only a single local shard per rank. 4. Supports all specs except for scale_grad_by_freq, sparse, etc. Based on the dimension that the weight is sharded on, there are two algorithms: ROWWISE SHARDING ================ For row-wise sharding the weight is sharded on dimension 0. The overall algorithm can be best explained with an example. Let's assume the dims for input are (4 x 6) and W are (16 x 17) and W is sharded across 4 GPUs creating 4 shard of (4 x 17). The algorithm is as follows: 1. First the input is all gathered to all ranks, since this is SPMD and input is actually sharded across all ranks. The inputs then become a 4 (4 x 6) tensor on each rank. For example if the given input is tensor([[6, 5, 2, 9, 6, 3], [3, 1, 2, 4, 7, 6], [4, 0, 4, 9, 8, 9], [8, 6, 6, 4, 6, 1]]) on rank 0. Then on every rank, we will have this tensor. If input itself is already replicated, no all-gather will be done. 2. Next, we mask the ID which are not stored on that rank. For example on rank 0, we store ID [0, 1, 2]. We only keep the ID inside the set of numbers. The rest of them will be masked to an extra row. The masked matrix will be used for embedding look up and is like: tensor([[4, 4, 2, 4, 4, 4], [4, 1, 2, 4, 4, 4], [4, 0, 4, 4, 4, 4], [4, 4, 4, 4, 4, 1]]) 3. If ``max_norm`` is specified, the extra row guarantees that the mask ID will not affect the behavior of weigh re-norm. 4. The example above only happens in one rank and each rank does a very similar thing. For ""Mean"" mode we need to divide by either column size (2D) or the interval length defined by the offset (excluding the row specified in ``padding_idx``). We also need to mask the unexisting row to neg Inf so that negative value does not gets wiped out in ",0.95185035
tensorflow.python.distribute.one_device_strategy.distribute_datasets_from_function,"Distributes `tf.data.Dataset` instances created by calls to `dataset_fn`. `dataset_fn` will be called once for each worker in the strategy. In this case, we only have one worker and one device so `dataset_fn` is called once. The `dataset_fn` should take an `tf.distribute.InputContext` instance where information about batching and input replication can be accessed: def dataset_fn(input_context): batch_size = input_context.get_per_replica_batch_size(global_batch_size) d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size) return d.shard( input_context.num_input_pipelines, input_context.input_pipeline_id) inputs = strategy.distribute_datasets_from_function(dataset_fn) for batch in inputs: replica_results = strategy.run(replica_fn, args=(batch,)) IMPORTANT: The `tf.data.Dataset` returned by `dataset_fn` should have a per-replica batch size, unlike `experimental_distribute_dataset`, which uses the global batch size. This may be computed using `input_context.get_per_replica_batch_size`. Args: dataset_fn: A function taking a `tf.distribute.InputContext` instance and returning a `tf.data.Dataset`. options: `tf.distribute.InputOptions` used to control options on how this dataset is distributed. Returns: A ""distributed `Dataset`"", which the caller can iterate over like regular datasets.",torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook.post_localSGD_hook,"Run post-localSGD algorithm. This DDP communication hook is used for running post-localSGD algorithm, by combining with a model averaging component (e.g., :class:`~torch.distributed.algorithms.model_averaging.averagers.PeriodicModelAverager`) that runs after the optimizer step. Args: state (PostLocalSGDState): State information to run post-localSGD. Users mainly need to tune ``start_localSGD_iter`` to determine when to start local SGD. bucket (dist.GradBucket): Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket. Returns: Future handler of the communication, which updates the gradients in place. Example:: >>> # xdoctest: +SKIP >>> state = PostLocalSGDState(process_group=process_group, subgroup=subgroup, start_localSGD_iter=10) >>> ddp_model.register_comm_hook(state, post_localSGD_hook) >>> # Also need to establish a model averaging module and run model averaging after ``optimizer.step()``. >>> # Please refer to the examples in ``torch.distributed.algorithms.model_averaging.averagers`` module.",0.95184946
tensorflow.python.debug.lib.debug_data.node_exists,"Test if a node exists in the partition graphs. Args: node_name: (`str`) name of the node to be checked. device_name: optional device name. If None, will search for the node on all available devices. Otherwise, search for the node only on the given device. Returns: A boolean indicating whether the node exists. Raises: LookupError: If no partition graphs have been loaded yet. ValueError: If device_name is specified but cannot be found.",torch.onnx._internal.fx.passes.type_promotion.get_type_promotion_rule,Get type promotion rule for a node. Args: diagnostic: Diagnostic object. node: Node to get type promotion rule for. type_promotion_table: Type promotion table. Returns: Type promotion rule for the node. None if no rule is found or if the node is not representing a torch operator.,0.95183134
tensorflow.python.ops.ragged.ragged_string_ops.strings_split_v1,"Split elements of `input` based on `sep`. Let N be the size of `input` (typically N will be the batch size). Split each element of `input` based on `sep` and return a `SparseTensor` or `RaggedTensor` containing the split tokens. Empty tokens are ignored. Examples: >>> print(tf.compat.v1.strings.split(['hello world', 'a b c'])) SparseTensor(indices=tf.Tensor( [[0 0] [0 1] [1 0] [1 1] [1 2]], ...), values=tf.Tensor([b'hello' b'world' b'a' b'b' b'c'], ...), dense_shape=tf.Tensor([2 3], shape=(2,), dtype=int64)) >>> print(tf.compat.v1.strings.split(['hello world', 'a b c'], ... result_type=""RaggedTensor"")) <tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]> If `sep` is given, consecutive delimiters are not grouped together and are deemed to delimit empty strings. For example, `input` of `""1<>2<><>3""` and `sep` of `""<>""` returns `[""1"", ""2"", """", ""3""]`. If `sep` is None or an empty string, consecutive whitespace are regarded as a single separator, and the result will contain no empty strings at the start or end if the string has leading or trailing whitespace. Note that the above mentioned behavior matches python's str.split. Args: input: A string `Tensor` of rank `N`, the strings to split. If `rank(input)` is not known statically, then it is assumed to be `1`. sep: `0-D` string `Tensor`, the delimiter character. maxsplit: An `int`. If `maxsplit > 0`, limit of the split of the result. result_type: The tensor type for the result: one of `""RaggedTensor""` or `""SparseTensor""`. source: alias for ""input"" argument. name: A name for the operation (optional). Raises: ValueError: If sep is not a string. Returns: A `SparseTensor` or `RaggedTensor` of rank `N+1`, the strings split according to the delimiter.",torch.nn.functional.ctc_loss,"Apply the Connectionist Temporal Classification loss. See :class:`~torch.nn.CTCLoss` for details. Note: {cudnn_reproducibility_note} Note: {backward_reproducibility_note} Args: log_probs: :math:`(T, N, C)` or :math:`(T, C)` where `C = number of characters in alphabet including blank`, `T = input length`, and `N = batch size`. The logarithmized probabilities of the outputs (e.g. obtained with :func:`torch.nn.functional.log_softmax`). targets: :math:`(N, S)` or `(sum(target_lengths))`. Targets cannot be blank. In the second form, the targets are assumed to be concatenated. input_lengths: :math:`(N)` or :math:`()`. Lengths of the inputs (must each be :math:`\leq T`) target_lengths: :math:`(N)` or :math:`()`. Lengths of the targets blank (int, optional): Blank label. Default :math:`0`. reduction (str, optional): Specifies the reduction to apply to the output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied, ``'mean'``: the output losses will be divided by the target lengths and then the mean over the batch is taken, ``'sum'``: the output will be summed. Default: ``'mean'`` zero_infinity (bool, optional): Whether to zero infinite losses and the associated gradients. Default: ``False`` Infinite losses mainly occur when the inputs are too short to be aligned to the targets. Example:: >>> log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() >>> targets = torch.randint(1, 20, (16, 30), dtype=torch.long) >>> input_lengths = torch.full((16,), 50, dtype=torch.long) >>> target_lengths = torch.randint(10, 30, (16,), dtype=torch.long) >>> loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths) >>> loss.backward()",0.95181274
tensorflow.python.compiler.tensorrt.test.test_utils.experimental_feature_scope,"Creates a context manager to enable the given experimental feature. This helper function creates a context manager setting up an experimental feature temporarily. Example: python with self._experimental_feature_scope(""feature_1""): do_smthg() Args: feature_name: Name of the feature being tested for activation.",torch.ao.quantization.fake_quantize.disable_observer,"Disable observation for this module. Disable observation for this module, if applicable. Example usage:: # model is any PyTorch model model.apply(torch.ao.quantization.disable_observer)",0.9517971
tensorflow.python.framework.subscribe.subscribe,"Subscribe to a tensor. This method will attach side effect graphs to a given set of tensors. Set of tensors follows from session.run and supports single `Tensor`, `list`, nested `list`, `tuple`, `namedtuple`, or `dict`. It returns the tensors in the same passed in structure, but as clones with side effects applied. The supplied side effect graphs are specified as a constructor function which takes the target tensor and constructs a side effect graph and returns a list of ops that should be control dependencies on fetching the tensor. It will append 'subscription' to the name scope of the tensor for every node in the side effect graph. These control dependencies are what trigger the side effects. Subscribe will construct the additions to your graph and return the created identity tensor downstream of the control dependencies. Use these tensors as you would normally in the rest of your tensorflow code. If a given tensor has already been subscribed or a tensor returned by a call to subscribe is passed, the previously created identity tensor will be reused and the side effect graphs will be added to the existing ones. Args: tensors: `Tensor` or set of tensors to subscribe to. Set of tensors format follows from `Session.run` and supports single `Tensor`, `list`, nested `list`, `tuple`, `namedtuple`, or `dict`. side_effects: Function(s) that takes a `Tensor`, construct a subgraph, and return a nonempty list of control dependencies. This can be a single function or list of functions. Returns: Subscribed tensors, which are identity copies of the passed in tensors in the same passed in structure, but the graph has been modified such that these are downstream of the control dependencies for the side effect graphs. Use these functionally equivalent tensors instead of the passed in tensors for further construction or running.",torch.distributed.fsdp.fully_sharded_data_parallel.register_comm_hook,"Register a communication hook. This is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like `GossipGrad <https://arxiv.org/abs/1803.05880>`_ and gradient compression which involve different communication strategies for parameter syncs while training with :class:`FullyShardedDataParallel`. .. warning :: FSDP communication hook should be registered before running an initial forward pass and only once. Args: state (object): Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in `GossipGrad <https://arxiv.org/abs/1803.05880>`_, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker. hook (Callable): Callable, which has one of the following signatures: 1) ``hook: Callable[torch.Tensor] -> None``: This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns ``None``; 2) ``hook: Callable[torch.Tensor, torch.Tensor] -> None``: This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns ``None``. Callables with signature 1 are expected to handle gradient communication for a `NO_SHARD` case. Callables with signature 2 are expected to handle gradient communication for sharded cases.",0.95178646
tensorflow.python.ops.control_flow_v2_toggles.disable_control_flow_v2,Opts out of control flow v2. Note: v2 control flow is always enabled inside of tf.function. Calling this function has no effect in that case. If your code needs tf.disable_control_flow_v2() to be called to work properly please file a bug.,torch._inductor.compile_fx.compile_fx_inner,"Inductor API that compiles a single graph. If you change the argument list for this function, make sure you also update the call to save_args_for_compile_fx_inner below accordingly.",0.9517844
tensorflow.python.ops.ragged.ragged_where_op.where,"Return the elements, either from `x` or `y`, depending on the `condition`. : If both `x` and `y` are `None`: Returns the coordinates of true elements of `condition`. The coordinates are returned in a 2-D tensor with shape `[num_true_values, dim_size(condition)]`, where `result[i]` is the coordinates of the `i`th true value (in row-major order). : If both `x` and `y` are non-`None`: Returns a tensor formed by selecting values from `x` where condition is true, and from `y` when condition is false. In particular: : If `condition`, `x`, and `y` all have the same shape: * `result[i1...iN] = x[i1...iN]` if `condition[i1...iN]` is true. * `result[i1...iN] = y[i1...iN]` if `condition[i1...iN]` is false. : Otherwise: * `condition` must be a vector. * `x` and `y` must have the same number of dimensions. * The outermost dimensions of `condition`, `x`, and `y` must all have the same size. * `result[i] = x[i]` if `condition[i]` is true. * `result[i] = y[i]` if `condition[i]` is false. Args: condition: A potentially ragged tensor of type `bool` x: A potentially ragged tensor (optional). y: A potentially ragged tensor (optional). Must be specified if `x` is specified. Must have the same rank and type as `x`. name: A name of the operation (optional) Returns: : If both `x` and `y` are `None`: A `Tensor` with shape `(num_true, dim_size(condition))`. : Otherwise: A potentially ragged tensor with the same type, rank, and outermost dimension size as `x` and `y`. `result.ragged_rank = max(x.ragged_rank, y.ragged_rank)`. Raises: ValueError: When exactly one of `x` or `y` is non-`None`; or when `condition`, `x`, and `y` have incompatible shapes. #### Examples: >>> # Coordinates where condition is true. >>> condition = tf.ragged.constant([[True, False, True], [False, True]]) >>> print(where(condition)) tf.Tensor( [[0 0] [0 2] [1 1]], shape=(3, 2), dtype=int64) >>> # Elementwise selection between x and y, based on condition. >>> condition = tf.ragged.constant([[True, False, True], [False, Tr",torch.nn.utils.rnn.pad_sequence,"Pad a list of variable length Tensors with :attr:`padding_value`. ``pad_sequence`` stacks a list of Tensors along a new dimension, and pads them to equal length. :attr:`sequences` can be list of sequences with size ``L x *``, where `L` is length of the sequence and ``*`` is any number of dimensions (including 0). If :attr:`batch_first` is ``False``, the output is of size ``T x B x *``, and ``B x T x *`` otherwise, where ``B`` is the batch size (the number of elements in :attr:`sequences`), ``T`` is the length of the longest sequence. Example: >>> from torch.nn.utils.rnn import pad_sequence >>> a = torch.ones(25, 300) >>> b = torch.ones(22, 300) >>> c = torch.ones(15, 300) >>> pad_sequence([a, b, c]).size() torch.Size([25, 3, 300]) Note: This function returns a Tensor of size ``T x B x *`` or ``B x T x *`` where `T` is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same. Args: sequences (list[Tensor]): list of variable length sequences. batch_first (bool, optional): if ``True``, the output will be in ``B x T x *`` format, ``T x B x *`` otherwise. padding_value (float, optional): value for padded elements. Default: 0. Returns: Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``. Tensor of size ``B x T x *`` otherwise",0.95171773
tensorflow.python.ops.sparse_ops.sparse_slice,"Slice a `SparseTensor` based on the `start` and `size`. For example, if the input is input_tensor = shape = [2, 7] [ a d e ] [b c ] Graphically the output tensors are: sparse.slice([0, 0], [2, 4]) = shape = [2, 4] [ a ] [b c ] sparse.slice([0, 4], [2, 3]) = shape = [2, 3] [ d e ] [ ] Args: sp_input: The `SparseTensor` to split. start: 1-D. tensor represents the start of the slice. size: 1-D. tensor represents the size of the slice. name: A name for the operation (optional). Returns: A `SparseTensor` objects resulting from splicing. Raises: TypeError: If `sp_input` is not a `SparseTensor`.",torch.nn.attention.bias.causal_lower_right,"Creates a lower-right triangular causal bias. This function generates a lower-right triangular matrix to represent causal attention bias with a diagonal offset set so that the inclusive values are aligned to the lower right corner of the matrix. The equivalent pytorch code for constructing this bias is: .. code-block:: python diagonal_offset = size[1] - size[0] torch.tril( torch.ones(size, dtype=torch.bool), diagonal=diagonal_offset, ) For instance, with `shape=(3,4)`, the materialized bias tensor will be: .. code-block:: text [[1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]] Args: size: The size of the bias matrix. Returns: CausalBias: The LOWER_RIGHT triangular causal bias variant.",0.9517158
tensorflow.python.keras.engine.node.parent_nodes,Returns all the `Node`s whose output this node immediately depends on.,torch.ao.ns.fx.n_shadows_utils.maybe_remap_node_to_shadow,"If unshadowed `node` has a shadow version, return that. If not, return `node`.",0.95171475
tensorflow.python.compiler.tensorrt.test.tf_trt_integration_test_base.BuildParams,"Build test parameters. The input_shapes and output_shapes arguments are known (static) shapes that can be used to generate test data. To define the model, we also specify corresponding input/output TensorSpecs. These are defined using the shape arguments. For each input tensor we define: input_spec = [None] + input_shape[1:] and similarly for output shapes. This means that we leave the first (batch) dimension unknown, the rest is just copied from the shapes arg. Args: graph_fn: The function to build the graph. dtype: The element type. input_shapes: The input shapes. output_shapes: The output shapes. Returns: The test parameters.",torch.ao.nn.quantized.functional.upsample_nearest,"Upsamples the input, using nearest neighbours' pixel values. .. warning:: This function is deprecated in favor of :func:`torch.ao.nn.quantized.functional.interpolate`. This is equivalent with ``nn.quantized.functional.interpolate(..., mode='nearest')``. .. note:: The input quantization parameters propagate to the output. .. note:: Only 2D inputs are supported Args: input (Tensor): quantized input size (int or Tuple[int, int] or Tuple[int, int, int]): output spatial size. scale_factor (int): multiplier for spatial size. Has to be an integer.",0.9517131
tensorflow.python.keras.engine.data_adapter.batch_size,"Return the batch size of the dataset created. For certain type of the data input, the batch size is known, and even required, like numpy array. Where as for dataset, the batch is unknown unless we take a peek. Returns: int, the batch size of the dataset, or None if it is unknown.",torch.testing._internal.opinfo.core.reference_inputs,Returns an iterable of SampleInputs. Distinct from sample_inputs() above because this returns an expanded set of inputs when reference_inputs_func is defined. If undefined this returns the sample inputs.,0.95168704
tensorflow.python.distribute.distribute_lib.group,Shortcut for `tf.group(self.experimental_local_results(value))`.,torch.onnx._internal.fx.passes.modularization.module_display_name,The display name of the module. E.g. `h_1_mlp_c_proj`.,0.95160425
tensorflow.python.tpu.tensor_tracer.keras_layer_tracepoint,An interface for adding the tensor outputs of a keras layer. Encapsulates trace_tensor. Args: layer: A keras layer. checkpoint_name: a string name for the checkpoint. This name has to be a unique name if used within model comparison. The tensors that have the same checkpoint identifier is compared in model comparison. Returns: The provided layer.,torch.distributed.checkpoint.storage.finish,Write the metadata and marks the current checkpoint as successful. The actual format/schema used for serializing `metadata` is an implementation detail. The only requirement is that it's recoverable in to the same object graph. Args: metadata (Metadata): metadata for the new checkpoint results: A list of WriteResults from all ranks. Returns: None,0.95157856
tensorflow.python.autograph.pyct.cfg.add_continue_node,"Grows the graph by adding a reentry node. This node causes control flow to go back to the loop section's entry. Args: ast_node: ast.AST section_id: Hashable, the node for which ast_node should be considered to be an exit node guards: Tuple[ast.AST, ...], the finally sections that guard ast_node",torch._export.utils.sequential_split,Splits the graph module into multiple submodules based on the node_call_back. The node_call_back should return True if the node is a delimiter. Delimiter will be the first node in the next submodule.,0.9515756
tensorflow.python.ops.distributions.bijector_impl.forward_log_det_jacobian,"Returns both the forward_log_det_jacobian. Args: x: `Tensor`. The input to the ""forward"" Jacobian determinant evaluation. event_ndims: Number of dimensions in the probabilistic events being transformed. Must be greater than or equal to `self.forward_min_event_ndims`. The result is summed over the final dimensions to produce a scalar Jacobian determinant for each event, i.e. it has shape `x.shape.ndims - event_ndims` dimensions. name: The name to give this op. Returns: `Tensor`, if this bijector is injective. If not injective this is not implemented. Raises: TypeError: if `self.dtype` is specified and `y.dtype` is not `self.dtype`. NotImplementedError: if neither `_forward_log_det_jacobian` nor {`_inverse`, `_inverse_log_det_jacobian`} are implemented, or this is a non-injective bijector.",torch.nn.utils.parametrize.remove_parametrizations,"Remove the parametrizations on a tensor in a module. - If ``leave_parametrized=True``, ``module[tensor_name]`` will be set to its current output. In this case, the parametrization shall not change the ``dtype`` of the tensor. - If ``leave_parametrized=False``, ``module[tensor_name]`` will be set to the unparametrised tensor in ``module.parametrizations[tensor_name].original``. This is only possible when the parametrization depends on just one tensor. Args: module (nn.Module): module from which remove the parametrization tensor_name (str): name of the parametrization to be removed leave_parametrized (bool, optional): leave the attribute :attr:`tensor_name` parametrized. Default: ``True`` Returns: Module: module Raises: ValueError: if ``module[tensor_name]`` is not parametrized ValueError: if ``leave_parametrized=False`` and the parametrization depends on several tensors",0.9515337
tensorflow.python.keras.losses.sparse_categorical_crossentropy,"Computes the sparse categorical crossentropy loss. Standalone usage: >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss.numpy() array([0.0513, 2.303], dtype=float32) Args: y_true: Ground truth values. y_pred: The predicted values. from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. axis: Defaults to -1. The dimension along which the entropy is computed. Returns: Sparse categorical crossentropy loss value.",torch.nn.attention.bias.causal_upper_left,"Creates an upper-left triangular causal bias. This function generates a upper-left triangular matrix to represent causal attention bias with a diagonal offset set so that the inclusive values are aligned to the upper left corner of the matrix. This equivalent to the `is_causal=True` argument in `scaled_dot_product_attention`. The equivalent pytorch code for constructing this bias is: .. code-block:: python torch.tril(torch.ones(size, dtype=torch.bool)) For instance, with `shape=(3,4)`, the materialized bias tensor will be: .. code-block:: text [[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]] Args: size: The size of the bias matrix. Returns: CausalBias: The UPPER_LEFT triangular causal bias variant.",0.9514801
tensorflow.python.profiler.pprof_profiler.function_protos,Returns list of `profile_pb2.Function` protos.,torch.serialization.get_safe_globals,Returns the list of user-added globals that are safe for ``weights_only`` load.,0.9514547
tensorflow.python.eager.context.function_scope_id,Returns an id that is unique to each scope holding functions.,torch.export.exported_program.parameters,Returns an iterator over original module's parameters.,0.9514522
tensorflow.python.keras.utils.data_utils.stop,"Stops running threads and wait for them to exit, if necessary. Should be called by the same thread which called `start()`. Args: timeout: maximum time to wait on `thread.join()`",torch._functorch.batch_norm_replacement.replace_all_batch_norm_modules_,In place updates :attr:`root` by setting the ``running_mean`` and ``running_var`` to be None and setting track_running_stats to be False for any nn.BatchNorm module in :attr:`root`,0.95144445
tensorflow.python.debug.wrappers.local_cli_wrapper.add_tensor_filter,Add a tensor filter. Args: filter_name: (`str`) name of the filter. tensor_filter: (`callable`) the filter callable. See the doc string of `DebugDumpDir.find()` for more details about its signature.,torch.fx.subgraph_rewriter.replace_pattern_with_filters,"See replace_pattern for documentation. This function is an overload with an additional match_filter argument. Args: ``match_filters``: A list of functions that take in (match: InternalMatch, original_graph: Graph, pattern_graph: Graph) and return a boolean indicating whether the match satisfies the condition. See matcher_utils.py for definition of InternalMatch.",0.95141625
tensorflow.python.tools.saved_model_cli.get_meta_graph_def,"DEPRECATED: Use saved_model_utils.get_meta_graph_def instead. Gets MetaGraphDef from SavedModel. Returns the MetaGraphDef for the given tag-set and SavedModel directory. Args: saved_model_dir: Directory containing the SavedModel to inspect or execute. tag_set: Group of tag(s) of the MetaGraphDef to load, in string format, separated by ','. For tag-set contains multiple tags, all tags must be passed in. Raises: RuntimeError: An error when the given tag-set does not exist in the SavedModel. Returns: A MetaGraphDef corresponding to the tag-set.",torch.ao.pruning._experimental.data_sparsifier.benchmarks.evaluate_disk_savings.sparsify_model,"Sparsifies the embedding layers of the dlrm model for different sparsity levels, norms and block shapes using the DataNormSparsifier. The function tracks the step time of the sparsifier and the size of the compressed checkpoint and collates it into a csv. Note:: This function dumps a csv sparse_model_metadata.csv in the current directory. Args: path_to_model (str) path to the trained criteo model ckpt file sparsity_levels (List of float) list of sparsity levels to be sparsified on norms (List of str) list of norms to be sparsified on sparse_block_shapes (List of tuples) List of sparse block shapes to be sparsified on",0.9514025
tensorflow.python.ops.ragged.ragged_embedding_ops.embedding_lookup,"Look up the ragged ids in a list of embedding tensors. Args: params: A tensor representing the complete embedding tensor having the shape [e1, ...eM] ragged_ids: A 'RaggedTensor' with type 'int32' or 'int64' containing the ids to be looked up in 'params' of shape [r0, ..rN]. Values must be in the range '[0, params.shape[0]]'. partition_strategy: A string specifying the partitioning strategy. max_norm: If not `None`, each embedding is clipped if its l2-norm is larger than this value. name: A name for the operation (optional) Returns: A ragged tensor of shape [r0, r1, ...rN, e1, ...eM]. Raises: ValueError: When params is empty or the type of the ids is not int32 or int64.",torch._export.utils.placeholder_naming_pass,"This pass is run at the end of _export_non_strict() to assign better placeholder node names: - User inputs: These follow the signature of mod.forward(), e.g. forward(x, y) produces nodes x, y. For nested inputs from dictionaries, lists, tuples, or dataclasses, the names are a concatenation of the path to the tensor. e.g. x = { 'a': torch.randn(), 'b': [torch.randn(), torch.randn()] } produces nodes x_a, x_b_0, x_b_1. - Parameters/buffers/constants/custom objects: These follow the FQN of the object, prefixed by ""p"", ""b"", ""c"", ""obj"" respectively. e.g. self.bar.l0.weight produces ""p_bar_l0_weight"". - Effect tokens: These are named token, token_1, ...",0.9513929
tensorflow.python.tpu.feature_column.shared_embedding_columns,"List of dense columns that convert from sparse, categorical input. Note that the interface for TPU embedding_column is different from the non-TPU version. The following args available for the non-TPU version are NOT supported: ckpt_to_load_from, tensor_name_in_ckp, max_norm and trainable. Args: categorical_columns: A list of categorical_columns returned from categorical_column_with_identity, weighted_categorical_column, categorical_column_with_vocabulary_file, categorical_column_with_vocabulary_list, sequence_categorical_column_with_identity, sequence_categorical_column_with_vocabulary_file, sequence_categorical_column_with_vocabulary_list dimension: An integer specifying dimension of the embedding, must be > 0. combiner: A string specifying how to reduce if there are multiple entries in a single row for a non-sequence column. For more information, see `tf.feature_column.embedding_column`. initializer: A variable initializer function to be used in embedding variable initialization. If not specified, defaults to `tf.truncated_normal_initializer` with mean `0.0` and standard deviation `1/sqrt(dimension)`. shared_embedding_collection_name: Optional name of the collection where shared embedding weights are added. If not given, a reasonable name will be chosen based on the names of `categorical_columns`. This is also used in `variable_scope` when creating shared embedding weights. max_sequence_lengths: An list of non-negative integers, either None or empty or the same length as the argument categorical_columns. Entries corresponding to non-sequence columns must be 0 and entries corresponding to sequence columns specify the max sequence length for the column. Any sequence shorter then this will be padded with 0 embeddings and any sequence longer will be truncated. learning_rate_fn: A function that takes global step and returns learning rate for the embedding table. If you intend to use the same learning rate for multiple embedding tables, please ensure that you pass the e",torch.nn.functional.multi_head_attention_forward,"Forward method for MultiHeadAttention. See :class:`torch.nn.MultiheadAttention` for details. Args: query, key, value: map a query and a set of key-value pairs to an output. See ""Attention Is All You Need"" for more details. embed_dim_to_check: total dimension of the model. num_heads: parallel attention heads. in_proj_weight, in_proj_bias: input projection weight and bias. bias_k, bias_v: bias of the key and value sequences to be added at dim=0. add_zero_attn: add a new batch of zeros to the key and value sequences at dim=1. dropout_p: probability of an element to be zeroed. out_proj_weight, out_proj_bias: the output projection weight and bias. training: apply dropout if is ``True``. key_padding_mask: if provided, specified padding elements in the key will be ignored by the attention. This is an binary mask. When the value is True, the corresponding value on the attention layer will be filled with -inf. need_weights: output attn_output_weights. Default: `True` Note: `needs_weight` defaults to `True`, but should be set to `False` For best performance when attention weights are not needed. *Setting needs_weights to `True` leads to a significant performance degradation.* attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch. is_causal: If specified, applies a causal mask as attention mask, and ignores attn_mask for computing scaled dot product attention. Default: ``False``. .. warning:: is_causal is provides a hint that the attn_mask is the causal mask.Providing incorrect hints can result in incorrect execution, including forward and backward compatibility. use_separate_proj_weight: the function accept the proj. weights for query, key, and value in different forms. If false, in_proj_weight will be used, which is a combination of q_proj_weight, k_proj_weight, v_proj_weight. q_proj_weight, k_proj_weight, v_proj_weight, in_proj_",0.9513131
tensorflow.python.ops.numpy_ops.tests.np_test.testIssue1233,Following numpy test suite from `test_repeat` at https://github.com/numpy/numpy/blob/master/numpy/core/tests/test_multiarray.py,torch._dynamo.bytecode_transformation.encode_varint,6-bit chunk encoding of an unsigned integer See https://github.com/python/cpython/blob/3.11/Objects/locations.md,0.9512758
tensorflow.python.framework.extension_type_test.temporarily_register_type_spec,Context manager for making temporary changes to the TypeSpec registry.,torch._inductor.codegen.cpp.masked,Context manager to add an additional mask to loads and stores.,0.9512687
tensorflow.python.keras.engine.base_layer.add_weight,"Adds a new variable to the layer. Args: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's ""trainable_variables"" (e.g. variables, biases) or ""non_trainable_variables"" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`.",torch.nn.modules.module.state_dict,"Return a dictionary containing references to the whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to ``None`` are not included. .. note:: The returned object is a shallow copy. It contains references to the module's parameters and buffers. .. warning:: Currently ``state_dict()`` also accepts positional arguments for ``destination``, ``prefix`` and ``keep_vars`` in order. However, this is being deprecated and keyword arguments will be enforced in future releases. .. warning:: Please avoid the use of argument ``destination`` as it is not designed for end-users. Args: destination (dict, optional): If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an ``OrderedDict`` will be created and returned. Default: ``None``. prefix (str, optional): a prefix added to parameter and buffer names to compose the keys in state_dict. Default: ``''``. keep_vars (bool, optional): by default the :class:`~torch.Tensor` s returned in the state dict are detached from autograd. If it's set to ``True``, detaching will not be performed. Default: ``False``. Returns: dict: a dictionary containing a whole state of the module Example:: >>> # xdoctest: +SKIP(""undefined vars"") >>> module.state_dict().keys() ['bias', 'weight']",0.95125544
tensorflow.python.ops.ragged.ragged_math_ops.tensor_not_equals,Ragged version of the operation invoked by `Tensor.__ne__`.,torch.ao.quantization.pt2e.export_utils.forward,Simple forward that just calls the ``fn`` provided to :meth:`WrapperModule.__init__`.,0.9512395
